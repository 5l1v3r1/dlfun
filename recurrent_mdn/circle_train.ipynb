{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.models.rnn import rnn_cell\n",
    "from tensorflow.models.rnn import seq2seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthesize data (circle with noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x118dd4c90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKIAAACbCAYAAADhh0B+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAFOBJREFUeJztnXuUFNW1h7/NKMbH8DKAXkGuJICaeGMUxQcJqIiARkmU\nXCCJkBAhUfLwcSMGDTGurECEayJG4/OKKIIagyPKZXwAioaIcTTAAsQgioiDXmBmEMFhZt8/do00\nPdPTNV1VXdU951ur1lRXnd5n9/Svz6nz2OeIquJwxE2buB1wOMAJ0ZEQnBAdicAJ0ZEInBAdicAJ\n0ZEInBCbQUTaiEiNiHQLM20Ifp0jIm9HnU8+KSohekKo9o46EdmVcm1US+2par2qlqrqe2GmDQlf\nHcAiMk5EFkftTFAOiNuBMFHV0oZzEdkAjFPVjF+CiJSoal1enIsPwado46SoSsQ0xDv2XRC5SUTm\nisgcEakCviMip4nI30Rku4hsFpE/ikiJl75EROpF5Gjv9Wzv/tNeKfuSiPRoaVrv/lARWefle6uI\nLBORS5v8ICIHe/a2ichK4OS0+5NF5F9ePitF5Bve9S8DM4GveTXDVu/6N0SkQkSqRGSjiFwfyn88\nCKpalAfwNnB22rWbgN3AMO/1QdiXegom2n8H1gKXe/dLgDrgaO/1bGAr8FXv3lzggRzSdgGqgQu8\ne1cCe4BLM3yW6cDzQDugO7Aa2JBy/xKgi3c+EqgBOnuvxwHPp9kbCBznnZ/g+Tkszu+rmEvETCxT\n1acBVHWPqv5DVVeosRG4GxiQkl7S3v+YqlZ4VfpDwIk5pD0fqFDVBapap6q3AP/XjM8jgJtUtVpV\nNwG3pd5U1cdUdat3PhfYCPTNZExVl6jqGu98JTAv7TPnndYoxE2pL0Skj4gsEJEtXnV9I/D5Zt7/\nQcr5LuCwHNL+W7ofQHONnCPT7r+TelNExorI617VvR3oQzOfQUROF5HFIrJVRHZgpWZznzlyWqMQ\n0x/c7wRWAj1VtT0whcYlW9hswarYVI5qQfrUZ81jgNuBCaraSVU7AuvY9xmaaqg8DDwKHKWqHYB7\nif4zN0trFGI6pUCVqn4iIscBE/KQ5wLgqyJyvtfI+TnNl0iPAr8UkfZeY+iKlHuHAfXAR56ty4Bj\nU+5XAt1E5IC092xX1VoROQ17royVYhai3y6Lq4GxIlIN3IE1KjLZyWbTV1rvee4/gVuAj4BjgAqs\nwdIUU7BqfiPwFDArxdZKrGW8Angf6AUsT3nvM8B6oFJE3veuXQ5M9R5FJmHPiLEiGsLEWBG5F2sB\nVqrqf2RIcyswFPgYGKuqrwfOuEgQkTaYiC5W1Zfi9icOwioR/wc4L9NNERkKfEFVe2FV359Dyrdg\nEZHzvKr2IOBXwKfAKzG7FRuhCFFVlwHbm0lyEfCAl/bvQHsR6RpG3gVMf2AD9gx3LjBcVWvjdSk+\n8jXEdxT7d1ds9q5V5in/xKGqNwA3xO1HUijmxoqjgMhXibiZ/fvBunnXGiEiiR+gd2RGVXPqjwyz\nRGw0ySCFMuBSAK/faoeqZqyWoxjLnDJlSkHZLUSfgxBKiSgic7CB9MNF5F2s36stoKp6l6o+LSLD\nROQtrPvm+2Hk6ygeQhGiqo72kWZiGHk5ipNW01gZOHBgQdmN0naUPudKKCMrYSIimjSfHP4QkUQ0\nVhyOnHFCdCQCJ0RHInBCdCSCogonTQy7dsHy5fDCC7BmDXTqBF272tGly/7n7dqBxDo5OhG4VnMY\n7NgBjzwCs2dDRQV8/DH07g3nnw8nn2z3Kyvt2Lp133llJezdu0+cF1wAkybBQQfF/YlyIkir2Qkx\nFzZvhttug6lTm77/+c9DXR1UV8MBB1ipV1pqf1PPS0uhpAR274adO+HRR+39v/kNjBwJvXrl7zOF\ngBNi1FRUwLRpMK+JGfUTJsDEifClLzWuYlVNZNXVUFNjf1PPm7p2//373j9mDNxyC3TsGOnHC4sg\nQoxswD7AwLnGSl2d6urVqnffrTpmjKrJyY4OHVSnTlX96KNofdi6VfW88/ble//9qvX10eYZAt53\nl9P37kpEgD174Lnn4PHHoazMqswzzrCje3cYMsSq2HxTVgYXXWTnX/mKPYOecEL+/fCJKxFzoaZG\n9ZFHVEeOVG3fXrV/f9UZM1Q3bMhP/n7Zvl117Nh9peNVV6lWV8ftVZMQoESMXXiNHIpaiJWVqsOH\nq5aWqg4ZonrnnapbtkSbZxiUl6secoh9ZYccYj+ihFXXsQsRGIItXvQmcG0T9wcAO4DXvOP6ZmxF\n9X9SrapSPekk1WuuUd2xI7p8oqKmRvUnP9lXOp57ruq6dXF79RmxChEbnXkLWwbjQOB14Ni0NAOA\nMp/2ovkvffKJ6llnqf74x4krSVrMiy+q9uxpX19JieoNN6ju2hW3V4GEGMYQ36nAelV9Ry0cci4W\nPppOfMMHe/fC6NHWcTxzZuGPZPTvD6tWwRVXWH/k889b99HTT8ftWc6EIcT0UNH3aHpBodO9Faue\nEpHjQ8jXH6rwox/ZaMcDD1gHcjFw8MH2o5o0Cd57D668EsaPb7qvswDIV5/EP7AFLHd5qz7MB3rn\nJefrrrPS49lnoW3bvGSZN0TgF7+wkn7SJBuRmTgRunWDM8+M27sWEYYQNwNHp7xuFCqqqjtTzheK\nyO0i0klVtzVl8Ne//vVn5wMHDsx9avv06dYX9+KLcFhzyxgWOGPH2rDi978P3/kOXHwxLFsGX/xi\npNkuWbKEJUuWhGMs14fLhgNberehsdIWa6wcl5ama8r5qcDGZuyF8+R8332qPXqobtoUjr1C4KWX\nVLt0sT7RXr2iHwFKg4R036zDlj+b5F2bAIz3zq8AVmFLr70M9GvGVvD/yPz5qkccobp2bXBbhcbq\n1ardu1trun9/6y3IE7ELMcwjsBCXLFHt3Fn11VeD2Slk3n1XtU8f+3pHjbLx8zzghNjAa6+ZCJ97\nLncbxcKmTTZJA1QnT85LlkGEWDyhAm+/bRNR//xnOPvsuL2Jn27d4LHHrLtq+nS47764PWqW4ph9\nU1trnbwjR1p/mmMfv/sdzJgB9fU2i3zQoMiychNjr70WVq+GJ58s/FGTsKmvh+HD4d134f33bRTm\ny1+OJKsgQiz84KnycnjoIZtF7UTYmDZtbESpb18YONBeJ5BkeuWXykrrzJ09Gzp3jtub5NKhgz0v\nLl5sQ54JpHCFWF8Pl14KP/gBnHVW3N4knxNPhJtvtlGXmpq4vWlE4T4j3nwzzJ8PS5fGM42/UBk/\n3sJb580L/VGm9TVWXnnFYoBXrIAePZpP69if3buth+G734Wf/zxU062rsVJdDaNGwR13OBHmwuc+\nZ8+L/fpZA6Z//7g9AgqtRFS12SXt2lnHtSN3Fi6Eyy6DV1+FI44IxWTrWR/x/vth5UoLOncEY+hQ\nGDfOBgHq6uL2poBKxLVr4WtfgyVLbFq8Izh1dbasSVlZKJ3csZeIIjJERNaKyJsicm2GNLeKyHov\nXODEptJkZPdu++X+9rdOhGFSUmLVclVV3J4EF6K3s+Zt2KaQXwJGicixaWmCbQp5ww22utZllwV1\n15FO+/bFIUT8RfEF2xRy8WK45ho3hBcFRSREP1F8mTaF9Ed1tQ1TOcKniIQYPVVV1mXjCJ927eyH\nHjN5ieKjBZtCQhNRfFVV9st1hE+AEjHMKL7A3TciUoIFTp0DbMF2YR+lqmtS0gwDrlDV871NIf+g\nqqdlsLd/982ePbZM3J497hkxCmbOhHXrbAXcgMQ6xKeqdSIyESjHqvp7VXWNiEwgjE0hq6vdgudR\nkpBnxLA2hfxfoE/atTvTXue2KaSrlqMlIUJMfmOloUR0RIMTok9ciRgt7do5IfrCCTFa2rdPRPdN\n8oXYtSv8619xe1G8uKrZJ/362dT2NWuyp3W0nIYSMeZZWMkXYps2FvDTsCuTI1y2b7dZ2/X1sbqR\nfCECjBjhhBgV8+bZXi4xr6RbGEI84wzYts0mxzrC5cEHLZAqZgpDiK56joY334R33ol0PRy/FIYQ\nwVXPUfDQQxYRmYC48MIR4plnwkcf2QC9IziqiamWoZCE6KrncFm+3HZZOOmkuD0BCkmI4KrnMGko\nDRMyqynQfEQR6QjMw3YU2Ah8W1UbddOLyEagCqgHalX11GZsZg6wr6uzlVCXLrVgKkdufPopHHWU\nLd1yzDGhmY0znHQS8Kyq9gGeB67LkK4eGKiqX21OhFkpKXHVcxgsWgTHHhuqCIMSVIgXAbO881nA\n8AzpJIS8DFc9BydBjZQGglbN21S1U6bXKdc3YNvk1gF3qerdzdhsfjWwhur5hRdslQJHy6iqgqOP\ntsXvOzX6qgIRaaiAiDwDpMYgC6DA9U0kz6SgM1V1i4h0Bp4RkTWquqzF3oJVz9/6lpWKv/xlTiZa\nNY8/brsuhCzCoGQVoqqem+meiFSKSFdVrRSRI4CtGWxs8f5+KCJ/xYLyMwox6158I0bY7gFOiC3n\nvvtC23khMVF8IjIN2Kaq07w1bzqq6qS0NIcAbVR1p4gcigVZ3aiq5RlsZl+os67OWn152PiwqFix\nwn7Eb70VyWhKnK3macC5ItIQTjrVc+hIEVngpekKLBORCmA58GQmEfqmpAS++U34y18CmWl1zJgB\nP/tZIob00imcZenSWbQIbrwRXn45eqeKgY0b4eST7W9paSRZtL41tMEC7rt2tbHnrv7Xc2q1XHkl\nHHgg/P73kWUR+/qIsXDQQTB4MCxYkD1ta2fHDpg1C37607g9yUjhChFsZvETT8TtRfK56y7bMLNb\nt7g9yUjhVs1g8RY9esCWLXDoodE6Vqh8+in07Gk1x4ktW6i3pbTOqhmgY0c45RR45pm4PUku8+bZ\nuHLEIgxKYQsRXPXcHKq2V/PVV8ftSVaKQ4gLFiRii4bE8dxzsHcvDBkStydZKXwh9uhhoyx/+1vc\nniSPhtIwIZNfm6PwhQhw4YWuek5n1Sp44w3bqasAKA4hNjwnJqwHIFZmzICJE62/tQAo7O6bBlRt\njl15ORx3XDSOFRJbtsDxx9vkhsMPz1u2rbf7pgERVz2nMnOmVcl5FGFQikOI4LpxGvjkE7j77tD3\nYo6aQEIUkUtEZJWI1IlIxgBZP3v1BWbgQFi/3uYotmZefNEiHAtsnmbQEnEl8E1gaaYEfvbqC4W2\nbWHOHLjkkta9WNOiRXDeeXF70WICCVFV16nqeiyOJRN+9uoLh8GDYdo024v4gw8iySLxlJe3PiH6\nxM9efeExZoxtiD1sGNTURJZNItm8Gd5/H/r2jduTFpNViCLyjIj8M+VY6f39Rj4czInJk20yxIgR\nUFsbtzf5o7wczjkn9kU3cyFQFJ9P/OzVtx9Zo/iyIQJ/+hMMHw4TJsC99xbEMFdg8lwthxnFh6oG\nPoDFwMkZ7pUAb2Hr47QFXgeOa8aWhsbOnap9+6r+6lfh2Uwqe/eqHn646qZNsbngfXc5aSho981w\nEdkEnAYsEJGF3vXPovhUtQ5o2KtvNTBXUzaMjJRDD7WZOQ8+CPfck5csY6OiwmJ3EjwLuzmKY4gv\nG2++CQMG2IjDxRcXXzW9YgVcd51Nfp0+PTY33BBfNnr3hvnzrRHTty/Mnm1T6AuZ+nooK4Ovf90a\nZRdcYOG1hUqudXpUB2E+I6ZTV6e6YIHqoEGqRx6petNNqh9+GF1+UbBrl+odd6j27q16yimqc+eq\n1tbG7ZWqBntGjF14jRyKUoip/POfquPGqXbooPrDH6quWpWffHOlstIaXV26qF54oerSpar19XF7\ntR9BhNg6quamOOEEa8CsW2dTyAYNspGZhQtj34VpP9auhfHjLQCqstKW43viCauSi+hZt3U0Vvyw\nZ49FvN1yC+zebWvEfPvb1iFeXW1HTc2+8/TXTZ3X1lq8yKhRcPrptiC9H1RteeYZM2x54csvt6Nz\n52j/BwFpnUuORIWqlTp/+IN1EB92mO1pXFpqf1tyXl9vpdecOfDxxzByJIwebaVxU6VZbS089pgJ\ncOdOuOoq+N734OCD8/9/yAEnxKSjCitXmiAfftjEPXq0lZQ9e1rpec898Mc/2rrW11xjY+V+S9CE\n4IRYSNTXW8Thww/DI49A9+62QtfgwRZxV4ATFhpwQixU9u61x4AvfMHCYgscJ0RHInAjK46CxwnR\nkQicEB2JwAnRkQjyFU66UUTeEJEKEXklSJ6O4iTycFKPcDaFDEBoU9rzZDdK21H6nCv5CCeFMDeF\nzJFC/FIL0edcyZc4FNuDb4WIXJanPB0FRJBNISer6pM+8wlvU0hHURLKyIqILAauVtXXfKSdAtSo\n6n9nuO+GVQqYXEdWwtyUrUkHmtgUcjCQMbgi1w/iKGwiDyclik0hHUVH4iY9OFonsXapiMjvRWSN\niLwuIn8RkXYZ0rV4fcWoOtujWhNSRDqKSLmIrBORRSLSPoi/fvIXkVtFZL33//e9I1A22yIyQER2\niMhr3nF9VqO5Rl2FcQCDsOdHsL2ef9dEmjbsW7LkQGzJkmN92O4D9AKeB05qJt0GbMNzvz5ntZuL\nz9je17/wzq8Fpubqr5/8gaHAU955P2C5z8/vx/YAoKwlWoi1RFTVZ1W1IWRuObZAUzo5ra+oEXW2\n+7Sbi88XAbO881nA8AD++sn/IuABAFX9O9BeRPzsN+z3s7Wo0ZmkSQ8/ABY2cT3q9RWj6GzPxecu\nqloJoKofAF0ypPPjr5/809Ns9uGjX9sAp3tV/lMicnw2o2F23zSJnw5xEZkM1KrqnLBt+6BRZzsw\nJQS7LfG3qWeoTK3IQhgc+AdwtKruEpGhwHygd3NviFyImmV9RREZCwwDzs6QJOP6itls+/Rvi/f3\nQxH5K3BqCHab9FlVM+7cLSKVItJVVStF5Ahgq19/gXQh+lmTcjPQPUuapshqW1V3ppwvFJHbRaST\nqm7LZDTuVvMQ4L+AC1V1T4ZkK4AvikgPEWkLjATKWppVhvwPEZHDvPOGzvZVQe2Sm89lwFjvfAzQ\naK+OFvjrJ/8y4FLP1mnAjoZHgyxktZ36rCkip2LdhBlFCMTeal4PvAO85h23e9ePBBakpBsCrPPS\nT/Jpezj2LPMJsAVYmG4bOAZr9VVgU9qy2vZjNxefgU7As957yoEOQfxtKn9gAjA+Jc1tWAv4DZrp\nWWipbeAK7AdSAbwM9Mtm03VoOxJBklrNjlaME6IjETghOhKBE6IjETghOhKBE6IjETghOhKBE6Ij\nEfw/vjqJI5ybHUAAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x118d68950>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_circle_data(nbatches=2500, tsteps = 25, cycle=15):\n",
    "    N = nbatches*tsteps + 2 #need one extra valus so that we can take differences\n",
    "    \n",
    "    #kernel for defining shape of training data (square)\n",
    "    theta_max = (math.pi*2)*N/cycle\n",
    "    thetas = np.reshape(np.linspace(0,theta_max,N),(1,N))\n",
    "    data = np.concatenate((np.cos(thetas), np.sin(thetas)),axis=0)\n",
    "\n",
    "    data += np.random.uniform(-0.1, 0.1, (2,N))\n",
    "    data = np.concatenate((data,np.zeros_like(thetas)),axis=0)\n",
    "    data = data.T\n",
    "    \n",
    "    data = data[1:,:] - data[:-1,:]\n",
    "    \n",
    "    #convert to 2D vols of data\n",
    "    X = data[:-1,:]\n",
    "    X_vol = np.zeros((X.shape[0]/tsteps,tsteps,3))\n",
    "    for i in range(X.shape[0]/tsteps):\n",
    "        X_vol[i,:,:] = X[i*tsteps : (i+1)*tsteps,:]\n",
    "    Y = data[1:,:]\n",
    "    Y_vol = np.zeros((Y.shape[0]/tsteps,tsteps,3))\n",
    "    for i in range(Y.shape[0]/tsteps):\n",
    "        Y_vol[i,:,:] = Y[i*tsteps : (i+1)*tsteps,:]\n",
    "    return (X_vol, Y_vol)\n",
    "\n",
    "(X_vol,Y_vol) = get_circle_data(nbatches=100, tsteps=25, cycle=20)\n",
    "\n",
    "r = Y_vol[0,:,:]\n",
    "r = np.cumsum(r, axis=0)\n",
    "\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.plot(r[:,0], r[:,1],'r-')\n",
    "plt.title('Training data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, batch_size=50, tsteps=300, scale_factor = 1, limit = 500):\n",
    "        self.batch_size = batch_size\n",
    "        self.tsteps = tsteps\n",
    "        self.scale_factor = scale_factor\n",
    "    def next_batch(self):\n",
    "        (x_batch, y_batch) = get_circle_data(self.batch_size, self.tsteps, cycle=15)\n",
    "        x_batch[:,:,:2] /= self.scale_factor\n",
    "        y_batch[:,:,:2] /= self.scale_factor\n",
    "        return x_batch, y_batch\n",
    "    def reset_batch_pointer(self):\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build deep recurrent model with MDN densecap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "generate = False\n",
    "hidden = 128\n",
    "nlayers = 2\n",
    "dropout_keep = 0.8\n",
    "tsteps = 50\n",
    "\n",
    "batch_size = 100\n",
    "nmixtures = 20\n",
    "num_epochs = 30\n",
    "\n",
    "save_every =50\n",
    "grad_clip = 10.\n",
    "learning_rate = 0.01\n",
    "decay_rate = .95\n",
    "\n",
    "data_scale=0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cell_func = rnn_cell.BasicLSTMCell\n",
    "cell0 = cell_func(hidden)\n",
    "cell1 = cell_func(hidden)\n",
    "# cells = rnn_cell.MultiRNNCell(cell_list)\n",
    "\n",
    "if (generate == False and dropout_keep < 1): # training mode\n",
    "    cell0 = rnn_cell.DropoutWrapper(cell0, output_keep_prob = dropout_keep)\n",
    "    cell1 = rnn_cell.DropoutWrapper(cell1, output_keep_prob = dropout_keep)\n",
    "\n",
    "input_data = tf.placeholder(dtype=tf.float32, shape=[None, tsteps, 3])\n",
    "target_data = tf.placeholder(dtype=tf.float32, shape=[None, tsteps, 3])\n",
    "istate_cell0 = cell0.zero_state(batch_size=batch_size, dtype=tf.float32)\n",
    "istate_cell1 = cell1.zero_state(batch_size=batch_size, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_out = 1 + nmixtures * 6 # end_of_stroke + gaussian mixtures defining stroke locations\n",
    "#put a mixture density network (mdn) cap on top of the rnn cells\n",
    "with tf.variable_scope('rnn_root'):\n",
    "    output_w = tf.get_variable(\"output_w\", [hidden, n_out])\n",
    "    output_b = tf.get_variable(\"output_b\", [n_out])\n",
    "\n",
    "#slice the input volume into separate vols for each tstep\n",
    "inputs = [tf.squeeze(input_, [1]) for input_ in tf.split(1, tsteps, input_data)]\n",
    "\n",
    "#build cell0 computational graph\n",
    "outputs, pstate_cell0 = seq2seq.rnn_decoder(inputs, istate_cell0, cell0, loop_function=None, scope='cell0')\n",
    "\n",
    "#build cell1 computational graph\n",
    "outputs, pstate_cell1 = seq2seq.rnn_decoder(outputs, istate_cell1, cell1, loop_function=None, scope='cell1')\n",
    "\n",
    "#put dense cap on top of cell1\n",
    "output = tf.reshape(tf.concat(1, outputs), [-1, hidden]) #concat outputs before sending them through mdn cap\n",
    "output = tf.nn.xw_plus_b(output, output_w, output_b) #data flows through mdn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add \"character window\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cell_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 2D gaussian looks like\n",
    "$\\mathcal{N}(x|\\mu,\\sigma,\\rho)=\\frac{1}{2\\pi\\sigma_1\\sigma_2\\sqrt(1-\\rho^2)}exp\\left[\\frac{-Z}{2(1-\\rho^2)}\\right]$ where $Z=\\frac{(x_1-\\mu_1)^2}{\\sigma_1^2}+\\frac{(x_2-\\mu_2)^2}{\\sigma_2^2}-\\frac{2\\rho(x_1-\\mu_1)(x_2-\\mu_2)}{\\sigma_1\\sigma_2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gaussian2d(x1, x2, mu1, mu2, s1, s2, rho):\n",
    "    # define gaussian mdn (eq 24, 25 from http://arxiv.org/abs/1308.0850)\n",
    "    x_mu1 = tf.sub(x1, mu1)\n",
    "    x_mu2 = tf.sub(x2, mu2)\n",
    "    Z = tf.square(tf.div(x_mu1, s1)) + \\\n",
    "        tf.square(tf.div(x_mu2, s2)) - \\\n",
    "        2*tf.div(tf.mul(rho, tf.mul(x_mu1, x_mu2)), tf.mul(s1, s2))\n",
    "    rho_square_term = 1-tf.square(rho)\n",
    "    power_e = tf.exp(tf.div(-Z,2*rho_square_term))\n",
    "    regularize_term = 2*np.pi*tf.mul(tf.mul(s1, s2), tf.sqrt(rho_square_term))\n",
    "    gaussian = tf.div(power_e, regularize_term)\n",
    "    return gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the mdn cap, our loss function becomes \n",
    "$$ \\mathcal{L}(x)=\\sum_{t=1}^{T} -log\\left(\\sum_{j} \\pi_t^j\\mathcal{N}(x_{t+1}|\\mu_t^j,\\sigma_t^j,\\rho_t^j)\n",
    "\\right)\n",
    "-\\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            \\log e_t & (x_{t+1})_3=1\\\\\n",
    "            \\log(1-e_t) & \\quad \\mathrm{otherwise}\n",
    "        \\end{array}\n",
    "    \\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_loss(pi, x1_data, x2_data, eos_data, mu1, mu2, sigma1, sigma2, rho, eos):\n",
    "    # define loss function (eq 26 of http://arxiv.org/abs/1308.0850)\n",
    "    gaussian = gaussian2d(x1_data, x2_data, mu1, mu2, sigma1, sigma2, rho)\n",
    "    term1 = tf.mul(gaussian, pi)\n",
    "    term1 = tf.reduce_sum(term1, 1, keep_dims=True) #do inner summation\n",
    "    term1 = -tf.log(tf.maximum(term1, 1e-20)) # some errors are zero -> numerical errors.\n",
    "\n",
    "    term2 = tf.mul(eos, eos_data) + tf.mul(1-eos, 1-eos_data) #modified Bernoulli -> eos probability\n",
    "    term2 = -tf.log(term2) #negative log error gives loss\n",
    "    \n",
    "    return tf.reduce_sum(term1 + term2) #do outer summation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gaussian mixture density network parameters are \n",
    "\n",
    "$$e_t=\\frac{1}{1+\\exp(\\hat e_t)} \\quad \\quad \\pi_t^j=\\frac{\\exp(\\hat \\pi_t^j)}{\\sum_{j'=1}^M\\exp(\\hat \\pi_t^{j'})} \\quad \\quad \\mu_t^j=\\hat \\mu_t^j \\quad \\quad \\sigma_t^j=\\exp(\\hat \\sigma_t^j)  \\quad \\quad  \\rho_t^j=\\tanh(\\hat \\rho_t^j)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# below is where we need to do MDN splitting of distribution params\n",
    "def get_mixture_coef(z):\n",
    "    # returns the tf slices containing mdn dist params (eq 18...23 of http://arxiv.org/abs/1308.0850)\n",
    "    z_eos = z[:, 0:1] #end of sentence tokens\n",
    "    z_pi, z_mu1, z_mu2, z_s1, z_s2, z_rho = tf.split(1, 6, z[:, 1:])\n",
    "    \n",
    "    # end of stroke signal\n",
    "    eos = tf.sigmoid(-1*z_eos) # technically we gained a negative sign\n",
    "\n",
    "    # softmax z_pi:\n",
    "    max_pi = tf.reduce_max(z_pi, 1, keep_dims=True)\n",
    "    z_pi = tf.exp( tf.sub(z_pi, max_pi) )\n",
    "    normalize_term = tf.inv(tf.reduce_sum(z_pi, 1, keep_dims=True))\n",
    "    pi = tf.mul(normalize_term, z_pi)\n",
    "    \n",
    "    #leave mu1, mu2 as they are\n",
    "    mu1 = z_mu1; mu2 = z_mu2\n",
    "    \n",
    "    # exp for sigmas\n",
    "    sigma1 = tf.exp(z_s1); sigma2 = tf.exp(z_s2)\n",
    "    \n",
    "    #tanh for rho (goes between -1 and 1)\n",
    "    rho = tf.tanh(z_rho)\n",
    "\n",
    "    return [eos, pi, mu1, mu2, sigma1, sigma2, rho]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reshape target data (as we did the input data)\n",
    "flat_target_data = tf.reshape(target_data,[-1, 3])\n",
    "[x1_data, x2_data, eos_data] = tf.split(1, 3, flat_target_data) #we might as well split these now\n",
    "\n",
    "[eos, pi, mu1, mu2, sigma1, sigma2, rho] = get_mixture_coef(output)\n",
    "\n",
    "loss = get_loss(pi, x1_data, x2_data, eos_data, mu1, mu2, sigma1, sigma2, rho, eos)\n",
    "cost = loss / (batch_size * tsteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model (build graph, then start session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#define how to train the model\n",
    "lr = tf.Variable(0.0, trainable=False)\n",
    "tvars = tf.trainable_variables()\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "optimizer = tf.train.AdamOptimizer(lr)\n",
    "train_op = optimizer.apply_gradients(zip(grads, tvars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/3000 (epoch 0), train_loss = 6.600, time/batch = 1.282\n",
      "1/3000 (epoch 0), train_loss = 5.830, time/batch = 0.686\n",
      "2/3000 (epoch 0), train_loss = 5.202, time/batch = 0.852\n",
      "3/3000 (epoch 0), train_loss = 5.082, time/batch = 0.675\n",
      "4/3000 (epoch 0), train_loss = 4.771, time/batch = 0.631\n",
      "5/3000 (epoch 0), train_loss = 4.308, time/batch = 0.623\n",
      "6/3000 (epoch 0), train_loss = 4.046, time/batch = 0.595\n",
      "7/3000 (epoch 0), train_loss = 3.888, time/batch = 0.610\n",
      "8/3000 (epoch 0), train_loss = 3.690, time/batch = 0.616\n",
      "9/3000 (epoch 0), train_loss = 3.539, time/batch = 0.640\n",
      "10/3000 (epoch 0), train_loss = 3.374, time/batch = 0.755\n",
      "11/3000 (epoch 0), train_loss = 3.339, time/batch = 0.650\n",
      "12/3000 (epoch 0), train_loss = 3.191, time/batch = 0.622\n",
      "13/3000 (epoch 0), train_loss = 3.179, time/batch = 0.587\n",
      "14/3000 (epoch 0), train_loss = 3.081, time/batch = 0.618\n",
      "15/3000 (epoch 0), train_loss = 3.085, time/batch = 0.601\n",
      "16/3000 (epoch 0), train_loss = 3.030, time/batch = 0.616\n",
      "17/3000 (epoch 0), train_loss = 3.000, time/batch = 0.597\n",
      "18/3000 (epoch 0), train_loss = 2.990, time/batch = 0.621\n",
      "19/3000 (epoch 0), train_loss = 2.973, time/batch = 0.618\n",
      "20/3000 (epoch 0), train_loss = 2.938, time/batch = 0.707\n",
      "21/3000 (epoch 0), train_loss = 2.951, time/batch = 0.681\n",
      "22/3000 (epoch 0), train_loss = 2.935, time/batch = 0.668\n",
      "23/3000 (epoch 0), train_loss = 2.861, time/batch = 0.683\n",
      "24/3000 (epoch 0), train_loss = 2.916, time/batch = 0.743\n",
      "25/3000 (epoch 0), train_loss = 2.861, time/batch = 0.615\n",
      "26/3000 (epoch 0), train_loss = 2.845, time/batch = 0.589\n",
      "27/3000 (epoch 0), train_loss = 2.837, time/batch = 0.606\n",
      "28/3000 (epoch 0), train_loss = 2.862, time/batch = 0.616\n",
      "29/3000 (epoch 0), train_loss = 2.832, time/batch = 0.615\n",
      "30/3000 (epoch 0), train_loss = 2.841, time/batch = 0.610\n",
      "31/3000 (epoch 0), train_loss = 2.809, time/batch = 0.623\n",
      "32/3000 (epoch 0), train_loss = 2.799, time/batch = 0.674\n",
      "33/3000 (epoch 0), train_loss = 2.747, time/batch = 0.634\n",
      "34/3000 (epoch 0), train_loss = 2.762, time/batch = 0.612\n",
      "35/3000 (epoch 0), train_loss = 2.780, time/batch = 0.617\n",
      "36/3000 (epoch 0), train_loss = 2.760, time/batch = 0.612\n",
      "37/3000 (epoch 0), train_loss = 2.751, time/batch = 0.602\n",
      "38/3000 (epoch 0), train_loss = 2.787, time/batch = 0.690\n",
      "39/3000 (epoch 0), train_loss = 2.753, time/batch = 0.612\n",
      "40/3000 (epoch 0), train_loss = 2.748, time/batch = 0.651\n",
      "41/3000 (epoch 0), train_loss = 2.716, time/batch = 0.617\n",
      "42/3000 (epoch 0), train_loss = 2.726, time/batch = 0.601\n",
      "43/3000 (epoch 0), train_loss = 2.697, time/batch = 0.597\n",
      "44/3000 (epoch 0), train_loss = 2.679, time/batch = 0.606\n",
      "45/3000 (epoch 0), train_loss = 2.673, time/batch = 0.623\n",
      "46/3000 (epoch 0), train_loss = 2.670, time/batch = 0.601\n",
      "47/3000 (epoch 0), train_loss = 2.633, time/batch = 0.607\n",
      "48/3000 (epoch 0), train_loss = 2.648, time/batch = 0.605\n",
      "49/3000 (epoch 0), train_loss = 2.637, time/batch = 0.582\n",
      "50/3000 (epoch 0), train_loss = 2.635, time/batch = 0.617\n",
      "model saved to saved/model.ckpt\n",
      "51/3000 (epoch 0), train_loss = 2.612, time/batch = 0.610\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-edb2ebb68d5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mfeed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_data\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mistate_cell0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstate0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mistate_cell1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstate1\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpstate_cell0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpstate_cell1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             print \"{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}\"                 .format(e * data_loader.batch_size + b,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 340\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    341\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m       results = self._do_run(handle, target_list, unique_fetches,\n\u001b[0;32m--> 564\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    565\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m       \u001b[0;31m# The movers are no longer used. Delete them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    635\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m--> 637\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m    638\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m    642\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStatusNotOK\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m       \u001b[0merror_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m    626\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m         return tf_session.TF_Run(\n\u001b[0;32m--> 628\u001b[0;31m             session, None, feed_dict, fetch_list, target_list, None)\n\u001b[0m\u001b[1;32m    629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_loader = DataLoader(batch_size=batch_size, tsteps=tsteps, scale_factor=data_scale)\n",
    "with tf.Session() as sess:\n",
    "    tf.initialize_all_variables().run()\n",
    "    saver = tf.train.Saver(tf.all_variables())\n",
    "    for e in xrange(num_epochs):\n",
    "        sess.run(tf.assign(lr, learning_rate * (decay_rate ** e)))\n",
    "        data_loader.reset_batch_pointer()\n",
    "        state0 = istate_cell0.eval()\n",
    "        state1 = istate_cell1.eval()\n",
    "        for b in xrange(data_loader.batch_size):\n",
    "            start = time.time()\n",
    "            x, y = data_loader.next_batch()\n",
    "            feed = {input_data: x, target_data: y, istate_cell0: state0, istate_cell1: state1}\n",
    "            train_loss, state0, state1, _ = sess.run([cost, pstate_cell0, pstate_cell1, train_op], feed)\n",
    "            end = time.time()\n",
    "            print \"{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}\" \\\n",
    "                .format(e * data_loader.batch_size + b,\n",
    "                        num_epochs * data_loader.batch_size,\n",
    "                        e, train_loss, end - start)\n",
    "            if (e * data_loader.batch_size + b) % save_every == 0 and ((e * data_loader.batch_size + b) > 0):\n",
    "                checkpoint_path = os.path.join('saved', 'model.ckpt')\n",
    "                saver.save(sess, checkpoint_path, global_step = e * data_loader.batch_size + b)\n",
    "                print \"model saved to {}\".format(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
