{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#general model params\n",
    "hidden = 128\n",
    "tsteps = 24\n",
    "nmixtures = 1\n",
    "nlayers = 2 #currently does nothing\n",
    "\n",
    "# window params\n",
    "kmixtures = 3\n",
    "alphabet = 2\n",
    "U_items = 3\n",
    "\n",
    "#training params\n",
    "generate = False\n",
    "batch_size = 100\n",
    "dropout_keep = 0.85\n",
    "num_epochs = 30\n",
    "\n",
    "grad_clip = 10.\n",
    "learning_rate = .002 #0.005\n",
    "decay_rate = .7\n",
    "save_every =100\n",
    "\n",
    "#data params\n",
    "data_scale=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthesize data (circles and squares with noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_circle(batch_size=25, cycle = 25):\n",
    "    tsteps=cycle\n",
    "    N = batch_size*tsteps + 1 #need one extra valus so that we can take differences\n",
    "    \n",
    "    #kernel for defining shape of training data (square)\n",
    "    theta_max = (math.pi*2)*N/cycle\n",
    "    thetas = np.reshape(np.linspace(0,theta_max,N),(1,N))\n",
    "    data = np.concatenate((np.cos(thetas), np.sin(thetas)),axis=0)\n",
    "\n",
    "#     data += np.random.uniform(-0.01, 0.01, (2,N))\n",
    "    data = np.concatenate((data,np.zeros_like(thetas)),axis=0)\n",
    "    data = data.T\n",
    "    \n",
    "    data = data[1:,:] - data[:-1,:] # data = data[1:,:]\n",
    "#     data = data[1:,:]\n",
    "    data[-1,-1] = 1\n",
    "    return data\n",
    "\n",
    "def get_square(batch_size=10, cycle = 25):\n",
    "    tsteps=cycle\n",
    "    N = batch_size*tsteps + 1 #need one extra valus so that we can take differences\n",
    "    \n",
    "    #kernel for defining shape of training data (square)\n",
    "    minv = -1\n",
    "    maxv = 1\n",
    "    side123 = cycle/4 ; side4 = tsteps -3*side123\n",
    "    v = np.linspace(minv,maxv,side123) #bottom side\n",
    "    v = np.concatenate((v, [maxv]*side123)) #right side\n",
    "    v = np.concatenate((v, np.linspace(maxv,minv,side123))) #top side\n",
    "    v = np.concatenate((v, [minv]*side4)) #left side\n",
    "    w = np.concatenate((v[-side123:],v[:-side123])) #shift y vector to make square\n",
    "    v = np.reshape(v,(1,cycle))\n",
    "    w = np.reshape(w,(1,cycle))\n",
    "    vw = np.concatenate((v, w),axis=0)\n",
    "    data = vw.T\n",
    "    data = np.matlib.repmat(vw.T, 1+N/tsteps,1) #repeat shape as many times as needed\n",
    "    data = np.reshape(data[:N,:],(N,-1)) #cut off so we have exactly N data points\n",
    "#     data += np.random.uniform(-0.01, 0.01, (N,2))\n",
    "    data = np.concatenate((data,np.zeros((data.shape[0],1))),axis=1)\n",
    "    data[-1,-1] = 1\n",
    "    \n",
    "    data = data[1:,:] - data[:-1,:] # data = data[1:,:]\n",
    "#     data = data[1:,:]\n",
    "    return np.flipud(data/2)\n",
    "\n",
    "def to3D(data,tsteps):\n",
    "    #convert to 2D vols of data\n",
    "    data = np.concatenate((data, np.zeros((1,3))),axis=0)\n",
    "    X = data[:-1,:]\n",
    "    X_vol = np.zeros((X.shape[0]/tsteps,tsteps,3))\n",
    "    for i in range(X.shape[0]/tsteps):\n",
    "        X_vol[i,:,:] = X[i*tsteps : (i+1)*tsteps,:]\n",
    "    Y = data[1:,:]\n",
    "    Y_vol = np.zeros((Y.shape[0]/tsteps,tsteps,3))\n",
    "    for i in range(Y.shape[0]/tsteps):\n",
    "        Y_vol[i,:,:] = Y[i*tsteps : (i+1)*tsteps,:]\n",
    "    return (X_vol, Y_vol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 24, 3)\n",
      "(100, 3, 2)\n",
      "[[ 0.  1.]\n",
      " [ 0.  1.]\n",
      " [ 0.  1.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKIAAACbCAYAAADhh0B+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1jIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x109c84490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_data_seq(U_items, alphabet, batch_size=10, cycle = 20):\n",
    "    data = np.zeros((0,3))\n",
    "    one_hots = np.zeros((0,U_items,alphabet))\n",
    "    for b in range(batch_size):\n",
    "        seq = np.random.randint(0,alphabet, (U_items))\n",
    "        one_hot = np.zeros((1,U_items,alphabet))\n",
    "        one_hot[0,np.arange(U_items),seq] = 1 #encode sequence in one-hot form\n",
    "        one_hots = np.concatenate((one_hots,one_hot),axis=0)\n",
    "        for i in range(U_items):\n",
    "            if seq[i] == 0:\n",
    "                circle = get_circle(batch_size=1, cycle = cycle)\n",
    "                data = np.concatenate((data, circle),axis=0)\n",
    "            elif seq[i] == 1:\n",
    "                square = get_square(batch_size=1, cycle = cycle)\n",
    "                data = np.concatenate((data, square),axis=0)\n",
    "            elif seq[i] == 2:\n",
    "                circle = get_circle(batch_size=1, cycle = cycle/2)\n",
    "                data = np.concatenate((data, circle),axis=0)\n",
    "                data = np.concatenate((data, circle),axis=0)\n",
    "            elif seq[i] == 3:\n",
    "                square = get_square(batch_size=1, cycle = cycle/2)\n",
    "                data = np.concatenate((data, square),axis=0)\n",
    "                data = np.concatenate((data, square),axis=0)\n",
    "            elif seq[i] == 4:\n",
    "                square = get_square(batch_size=1, cycle = cycle/2)\n",
    "                circle = get_circle(batch_size=1, cycle = cycle/2)\n",
    "                data = np.concatenate((data, square),axis=0)\n",
    "                data = np.concatenate((data, circle),axis=0)\n",
    "            elif seq[i] == 5:\n",
    "                square = get_square(batch_size=1, cycle = cycle/2)\n",
    "                circle = get_circle(batch_size=1, cycle = cycle/2)\n",
    "                data = np.concatenate((data, circle),axis=0)\n",
    "                data = np.concatenate((data, square),axis=0)\n",
    "    (X_vol,Y_vol) = to3D(data, U_items*cycle)\n",
    "    return X_vol, Y_vol, one_hots\n",
    "\n",
    "(X_vol,Y_vol,one_hots) = get_data_seq(U_items, alphabet, batch_size=batch_size, cycle = tsteps/U_items)\n",
    "\n",
    "print X_vol.shape\n",
    "print one_hots.shape\n",
    "print one_hots[0,:,:]\n",
    "r_ = Y_vol[0,:,:]\n",
    "r=r_\n",
    "r = np.cumsum(r_, axis=0)\n",
    "\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.plot(r[:,0], r[:,1],'r-')\n",
    "plt.title('Training data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, batch_size=50, tsteps=45, scale_factor = 1, limit = 500, U_items=1, alphabet=1):\n",
    "        self.batch_size = batch_size\n",
    "        self.tsteps = tsteps\n",
    "        self.scale_factor = scale_factor\n",
    "        self.alphabet = alphabet\n",
    "        self.U_items = U_items\n",
    "    def next_batch(self):\n",
    "        cycle = self.tsteps/self.U_items\n",
    "        (x_batch,y_batch,one_hots) = get_data_seq(self.U_items, self.alphabet, batch_size=self.batch_size, cycle=cycle)\n",
    "        x_batch[:,:,:2] /= self.scale_factor\n",
    "        y_batch[:,:,:2] /= self.scale_factor\n",
    "        return x_batch, y_batch, one_hots\n",
    "    def reset_batch_pointer(self):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 24, 3)\n",
      "(100, 3, 2)\n",
      "[[ 0.  1.]\n",
      " [ 1.  0.]\n",
      " [ 0.  1.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAKIAAACbCAYAAADhh0B+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1153e67d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_loader = DataLoader(batch_size=batch_size, tsteps=tsteps, scale_factor=1, U_items=U_items, alphabet=alphabet)\n",
    "x, y, c = data_loader.next_batch()\n",
    "\n",
    "print x.shape\n",
    "print c.shape\n",
    "print c[0,:,:]\n",
    "r_ = x[0,:,:]\n",
    "# print r_\n",
    "r=r_\n",
    "# r = np.cumsum(r_, axis=0)\n",
    "\n",
    "plt.figure(figsize=(2,2))\n",
    "plt.plot(r[:,0], r[:,1],'r-')\n",
    "plt.title('Training data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build deep recurrent model with MDN densecap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize LSTM cells and build first cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cell0 = tf.nn.rnn_cell.BasicLSTMCell(hidden, state_is_tuple=True)\n",
    "cell1 = tf.nn.rnn_cell.BasicLSTMCell(hidden, state_is_tuple=True)\n",
    "\n",
    "if (generate == False and dropout_keep < 1): # training mode\n",
    "    cell0 = tf.nn.rnn_cell.DropoutWrapper(cell0, output_keep_prob = dropout_keep)\n",
    "    cell1 = tf.nn.rnn_cell.DropoutWrapper(cell1, output_keep_prob = dropout_keep)\n",
    "\n",
    "input_data = tf.placeholder(dtype=tf.float32, shape=[None, tsteps, 3])\n",
    "target_data = tf.placeholder(dtype=tf.float32, shape=[None, tsteps, 3])\n",
    "zstate_cell0 = cell0.zero_state(batch_size=batch_size, dtype=tf.float32)\n",
    "zstate_cell1 = cell1.zero_state(batch_size=batch_size, dtype=tf.float32)\n",
    "\n",
    "#slice the input volume into separate vols for each tstep\n",
    "inputs = [tf.squeeze(input_, [1]) for input_ in tf.split(1, tsteps, input_data)]\n",
    "\n",
    "#build cell0 computational graph\n",
    "outs_cell0, pstate_cell0 = tf.nn.seq2seq.rnn_decoder(inputs, zstate_cell0, cell0, loop_function=None, scope='cell0')\n",
    "# out_cell0 = tf.reshape(tf.concat(1, outs_cell0), [-1, hidden]) #concat outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add \"character window\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def variable_summaries(var, name):\n",
    "    #Attach a lot of summaries to a Tensor.\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.scalar_summary('mean/' + name, mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_sum(tf.square(var - mean)))\n",
    "        tf.scalar_summary('sttdev/' + name, stddev)\n",
    "        tf.scalar_summary('max/' + name, tf.reduce_max(var))\n",
    "        tf.scalar_summary('min/' + name, tf.reduce_min(var))\n",
    "        tf.histogram_summary(name, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The character window $w_t$ is built from three parameters, $\\alpha$, $\\beta$, and $\\kappa$ as follows\n",
    "$$(\\hat \\alpha_t,\\hat \\beta_t, \\hat \\kappa_t)=W_{h^1 p}h_t^1+b_p$$\n",
    "\n",
    "$$\\alpha_t=\\exp (\\hat \\alpha_t) \\quad \\quad \\beta_t=\\exp (\\hat \\beta_t) \\quad \\quad \\kappa_t= \\kappa_{t-1} + \\exp (\\hat \\kappa_t)$$\n",
    "\n",
    "From these parameters we can construct the window as a convolution:\n",
    "$$w_t=\\sum_{u=1}^U \\phi(t,u)c_u \\quad \\quad \\phi(t,u)= \\sum_{k=1}^K \\alpha_t^k \\exp \\left( -\\beta_t^k(\\kappa_t^k-u)^2 \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_window(alpha, beta, kappa, c):\n",
    "    # phi -> [? x 1 x U_items] and is a tf matrix\n",
    "    # c -> [? x U_items x alphabet] and is a tf matrix\n",
    "    U_items = c.get_shape()[1].value #number of items in sequence\n",
    "    phi = get_phi(U_items, alpha, beta, kappa)\n",
    "    window = tf.batch_matmul(phi,c)\n",
    "    window = tf.squeeze(window, [1]) # window ~ [?,alphabet]\n",
    "    return window, phi\n",
    "    \n",
    "#get phi for all t,u (returns a [1 x tsteps] matrix) that defines the window\n",
    "def get_phi(U_items, alpha, beta, kappa):\n",
    "    # alpha, beta, kappa -> [?,kmixtures,1] and each is a tf variable\n",
    "    u = np.linspace(0,U_items-1,U_items) # weight all the U items in the sequence\n",
    "    kappa_term = tf.square( tf.sub(kappa,u))\n",
    "    exp_term = tf.mul(-beta,kappa_term)\n",
    "    phi_k = tf.mul(alpha, tf.exp(exp_term))\n",
    "    phi = tf.reduce_sum(phi_k,1, keep_dims=True)\n",
    "    return phi # phi ~ [?,1,U_items]\n",
    "    \n",
    "def get_window_params(i, out_cell0, kmixtures, prev_kappa, reuse=True):\n",
    "    hidden = out_cell0.get_shape()[1]\n",
    "    n_out = 3*kmixtures\n",
    "    with tf.variable_scope('window',reuse=reuse):\n",
    "        window_w = tf.get_variable(\"window_w\", [hidden, n_out])\n",
    "        variable_summaries(window_w, 'window_w_' + str(i) + '/weights')\n",
    "        window_b = tf.get_variable(\"window_b\", [n_out])\n",
    "    abk_hats = tf.nn.xw_plus_b(out_cell0, window_w, window_b) # abk_hats ~ [?,n_out]\n",
    "    abk = tf.exp(tf.reshape(abk_hats, [-1, 3*kmixtures,1]))\n",
    "    \n",
    "    alpha, beta, kappa = tf.split(1, 3, abk) # alpha_hat, etc ~ [?,kmixtures]\n",
    "    kappa = kappa/10 + prev_kappa\n",
    "    return alpha, beta, kappa # each ~ [?,kmixtures,1]\n",
    "\n",
    "init_kappa = tf.placeholder(dtype=tf.float32, shape=[None, kmixtures, 1])\n",
    "char_seq = tf.placeholder(dtype=tf.float32, shape=[None, U_items, alphabet])\n",
    "prev_kappa = init_kappa\n",
    "prev_window = char_seq[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#add gaussian window result\n",
    "reuse = False\n",
    "for i in range(len(outs_cell0)):\n",
    "    outs_cell0[i] = tf.concat(1, (outs_cell0[i],inputs[i])) #concat input data\n",
    "    outs_cell0[i] = tf.concat(1, (outs_cell0[i],prev_window)) #concat input data\n",
    "    [alpha, beta, new_kappa] = get_window_params(i, outs_cell0[i], kmixtures, prev_kappa, reuse=reuse)\n",
    "    window, phi = get_window(alpha, beta, new_kappa, char_seq)\n",
    "#     window = char_seq[:,i/(10),:]\n",
    "    outs_cell0[i] = tf.concat(1, (outs_cell0[i],window)) #concat outputs\n",
    "    if not reuse:\n",
    "        all_new_kappas = new_kappa\n",
    "    else:\n",
    "        all_new_kappas = tf.concat(2, (all_new_kappas,new_kappa))\n",
    "    prev_kappa = new_kappa\n",
    "    prev_window = window\n",
    "    reuse = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build graph for second LSTM cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_out = 1 + nmixtures * 6 # end_of_stroke + gaussian mixtures defining stroke locations\n",
    "\n",
    "#build cell1 computational graph\n",
    "outs_cell1, pstate_cell1 = tf.nn.seq2seq.rnn_decoder(outs_cell0, zstate_cell1, cell1, loop_function=None, scope='cell1')\n",
    "out_cell1 = tf.reshape(tf.concat(1, outs_cell1), [-1, hidden]) #concat outputs\n",
    "\n",
    "#put a dense cap on top of the rnn cells (to interface with the mixture density network)\n",
    "with tf.variable_scope('rnn_root'):\n",
    "    output_w = tf.get_variable(\"output_w\", [hidden, n_out])\n",
    "    output_b = tf.get_variable(\"output_b\", [n_out])\n",
    "\n",
    "#put dense cap on top of cell1\n",
    "output = tf.nn.xw_plus_b(out_cell1, output_w, output_b) #data flows through dense nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 2D gaussian looks like\n",
    "$\\mathcal{N}(x|\\mu,\\sigma,\\rho)=\\frac{1}{2\\pi\\sigma_1\\sigma_2\\sqrt(1-\\rho^2)}exp\\left[\\frac{-Z}{2(1-\\rho^2)}\\right]$ where $Z=\\frac{(x_1-\\mu_1)^2}{\\sigma_1^2}+\\frac{(x_2-\\mu_2)^2}{\\sigma_2^2}-\\frac{2\\rho(x_1-\\mu_1)(x_2-\\mu_2)}{\\sigma_1\\sigma_2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gaussian2d(x1, x2, mu1, mu2, s1, s2, rho):\n",
    "    # define gaussian mdn (eq 24, 25 from http://arxiv.org/abs/1308.0850)\n",
    "    x_mu1 = tf.sub(x1, mu1)\n",
    "    x_mu2 = tf.sub(x2, mu2)\n",
    "    Z = tf.square(tf.div(x_mu1, s1)) + \\\n",
    "        tf.square(tf.div(x_mu2, s2)) - \\\n",
    "        2*tf.div(tf.mul(rho, tf.mul(x_mu1, x_mu2)), tf.mul(s1, s2))\n",
    "    rho_square_term = 1-tf.square(rho)\n",
    "    power_e = tf.exp(tf.div(-Z,2*rho_square_term))\n",
    "    regularize_term = 2*np.pi*tf.mul(tf.mul(s1, s2), tf.sqrt(rho_square_term))\n",
    "    gaussian = tf.div(power_e, regularize_term)\n",
    "    return gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the mdn cap, our loss function becomes \n",
    "$$ \\mathcal{L}(x)=\\sum_{t=1}^{T} -log\\left(\\sum_{j} \\pi_t^j\\mathcal{N}(x_{t+1}|\\mu_t^j,\\sigma_t^j,\\rho_t^j)\n",
    "\\right)\n",
    "-\\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            \\log e_t & (x_{t+1})_3=1\\\\\n",
    "            \\log(1-e_t) & \\quad \\mathrm{otherwise}\n",
    "        \\end{array}\n",
    "    \\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_loss(pi, x1_data, x2_data, eos_data, mu1, mu2, sigma1, sigma2, rho, eos):\n",
    "    # define loss function (eq 26 of http://arxiv.org/abs/1308.0850)\n",
    "    gaussian = gaussian2d(x1_data, x2_data, mu1, mu2, sigma1, sigma2, rho)\n",
    "    term1 = tf.mul(gaussian, pi)\n",
    "    term1 = tf.reduce_sum(term1, 1, keep_dims=True) #do inner summation\n",
    "    term1 = -tf.log(tf.maximum(term1, 1e-20)) # some errors are zero -> numerical errors.\n",
    "\n",
    "    term2 = tf.mul(eos, eos_data) + tf.mul(1-eos, 1-eos_data) #modified Bernoulli -> eos probability\n",
    "    term2 = -tf.log(term2) #negative log error gives loss\n",
    "    \n",
    "    return tf.reduce_sum(term1 + term2) #do outer summation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gaussian mixture density network parameters are \n",
    "\n",
    "$$e_t=\\frac{1}{1+\\exp(\\hat e_t)} \\quad \\quad \\pi_t^j=\\frac{\\exp(\\hat \\pi_t^j)}{\\sum_{j'=1}^M\\exp(\\hat \\pi_t^{j'})} \\quad \\quad \\mu_t^j=\\hat \\mu_t^j \\quad \\quad \\sigma_t^j=\\exp(\\hat \\sigma_t^j)  \\quad \\quad  \\rho_t^j=\\tanh(\\hat \\rho_t^j)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# below is where we need to do MDN splitting of distribution params\n",
    "def get_mixture_coef(z):\n",
    "    # returns the tf slices containing mdn dist params (eq 18...23 of http://arxiv.org/abs/1308.0850)\n",
    "    z_eos = z[:, 0:1] #end of sentence tokens\n",
    "    z_pi, z_mu1, z_mu2, z_s1, z_s2, z_rho = tf.split(1, 6, z[:, 1:])\n",
    "    \n",
    "    # end of stroke signal\n",
    "    eos = tf.sigmoid(-1*z_eos) # technically we gained a negative sign\n",
    "\n",
    "    # softmax z_pi:\n",
    "    max_pi = tf.reduce_max(z_pi, 1, keep_dims=True)\n",
    "    z_pi = tf.exp( tf.sub(z_pi, max_pi) )\n",
    "    normalize_term = tf.inv(tf.reduce_sum(z_pi, 1, keep_dims=True))\n",
    "    pi = tf.mul(normalize_term, z_pi)\n",
    "    \n",
    "    #leave mu1, mu2 as they are\n",
    "    mu1 = z_mu1; mu2 = z_mu2\n",
    "    \n",
    "    # exp for sigmas\n",
    "    sigma1 = tf.exp(z_s1); sigma2 = tf.exp(z_s2)\n",
    "    \n",
    "    #tanh for rho (goes between -1 and 1)\n",
    "    rho = tf.tanh(z_rho)\n",
    "\n",
    "    return [eos, pi, mu1, mu2, sigma1, sigma2, rho]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reshape target data (as we did the input data)\n",
    "flat_target_data = tf.reshape(target_data,[-1, 3])\n",
    "[x1_data, x2_data, eos_data] = tf.split(1, 3, flat_target_data) #we might as well split these now\n",
    "\n",
    "[eos, pi, mu1, mu2, sigma1, sigma2, rho] = get_mixture_coef(output)\n",
    "\n",
    "loss = get_loss(pi, x1_data, x2_data, eos_data, mu1, mu2, sigma1, sigma2, rho, eos)\n",
    "cost = loss / (batch_size * tsteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model (build graph, then start session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#define how to train the model\n",
    "lr = tf.Variable(0.0, trainable=False)\n",
    "tvars = tf.trainable_variables()\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "optimizer = tf.train.AdamOptimizer(lr) #\n",
    "train_op = optimizer.apply_gradients(zip(grads, tvars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model:  saved/model.ckpt-200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception AssertionError: AssertionError(\"Nesting violated for default stack of <type 'weakref'> objects\",) in <bound method InteractiveSession.__del__ of <tensorflow.python.client.session.InteractiveSession object at 0x11ab49550>> ignored\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "try:\n",
    "    checkpoint_path = os.path.join('saved', 'model.ckpt')\n",
    "    sess = tf.InteractiveSession()\n",
    "    saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "    ckpt = tf.train.get_checkpoint_state('saved')\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "except:\n",
    "    print \"no saved model to load. starting new session\"\n",
    "    tf.initialize_all_variables().run()\n",
    "else:\n",
    "    print \"loaded model: \",ckpt.model_checkpoint_path\n",
    "\n",
    "merged = tf.merge_all_summaries()\n",
    "train_writer = tf.train.SummaryWriter('/tmp/scribe/train',sess.graph)\n",
    "saver = tf.train.Saver(tf.all_variables())\n",
    "data_loader = DataLoader(batch_size=batch_size, tsteps=tsteps, scale_factor=data_scale, U_items=U_items, alphabet=alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/3000 (epoch 0), train_loss = -2.243, time/batch = 0.804\n",
      "1/3000 (epoch 0), train_loss = -0.972, time/batch = 0.188\n",
      "2/3000 (epoch 0), train_loss = 1.886, time/batch = 0.325\n",
      "3/3000 (epoch 0), train_loss = -0.040, time/batch = 0.207\n",
      "4/3000 (epoch 0), train_loss = -2.207, time/batch = 0.169\n",
      "5/3000 (epoch 0), train_loss = -2.820, time/batch = 0.285\n",
      "6/3000 (epoch 0), train_loss = -2.878, time/batch = 0.277\n",
      "7/3000 (epoch 0), train_loss = -3.020, time/batch = 0.191\n",
      "8/3000 (epoch 0), train_loss = -2.873, time/batch = 0.203\n",
      "9/3000 (epoch 0), train_loss = -2.621, time/batch = 0.263\n",
      "10/3000 (epoch 0), train_loss = -3.017, time/batch = 0.177\n",
      "11/3000 (epoch 0), train_loss = -3.304, time/batch = 0.168\n",
      "12/3000 (epoch 0), train_loss = -3.326, time/batch = 0.220\n",
      "13/3000 (epoch 0), train_loss = -3.447, time/batch = 0.204\n",
      "14/3000 (epoch 0), train_loss = -3.365, time/batch = 0.177\n",
      "15/3000 (epoch 0), train_loss = -3.630, time/batch = 0.192\n",
      "16/3000 (epoch 0), train_loss = -3.700, time/batch = 0.183\n",
      "17/3000 (epoch 0), train_loss = -3.145, time/batch = 0.185\n",
      "18/3000 (epoch 0), train_loss = -3.321, time/batch = 0.173\n",
      "19/3000 (epoch 0), train_loss = -3.916, time/batch = 0.189\n",
      "20/3000 (epoch 0), train_loss = -2.976, time/batch = 0.200\n",
      "21/3000 (epoch 0), train_loss = -2.189, time/batch = 0.212\n",
      "22/3000 (epoch 0), train_loss = -2.895, time/batch = 0.195\n",
      "23/3000 (epoch 0), train_loss = -3.710, time/batch = 0.173\n",
      "24/3000 (epoch 0), train_loss = -3.308, time/batch = 0.167\n",
      "25/3000 (epoch 0), train_loss = -3.082, time/batch = 0.178\n",
      "26/3000 (epoch 0), train_loss = -3.698, time/batch = 0.196\n",
      "27/3000 (epoch 0), train_loss = -4.069, time/batch = 0.169\n",
      "28/3000 (epoch 0), train_loss = -4.270, time/batch = 0.183\n",
      "29/3000 (epoch 0), train_loss = -4.281, time/batch = 0.163\n",
      "30/3000 (epoch 0), train_loss = -4.376, time/batch = 0.164\n",
      "31/3000 (epoch 0), train_loss = -4.483, time/batch = 0.178\n",
      "32/3000 (epoch 0), train_loss = -4.482, time/batch = 0.167\n",
      "33/3000 (epoch 0), train_loss = -4.429, time/batch = 0.165\n",
      "34/3000 (epoch 0), train_loss = -4.436, time/batch = 0.166\n",
      "35/3000 (epoch 0), train_loss = -4.250, time/batch = 0.166\n",
      "36/3000 (epoch 0), train_loss = -4.316, time/batch = 0.164\n",
      "37/3000 (epoch 0), train_loss = -4.431, time/batch = 0.170\n",
      "38/3000 (epoch 0), train_loss = -3.502, time/batch = 0.179\n",
      "39/3000 (epoch 0), train_loss = -3.338, time/batch = 0.170\n",
      "40/3000 (epoch 0), train_loss = -3.285, time/batch = 0.168\n",
      "41/3000 (epoch 0), train_loss = -3.343, time/batch = 0.171\n",
      "42/3000 (epoch 0), train_loss = -4.447, time/batch = 0.178\n",
      "43/3000 (epoch 0), train_loss = -2.826, time/batch = 0.167\n",
      "44/3000 (epoch 0), train_loss = -1.615, time/batch = 0.172\n",
      "45/3000 (epoch 0), train_loss = -3.333, time/batch = 0.170\n",
      "46/3000 (epoch 0), train_loss = -4.039, time/batch = 0.170\n",
      "47/3000 (epoch 0), train_loss = -3.912, time/batch = 0.167\n",
      "48/3000 (epoch 0), train_loss = -4.548, time/batch = 0.166\n",
      "49/3000 (epoch 0), train_loss = -3.750, time/batch = 0.165\n",
      "50/3000 (epoch 0), train_loss = -3.156, time/batch = 0.169\n",
      "51/3000 (epoch 0), train_loss = -3.643, time/batch = 0.163\n",
      "52/3000 (epoch 0), train_loss = -4.064, time/batch = 0.166\n",
      "53/3000 (epoch 0), train_loss = -4.024, time/batch = 0.184\n",
      "54/3000 (epoch 0), train_loss = -4.131, time/batch = 0.166\n",
      "55/3000 (epoch 0), train_loss = -4.330, time/batch = 0.167\n",
      "56/3000 (epoch 0), train_loss = -3.961, time/batch = 0.168\n",
      "57/3000 (epoch 0), train_loss = -3.523, time/batch = 0.172\n",
      "58/3000 (epoch 0), train_loss = -3.896, time/batch = 0.167\n",
      "59/3000 (epoch 0), train_loss = -4.242, time/batch = 0.170\n",
      "60/3000 (epoch 0), train_loss = -3.954, time/batch = 0.168\n",
      "61/3000 (epoch 0), train_loss = -3.519, time/batch = 0.173\n",
      "62/3000 (epoch 0), train_loss = -3.730, time/batch = 0.170\n",
      "63/3000 (epoch 0), train_loss = -4.212, time/batch = 0.166\n",
      "64/3000 (epoch 0), train_loss = -3.868, time/batch = 0.167\n",
      "65/3000 (epoch 0), train_loss = -3.142, time/batch = 0.173\n",
      "66/3000 (epoch 0), train_loss = -3.351, time/batch = 0.168\n",
      "67/3000 (epoch 0), train_loss = -4.078, time/batch = 0.165\n",
      "68/3000 (epoch 0), train_loss = -4.371, time/batch = 0.168\n",
      "69/3000 (epoch 0), train_loss = -4.203, time/batch = 0.175\n",
      "70/3000 (epoch 0), train_loss = -4.265, time/batch = 0.177\n",
      "71/3000 (epoch 0), train_loss = -4.442, time/batch = 0.165\n",
      "72/3000 (epoch 0), train_loss = -4.169, time/batch = 0.186\n",
      "73/3000 (epoch 0), train_loss = -3.829, time/batch = 0.182\n",
      "74/3000 (epoch 0), train_loss = -4.130, time/batch = 0.176\n",
      "75/3000 (epoch 0), train_loss = -4.462, time/batch = 0.166\n",
      "76/3000 (epoch 0), train_loss = -4.310, time/batch = 0.201\n",
      "77/3000 (epoch 0), train_loss = -4.170, time/batch = 0.184\n",
      "78/3000 (epoch 0), train_loss = -4.338, time/batch = 0.176\n",
      "79/3000 (epoch 0), train_loss = -4.323, time/batch = 0.164\n",
      "80/3000 (epoch 0), train_loss = -4.322, time/batch = 0.189\n",
      "81/3000 (epoch 0), train_loss = -4.371, time/batch = 0.191\n",
      "82/3000 (epoch 0), train_loss = -4.452, time/batch = 0.167\n",
      "83/3000 (epoch 0), train_loss = -4.589, time/batch = 0.169\n",
      "84/3000 (epoch 0), train_loss = -4.564, time/batch = 0.169\n",
      "85/3000 (epoch 0), train_loss = -4.308, time/batch = 0.170\n",
      "86/3000 (epoch 0), train_loss = -4.593, time/batch = 0.174\n",
      "87/3000 (epoch 0), train_loss = -4.847, time/batch = 0.166\n",
      "88/3000 (epoch 0), train_loss = -4.170, time/batch = 0.169\n",
      "89/3000 (epoch 0), train_loss = -3.720, time/batch = 0.167\n",
      "90/3000 (epoch 0), train_loss = -3.847, time/batch = 0.166\n",
      "91/3000 (epoch 0), train_loss = -3.574, time/batch = 0.180\n",
      "92/3000 (epoch 0), train_loss = -3.191, time/batch = 0.181\n",
      "93/3000 (epoch 0), train_loss = -3.914, time/batch = 0.170\n",
      "94/3000 (epoch 0), train_loss = -4.713, time/batch = 0.184\n",
      "95/3000 (epoch 0), train_loss = -3.561, time/batch = 0.206\n",
      "96/3000 (epoch 0), train_loss = -2.337, time/batch = 0.167\n",
      "97/3000 (epoch 0), train_loss = -2.639, time/batch = 0.179\n",
      "98/3000 (epoch 0), train_loss = -3.922, time/batch = 0.179\n",
      "99/3000 (epoch 0), train_loss = -4.554, time/batch = 0.170\n",
      "100/3000 (epoch 1), train_loss = -3.435, time/batch = 0.162\n",
      "model saved to saved/model.ckpt\n",
      "101/3000 (epoch 1), train_loss = -4.539, time/batch = 0.176\n",
      "102/3000 (epoch 1), train_loss = -4.556, time/batch = 0.188\n",
      "103/3000 (epoch 1), train_loss = -4.589, time/batch = 0.188\n",
      "104/3000 (epoch 1), train_loss = -4.579, time/batch = 0.188\n",
      "105/3000 (epoch 1), train_loss = -4.664, time/batch = 0.188\n",
      "106/3000 (epoch 1), train_loss = -4.701, time/batch = 0.192\n",
      "107/3000 (epoch 1), train_loss = -4.652, time/batch = 0.182\n",
      "108/3000 (epoch 1), train_loss = -4.624, time/batch = 0.169\n",
      "109/3000 (epoch 1), train_loss = -4.666, time/batch = 0.181\n",
      "110/3000 (epoch 1), train_loss = -4.826, time/batch = 0.168\n",
      "111/3000 (epoch 1), train_loss = -4.872, time/batch = 0.174\n",
      "112/3000 (epoch 1), train_loss = -4.728, time/batch = 0.191\n",
      "113/3000 (epoch 1), train_loss = -4.801, time/batch = 0.172\n",
      "114/3000 (epoch 1), train_loss = -4.442, time/batch = 0.168\n",
      "115/3000 (epoch 1), train_loss = -4.382, time/batch = 0.181\n",
      "116/3000 (epoch 1), train_loss = -4.731, time/batch = 0.183\n",
      "117/3000 (epoch 1), train_loss = -4.709, time/batch = 0.181\n",
      "118/3000 (epoch 1), train_loss = -4.539, time/batch = 0.177\n",
      "119/3000 (epoch 1), train_loss = -3.946, time/batch = 0.180\n",
      "120/3000 (epoch 1), train_loss = -3.677, time/batch = 0.210\n",
      "121/3000 (epoch 1), train_loss = -3.956, time/batch = 0.238\n",
      "122/3000 (epoch 1), train_loss = -4.130, time/batch = 0.179\n",
      "123/3000 (epoch 1), train_loss = -4.213, time/batch = 0.188\n",
      "124/3000 (epoch 1), train_loss = -4.606, time/batch = 0.204\n",
      "125/3000 (epoch 1), train_loss = -4.734, time/batch = 0.196\n",
      "126/3000 (epoch 1), train_loss = -4.612, time/batch = 0.228\n",
      "127/3000 (epoch 1), train_loss = -4.801, time/batch = 0.276\n",
      "128/3000 (epoch 1), train_loss = -4.401, time/batch = 0.210\n",
      "129/3000 (epoch 1), train_loss = -3.939, time/batch = 0.179\n",
      "130/3000 (epoch 1), train_loss = -4.218, time/batch = 0.186\n",
      "131/3000 (epoch 1), train_loss = -4.641, time/batch = 0.229\n",
      "132/3000 (epoch 1), train_loss = -4.317, time/batch = 0.213\n",
      "133/3000 (epoch 1), train_loss = -3.782, time/batch = 0.207\n",
      "134/3000 (epoch 1), train_loss = -3.936, time/batch = 0.192\n",
      "135/3000 (epoch 1), train_loss = -4.282, time/batch = 0.216\n",
      "136/3000 (epoch 1), train_loss = -4.277, time/batch = 0.194\n",
      "137/3000 (epoch 1), train_loss = -4.003, time/batch = 0.201\n",
      "138/3000 (epoch 1), train_loss = -4.058, time/batch = 0.284\n",
      "139/3000 (epoch 1), train_loss = -4.455, time/batch = 0.205\n",
      "140/3000 (epoch 1), train_loss = -4.709, time/batch = 0.204\n",
      "141/3000 (epoch 1), train_loss = -4.634, time/batch = 0.248\n",
      "142/3000 (epoch 1), train_loss = -4.684, time/batch = 0.184\n",
      "143/3000 (epoch 1), train_loss = -4.710, time/batch = 0.180\n",
      "144/3000 (epoch 1), train_loss = -4.716, time/batch = 0.186\n",
      "145/3000 (epoch 1), train_loss = -4.767, time/batch = 0.207\n",
      "146/3000 (epoch 1), train_loss = -4.766, time/batch = 0.170\n",
      "147/3000 (epoch 1), train_loss = -4.761, time/batch = 0.174\n",
      "148/3000 (epoch 1), train_loss = -4.823, time/batch = 0.182\n",
      "149/3000 (epoch 1), train_loss = -4.896, time/batch = 0.172\n",
      "150/3000 (epoch 1), train_loss = -4.884, time/batch = 0.194\n",
      "151/3000 (epoch 1), train_loss = -4.790, time/batch = 0.187\n",
      "152/3000 (epoch 1), train_loss = -4.784, time/batch = 0.183\n",
      "153/3000 (epoch 1), train_loss = -4.928, time/batch = 0.204\n",
      "154/3000 (epoch 1), train_loss = -4.933, time/batch = 0.186\n",
      "155/3000 (epoch 1), train_loss = -4.916, time/batch = 0.189\n",
      "156/3000 (epoch 1), train_loss = -4.890, time/batch = 0.173\n",
      "157/3000 (epoch 1), train_loss = -4.815, time/batch = 0.235\n",
      "158/3000 (epoch 1), train_loss = -4.839, time/batch = 0.223\n",
      "159/3000 (epoch 1), train_loss = -4.881, time/batch = 0.171\n",
      "160/3000 (epoch 1), train_loss = -4.937, time/batch = 0.177\n",
      "161/3000 (epoch 1), train_loss = -4.910, time/batch = 0.201\n",
      "162/3000 (epoch 1), train_loss = -4.925, time/batch = 0.183\n",
      "163/3000 (epoch 1), train_loss = -4.828, time/batch = 0.172\n",
      "164/3000 (epoch 1), train_loss = -4.874, time/batch = 0.179\n",
      "165/3000 (epoch 1), train_loss = -4.922, time/batch = 0.325\n",
      "166/3000 (epoch 1), train_loss = -4.960, time/batch = 0.241\n",
      "167/3000 (epoch 1), train_loss = -4.791, time/batch = 0.309\n",
      "168/3000 (epoch 1), train_loss = -4.841, time/batch = 0.249\n",
      "169/3000 (epoch 1), train_loss = -4.974, time/batch = 0.243\n",
      "170/3000 (epoch 1), train_loss = -5.001, time/batch = 0.216\n",
      "171/3000 (epoch 1), train_loss = -4.809, time/batch = 0.251\n",
      "172/3000 (epoch 1), train_loss = -4.842, time/batch = 0.171\n",
      "173/3000 (epoch 1), train_loss = -5.008, time/batch = 0.168\n",
      "174/3000 (epoch 1), train_loss = -4.961, time/batch = 0.192\n",
      "175/3000 (epoch 1), train_loss = -4.831, time/batch = 0.208\n",
      "176/3000 (epoch 1), train_loss = -4.825, time/batch = 0.174\n",
      "177/3000 (epoch 1), train_loss = -4.906, time/batch = 0.267\n",
      "178/3000 (epoch 1), train_loss = -4.950, time/batch = 0.174\n",
      "179/3000 (epoch 1), train_loss = -4.956, time/batch = 0.177\n",
      "180/3000 (epoch 1), train_loss = -5.018, time/batch = 0.178\n",
      "181/3000 (epoch 1), train_loss = -4.854, time/batch = 0.191\n",
      "182/3000 (epoch 1), train_loss = -4.820, time/batch = 0.170\n",
      "183/3000 (epoch 1), train_loss = -4.960, time/batch = 0.173\n",
      "184/3000 (epoch 1), train_loss = -4.992, time/batch = 0.168\n",
      "185/3000 (epoch 1), train_loss = -4.841, time/batch = 0.169\n",
      "186/3000 (epoch 1), train_loss = -4.850, time/batch = 0.167\n",
      "187/3000 (epoch 1), train_loss = -4.937, time/batch = 0.168\n",
      "188/3000 (epoch 1), train_loss = -4.976, time/batch = 0.173\n",
      "189/3000 (epoch 1), train_loss = -4.825, time/batch = 0.167\n",
      "190/3000 (epoch 1), train_loss = -4.866, time/batch = 0.165\n",
      "191/3000 (epoch 1), train_loss = -4.891, time/batch = 0.167\n",
      "192/3000 (epoch 1), train_loss = -4.891, time/batch = 0.263\n",
      "193/3000 (epoch 1), train_loss = -4.998, time/batch = 0.268\n",
      "194/3000 (epoch 1), train_loss = -4.970, time/batch = 0.227\n",
      "195/3000 (epoch 1), train_loss = -4.766, time/batch = 0.181\n",
      "196/3000 (epoch 1), train_loss = -4.724, time/batch = 0.183\n",
      "197/3000 (epoch 1), train_loss = -5.007, time/batch = 0.211\n",
      "198/3000 (epoch 1), train_loss = -4.962, time/batch = 0.173\n",
      "199/3000 (epoch 1), train_loss = -4.918, time/batch = 0.191\n",
      "200/3000 (epoch 2), train_loss = -2.453, time/batch = 0.177\n",
      "model saved to saved/model.ckpt\n",
      "201/3000 (epoch 2), train_loss = -4.942, time/batch = 0.261\n",
      "202/3000 (epoch 2), train_loss = -4.964, time/batch = 0.232\n",
      "203/3000 (epoch 2), train_loss = -4.898, time/batch = 0.270\n",
      "204/3000 (epoch 2), train_loss = -4.874, time/batch = 0.260\n",
      "205/3000 (epoch 2), train_loss = -4.982, time/batch = 0.195\n",
      "206/3000 (epoch 2), train_loss = -4.956, time/batch = 0.179\n",
      "207/3000 (epoch 2), train_loss = -4.921, time/batch = 0.189\n",
      "208/3000 (epoch 2), train_loss = -5.023, time/batch = 0.226\n",
      "209/3000 (epoch 2), train_loss = -4.955, time/batch = 0.179\n",
      "210/3000 (epoch 2), train_loss = -4.916, time/batch = 0.172\n",
      "211/3000 (epoch 2), train_loss = -4.856, time/batch = 0.255\n",
      "212/3000 (epoch 2), train_loss = -4.813, time/batch = 0.223\n",
      "213/3000 (epoch 2), train_loss = -4.978, time/batch = 0.262\n",
      "214/3000 (epoch 2), train_loss = -4.795, time/batch = 0.264\n",
      "215/3000 (epoch 2), train_loss = -4.686, time/batch = 0.196\n",
      "216/3000 (epoch 2), train_loss = -4.852, time/batch = 0.177\n",
      "217/3000 (epoch 2), train_loss = -4.731, time/batch = 0.170\n",
      "218/3000 (epoch 2), train_loss = -4.639, time/batch = 0.270\n",
      "219/3000 (epoch 2), train_loss = -4.894, time/batch = 0.182\n",
      "220/3000 (epoch 2), train_loss = -4.837, time/batch = 0.173\n",
      "221/3000 (epoch 2), train_loss = -4.698, time/batch = 0.182\n",
      "222/3000 (epoch 2), train_loss = -4.858, time/batch = 0.273\n",
      "223/3000 (epoch 2), train_loss = -4.991, time/batch = 0.173\n",
      "224/3000 (epoch 2), train_loss = -4.872, time/batch = 0.176\n",
      "225/3000 (epoch 2), train_loss = -4.981, time/batch = 0.182\n",
      "226/3000 (epoch 2), train_loss = -4.947, time/batch = 0.197\n",
      "227/3000 (epoch 2), train_loss = -4.871, time/batch = 0.173\n",
      "228/3000 (epoch 2), train_loss = -4.997, time/batch = 0.172\n",
      "229/3000 (epoch 2), train_loss = -4.799, time/batch = 0.195\n",
      "230/3000 (epoch 2), train_loss = -4.639, time/batch = 0.185\n",
      "231/3000 (epoch 2), train_loss = -4.805, time/batch = 0.175\n",
      "232/3000 (epoch 2), train_loss = -4.990, time/batch = 0.209\n",
      "233/3000 (epoch 2), train_loss = -4.995, time/batch = 0.194\n",
      "234/3000 (epoch 2), train_loss = -5.049, time/batch = 0.165\n",
      "235/3000 (epoch 2), train_loss = -4.861, time/batch = 0.164\n",
      "236/3000 (epoch 2), train_loss = -4.733, time/batch = 0.210\n",
      "237/3000 (epoch 2), train_loss = -4.959, time/batch = 0.248\n",
      "238/3000 (epoch 2), train_loss = -4.906, time/batch = 0.231\n",
      "239/3000 (epoch 2), train_loss = -4.849, time/batch = 0.288\n",
      "240/3000 (epoch 2), train_loss = -5.030, time/batch = 0.247\n",
      "241/3000 (epoch 2), train_loss = -4.859, time/batch = 0.263\n",
      "242/3000 (epoch 2), train_loss = -4.674, time/batch = 0.173\n",
      "243/3000 (epoch 2), train_loss = -4.914, time/batch = 0.169\n",
      "244/3000 (epoch 2), train_loss = -5.003, time/batch = 0.261\n",
      "245/3000 (epoch 2), train_loss = -4.956, time/batch = 0.262\n",
      "246/3000 (epoch 2), train_loss = -5.010, time/batch = 0.184\n",
      "247/3000 (epoch 2), train_loss = -4.931, time/batch = 0.167\n",
      "248/3000 (epoch 2), train_loss = -4.869, time/batch = 0.190\n",
      "249/3000 (epoch 2), train_loss = -5.022, time/batch = 0.196\n",
      "250/3000 (epoch 2), train_loss = -4.923, time/batch = 0.279\n",
      "251/3000 (epoch 2), train_loss = -4.754, time/batch = 0.241\n",
      "252/3000 (epoch 2), train_loss = -4.948, time/batch = 0.185\n",
      "253/3000 (epoch 2), train_loss = -4.999, time/batch = 0.175\n",
      "254/3000 (epoch 2), train_loss = -4.892, time/batch = 0.194\n",
      "255/3000 (epoch 2), train_loss = -5.058, time/batch = 0.197\n",
      "256/3000 (epoch 2), train_loss = -4.964, time/batch = 0.171\n",
      "257/3000 (epoch 2), train_loss = -4.842, time/batch = 0.232\n",
      "258/3000 (epoch 2), train_loss = -4.979, time/batch = 0.229\n",
      "259/3000 (epoch 2), train_loss = -4.965, time/batch = 0.234\n",
      "260/3000 (epoch 2), train_loss = -4.903, time/batch = 0.273\n",
      "261/3000 (epoch 2), train_loss = -5.028, time/batch = 0.242\n",
      "262/3000 (epoch 2), train_loss = -4.925, time/batch = 0.268\n",
      "263/3000 (epoch 2), train_loss = -4.811, time/batch = 0.176\n",
      "264/3000 (epoch 2), train_loss = -4.942, time/batch = 0.189\n",
      "265/3000 (epoch 2), train_loss = -4.995, time/batch = 0.166\n",
      "266/3000 (epoch 2), train_loss = -4.957, time/batch = 0.187\n",
      "267/3000 (epoch 2), train_loss = -5.055, time/batch = 0.209\n",
      "268/3000 (epoch 2), train_loss = -4.847, time/batch = 0.172\n",
      "269/3000 (epoch 2), train_loss = -4.746, time/batch = 0.174\n",
      "270/3000 (epoch 2), train_loss = -4.956, time/batch = 0.178\n",
      "271/3000 (epoch 2), train_loss = -5.023, time/batch = 0.288\n",
      "272/3000 (epoch 2), train_loss = -4.961, time/batch = 0.179\n",
      "273/3000 (epoch 2), train_loss = -5.054, time/batch = 0.259\n",
      "274/3000 (epoch 2), train_loss = -4.863, time/batch = 0.233\n",
      "275/3000 (epoch 2), train_loss = -4.738, time/batch = 0.181\n",
      "276/3000 (epoch 2), train_loss = -4.974, time/batch = 0.193\n",
      "277/3000 (epoch 2), train_loss = -5.082, time/batch = 0.191\n",
      "278/3000 (epoch 2), train_loss = -4.960, time/batch = 0.200\n",
      "279/3000 (epoch 2), train_loss = -5.061, time/batch = 0.216\n",
      "280/3000 (epoch 2), train_loss = -4.881, time/batch = 0.240\n",
      "281/3000 (epoch 2), train_loss = -4.739, time/batch = 0.208\n",
      "282/3000 (epoch 2), train_loss = -4.941, time/batch = 0.294\n",
      "283/3000 (epoch 2), train_loss = -5.074, time/batch = 0.245\n",
      "284/3000 (epoch 2), train_loss = -4.995, time/batch = 0.328\n",
      "285/3000 (epoch 2), train_loss = -5.075, time/batch = 0.222\n",
      "286/3000 (epoch 2), train_loss = -4.904, time/batch = 0.169\n",
      "287/3000 (epoch 2), train_loss = -4.757, time/batch = 0.183\n",
      "288/3000 (epoch 2), train_loss = -4.985, time/batch = 0.194\n",
      "289/3000 (epoch 2), train_loss = -5.085, time/batch = 0.190\n",
      "290/3000 (epoch 2), train_loss = -5.015, time/batch = 0.186\n",
      "291/3000 (epoch 2), train_loss = -5.069, time/batch = 0.168\n",
      "292/3000 (epoch 2), train_loss = -4.905, time/batch = 0.173\n",
      "293/3000 (epoch 2), train_loss = -4.744, time/batch = 0.182\n",
      "294/3000 (epoch 2), train_loss = -4.960, time/batch = 0.274\n",
      "295/3000 (epoch 2), train_loss = -5.090, time/batch = 0.224\n",
      "296/3000 (epoch 2), train_loss = -4.977, time/batch = 0.294\n",
      "297/3000 (epoch 2), train_loss = -5.062, time/batch = 0.262\n",
      "298/3000 (epoch 2), train_loss = -4.955, time/batch = 0.233\n",
      "299/3000 (epoch 2), train_loss = -4.757, time/batch = 0.177\n",
      "300/3000 (epoch 3), train_loss = -2.950, time/batch = 0.248"
     ]
    }
   ],
   "source": [
    "for e in xrange(num_epochs):\n",
    "    sess.run(tf.assign(lr, learning_rate * (decay_rate ** e)))\n",
    "    data_loader.reset_batch_pointer()\n",
    "    c0, h0 = zstate_cell0.c.eval(), zstate_cell0.h.eval()\n",
    "    c1, h1 = zstate_cell1.c.eval(), zstate_cell1.h.eval()\n",
    "\n",
    "    kappa = np.zeros((batch_size, kmixtures, 1)) #this is a major problem for training\n",
    "    for b in xrange(data_loader.batch_size):\n",
    "        start = time.time()\n",
    "        i = e * data_loader.batch_size + b\n",
    "        x, y, c = data_loader.next_batch()\n",
    "\n",
    "        feed = {input_data: x, target_data: y, \\\n",
    "                zstate_cell0.c: c0, zstate_cell0.h: h0, zstate_cell1.c: c1, zstate_cell1.h: h1, \\\n",
    "                char_seq: c, init_kappa: kappa}\n",
    "        fetch = [merged, cost, \\\n",
    "                 pstate_cell0.c, pstate_cell0.h, pstate_cell1.c, pstate_cell1.h, \\\n",
    "                 train_op]\n",
    "        summary, train_loss, c0, h0, c1, c1, _ = sess.run(fetch, feed)\n",
    "        \n",
    "        train_writer.add_summary(summary, i)\n",
    "        end = time.time()\n",
    "#         print \"all_new_kappas: \", all_new_kappas_.shape, all_new_kappas_[0,:,0:5].T\n",
    "        print \"{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}\" \\\n",
    "            .format(i, num_epochs * data_loader.batch_size,\n",
    "                    e, train_loss, end - start)\n",
    "        if (e * data_loader.batch_size + b) % save_every == 0 and ((e * data_loader.batch_size + b) > 0):\n",
    "            checkpoint_path = os.path.join('saved', 'model.ckpt')\n",
    "            saver.save(sess, checkpoint_path, global_step = e * data_loader.batch_size + b)\n",
    "            print \"model saved to {}\".format(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
