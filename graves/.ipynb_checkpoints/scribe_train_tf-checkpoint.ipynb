{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#general model params\n",
    "hidden = 128\n",
    "tsteps = 51\n",
    "nmixtures = 20\n",
    "\n",
    "# window params\n",
    "kmixtures = 10\n",
    "alphabet = 2\n",
    "U_items = 3\n",
    "\n",
    "#training params\n",
    "generate = False\n",
    "batch_size = 100\n",
    "dropout_keep = 0.85\n",
    "num_epochs = 30\n",
    "\n",
    "grad_clip = 10.\n",
    "learning_rate = .008 #0.005\n",
    "decay_rate = .9\n",
    "save_every =100\n",
    "\n",
    "#data params\n",
    "data_scale=10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthesize data (circles and squares with noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_circle(batch_size=25, cycle = 25):\n",
    "    N = batch_size*cycle + 1 #need one extra value so that we can take differences between points\n",
    "    \n",
    "    # start of kernel for defining shape of training data (circle)\n",
    "    theta_max = (math.pi*2)*(N+1)/cycle\n",
    "    thetas = np.reshape(np.linspace(0,theta_max,N),(1,N))\n",
    "    data = np.concatenate((np.cos(thetas), np.sin(thetas)),axis=0) #two dimensional data\n",
    "    #end of kernel\n",
    "\n",
    "    data = np.concatenate((data,np.zeros_like(thetas)),axis=0) #eos dimension\n",
    "    data = data.T \n",
    "    \n",
    "    data = data[1:,:] - data[:-1,:] #take differences between points\n",
    "    data[-1,-1] = 1 #eos label\n",
    "    return data\n",
    "\n",
    "def get_square(batch_size, cycle):\n",
    "    N = batch_size*cycle + 1 #need one extra valus so that we can take differences\n",
    "    \n",
    "    # start of kernel for defining shape of training data (square)\n",
    "    minv = -1\n",
    "    maxv = 1\n",
    "    side123 = cycle/4 ; side4 = cycle - 3*side123\n",
    "    v = np.linspace(minv,maxv,side123) #bottom side\n",
    "    v = np.concatenate((v, [maxv]*side123)) #right side\n",
    "    v = np.concatenate((v, np.linspace(maxv,minv,side123))) #top side\n",
    "    v = np.concatenate((v, [minv]*side4)) #left side\n",
    "    w = np.concatenate((v[-side123:],v[:-side123])) #shift y vector to make square\n",
    "    v = np.reshape(v,(1,cycle))\n",
    "    w = np.reshape(w,(1,cycle))\n",
    "    vw = np.concatenate((v, w),axis=0)\n",
    "    data = vw.T\n",
    "    data = np.matlib.repmat(vw.T, 1+N/cycle,1) #repeat shape as many times as needed\n",
    "    data = np.reshape(data[:N,:],(N,-1)) #cut off so we have exactly N data points\n",
    "#     data += np.random.uniform(-0.01, 0.01, (N,2)) #maybe add noise in future\n",
    "    #end of kernel\n",
    "    \n",
    "    data = np.concatenate((data,np.zeros((data.shape[0],1))),axis=1) #eos dimension\n",
    "    data = data[1:,:] - data[:-1,:] # data = data[1:,:]\n",
    "    data[0,-1] = 1 #eos label\n",
    "    \n",
    "    return np.flipud(data) #just to make things different from the circles\n",
    "\n",
    "def to3D(data,tsteps):\n",
    "    #convert vectors to 3D vols of data (nbatches, tsteps, data dimensionality)\n",
    "    data = np.concatenate((data, np.zeros((1,3))),axis=0)\n",
    "    X = data[:-1,:]\n",
    "    X_vol = np.zeros((X.shape[0]/tsteps,tsteps,3))\n",
    "    for i in range(X.shape[0]/tsteps):\n",
    "        X_vol[i,:,:] = X[i*tsteps : (i+1)*tsteps,:]\n",
    "    Y = data[1:,:]\n",
    "    Y_vol = np.zeros((Y.shape[0]/tsteps,tsteps,3))\n",
    "    for i in range(Y.shape[0]/tsteps):\n",
    "        Y_vol[i,:,:] = Y[i*tsteps : (i+1)*tsteps,:]\n",
    "    return (X_vol, Y_vol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_data_seq(U_items, alphabet, batch_size, cycle):\n",
    "    data = np.zeros((0,3)) #data dimensionality of three\n",
    "    one_hots = np.zeros((0,U_items,alphabet))\n",
    "    for b in range(batch_size):\n",
    "        seq = np.random.randint(0,alphabet, (U_items))\n",
    "        one_hot = np.zeros((1,U_items,alphabet))\n",
    "        one_hot[0,np.arange(U_items),seq] = 1 #encode sequence in one-hot form\n",
    "        one_hots = np.concatenate((one_hots,one_hot),axis=0)\n",
    "        #build a set of data\n",
    "        for i in range(U_items):\n",
    "            if seq[i] == 0:\n",
    "                circle = get_circle(batch_size=1, cycle = cycle)\n",
    "                circle[0,0] += 2.5 #shift rightwards by 1.5 units\n",
    "                data = np.concatenate((data, circle),axis=0)\n",
    "            else:\n",
    "                square = get_square(batch_size=1, cycle = cycle)\n",
    "                square[0,0] += 2.5 #shift rightwards by 1.5 units\n",
    "                data = np.concatenate((data, square),axis=0)\n",
    "    (X_vol,Y_vol) = to3D(data, U_items*cycle)\n",
    "    return X_vol, Y_vol, one_hots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, batch_size=50, tsteps=45, scale_factor=1, U_items=1, alphabet=1):\n",
    "        self.batch_size = batch_size\n",
    "        self.tsteps = tsteps\n",
    "        self.scale_factor = scale_factor\n",
    "        self.alphabet = alphabet\n",
    "        self.U_items = U_items\n",
    "    def next_batch(self):\n",
    "        cycle = self.tsteps/self.U_items\n",
    "        (x_batch,y_batch,one_hots) = get_data_seq(self.U_items, self.alphabet, batch_size=self.batch_size, cycle=cycle)\n",
    "        x_batch[:,:,:2] /= self.scale_factor\n",
    "        y_batch[:,:,:2] /= self.scale_factor\n",
    "        return x_batch, y_batch, one_hots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def line_plot(strokes):\n",
    "    plt.figure(figsize=(4,2))\n",
    "    eos_preds = np.where(strokes[:,-1] == 1)\n",
    "    eos_preds = [0] + list(eos_preds[0]) + [-1] #add start and end indices\n",
    "    for i in range(len(eos_preds)-1):\n",
    "        start = eos_preds[i]+1\n",
    "        stop = eos_preds[i+1]\n",
    "#         print start, stop\n",
    "        plt.plot(strokes[start:stop,0], strokes[start:stop,1],'r-')\n",
    "    plt.title('Sample train data')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "    \n",
    "# test data loader\n",
    "data_loader = DataLoader(batch_size=batch_size, tsteps=tsteps, scale_factor=1, U_items=U_items, alphabet=alphabet)\n",
    "x, y, c = data_loader.next_batch()\n",
    "\n",
    "print x.shape\n",
    "print c.shape\n",
    "print c[0,:,:]\n",
    "r = x[0,:,:]\n",
    "r[:,:-1] = np.cumsum(r[:,:-1], axis=0)\n",
    "\n",
    "line_plot(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build deep recurrent model with MDN densecap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize LSTM cells and build first cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cell0 = tf.nn.rnn_cell.BasicLSTMCell(hidden, state_is_tuple=True)\n",
    "cell1 = tf.nn.rnn_cell.BasicLSTMCell(hidden, state_is_tuple=True)\n",
    "\n",
    "if (generate == False and dropout_keep < 1): # training mode\n",
    "    cell0 = tf.nn.rnn_cell.DropoutWrapper(cell0, output_keep_prob = dropout_keep)\n",
    "    cell1 = tf.nn.rnn_cell.DropoutWrapper(cell1, output_keep_prob = dropout_keep)\n",
    "\n",
    "input_data = tf.placeholder(dtype=tf.float32, shape=[None, tsteps, 3])\n",
    "target_data = tf.placeholder(dtype=tf.float32, shape=[None, tsteps, 3])\n",
    "zstate_cell0 = cell0.zero_state(batch_size=batch_size, dtype=tf.float32)\n",
    "zstate_cell1 = cell1.zero_state(batch_size=batch_size, dtype=tf.float32)\n",
    "\n",
    "#slice the input volume into separate vols for each tstep\n",
    "inputs = [tf.squeeze(input_, [1]) for input_ in tf.split(1, tsteps, input_data)]\n",
    "\n",
    "#build cell0 computational graph\n",
    "outs_cell0, pstate_cell0 = tf.nn.seq2seq.rnn_decoder(inputs, zstate_cell0, cell0, loop_function=None, scope='cell0')\n",
    "# out_cell0 = tf.reshape(tf.concat(1, outs_cell0), [-1, hidden]) #concat outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add \"character window\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def variable_summaries(var, name):\n",
    "    #Attach a lot of summaries to a Tensor.\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.scalar_summary('mean/' + name, mean)\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_sum(tf.square(var - mean)))\n",
    "        tf.scalar_summary('sttdev/' + name, stddev)\n",
    "        tf.scalar_summary('max/' + name, tf.reduce_max(var))\n",
    "        tf.scalar_summary('min/' + name, tf.reduce_min(var))\n",
    "        tf.histogram_summary(name, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The character window $w_t$ is built from three parameters, $\\alpha$, $\\beta$, and $\\kappa$ as follows\n",
    "$$(\\hat \\alpha_t,\\hat \\beta_t, \\hat \\kappa_t)=W_{h^1 p}h_t^1+b_p$$\n",
    "\n",
    "$$\\alpha_t=\\exp (\\hat \\alpha_t) \\quad \\quad \\beta_t=\\exp (\\hat \\beta_t) \\quad \\quad \\kappa_t= \\kappa_{t-1} + \\exp (\\hat \\kappa_t)$$\n",
    "\n",
    "From these parameters we can construct the window as a convolution:\n",
    "$$w_t=\\sum_{u=1}^U \\phi(t,u)c_u \\quad \\quad \\phi(t,u)= \\sum_{k=1}^K \\alpha_t^k \\exp \\left( -\\beta_t^k(\\kappa_t^k-u)^2 \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_window(alpha, beta, kappa, c):\n",
    "    # phi -> [? x 1 x U_items] and is a tf matrix\n",
    "    # c -> [? x U_items x alphabet] and is a tf matrix\n",
    "    U_items = c.get_shape()[1].value #number of items in sequence\n",
    "    phi = get_phi(U_items, alpha, beta, kappa)\n",
    "    window = tf.batch_matmul(phi,c)\n",
    "    window = tf.squeeze(window, [1]) # window ~ [?,alphabet]\n",
    "    return window, phi\n",
    "    \n",
    "#get phi for all t,u (returns a [1 x tsteps] matrix) that defines the window\n",
    "def get_phi(U_items, alpha, beta, kappa):\n",
    "    # alpha, beta, kappa -> [?,kmixtures,1] and each is a tf variable\n",
    "    u = np.linspace(0,U_items-1,U_items) # weight all the U items in the sequence\n",
    "    kappa_term = tf.square( tf.sub(kappa,u))\n",
    "    exp_term = tf.mul(-beta,kappa_term)\n",
    "    phi_k = tf.mul(alpha, tf.exp(exp_term))\n",
    "    phi = tf.reduce_sum(phi_k,1, keep_dims=True)\n",
    "    return phi # phi ~ [?,1,U_items]\n",
    "    \n",
    "def get_window_params(i, out_cell0, kmixtures, prev_kappa, reuse=True):\n",
    "    hidden = out_cell0.get_shape()[1]\n",
    "    n_out = 3*kmixtures\n",
    "    with tf.variable_scope('window',reuse=reuse):\n",
    "        tnormal = tf.truncated_normal_initializer(mean=-0.5, stddev=.25, seed=None, dtype=tf.float32)\n",
    "        window_w = tf.get_variable(\"window_w\", [hidden, n_out], initializer=tnormal)\n",
    "        variable_summaries(window_w, 'window_w_' + str(i) + '/weights')\n",
    "        window_b = tf.get_variable(\"window_b\", [n_out])\n",
    "    abk_hats = tf.nn.xw_plus_b(out_cell0, window_w, window_b) # abk_hats ~ [?,n_out]\n",
    "    abk = tf.exp(tf.reshape(abk_hats, [-1, 3*kmixtures,1]))\n",
    "    \n",
    "    alpha, beta, kappa = tf.split(1, 3, abk) # alpha_hat, etc ~ [?,kmixtures]\n",
    "    kappa = kappa/10 + prev_kappa\n",
    "    return alpha, beta, kappa # each ~ [?,kmixtures,1]\n",
    "\n",
    "init_kappa = tf.placeholder(dtype=tf.float32, shape=[None, kmixtures, 1])\n",
    "char_seq = tf.placeholder(dtype=tf.float32, shape=[None, U_items, alphabet])\n",
    "prev_kappa = init_kappa\n",
    "prev_window = char_seq[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#add gaussian window result\n",
    "reuse = False\n",
    "for i in range(len(outs_cell0)):\n",
    "    [alpha, beta, new_kappa] = get_window_params(i, outs_cell0[i], kmixtures, prev_kappa, reuse=reuse)\n",
    "    window, phi = get_window(alpha, beta, new_kappa, char_seq)\n",
    "    outs_cell0[i] = tf.concat(1, (outs_cell0[i],window)) #concat outputs\n",
    "    outs_cell0[i] = tf.concat(1, (outs_cell0[i],inputs[i])) #concat input data\n",
    "    outs_cell0[i] = tf.concat(1, (outs_cell0[i],prev_window)) #concat input data\n",
    "    prev_kappa = new_kappa\n",
    "    prev_window = window\n",
    "    reuse = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build graph for second LSTM cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_out = 1 + nmixtures * 6 # end_of_stroke + gaussian mixtures defining stroke locations\n",
    "\n",
    "#build cell1 computational graph\n",
    "outs_cell1, pstate_cell1 = tf.nn.seq2seq.rnn_decoder(outs_cell0, zstate_cell1, cell1, loop_function=None, scope='cell1')\n",
    "out_cell1 = tf.reshape(tf.concat(1, outs_cell1), [-1, hidden]) #concat outputs\n",
    "\n",
    "#put a dense cap on top of the rnn cells (to interface with the mixture density network)\n",
    "with tf.variable_scope('rnn_root'):\n",
    "    output_w = tf.get_variable(\"output_w\", [hidden, n_out])\n",
    "    output_b = tf.get_variable(\"output_b\", [n_out])\n",
    "\n",
    "#put dense cap on top of cell1\n",
    "output = tf.nn.xw_plus_b(out_cell1, output_w, output_b) #data flows through dense nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 2D gaussian looks like\n",
    "$\\mathcal{N}(x|\\mu,\\sigma,\\rho)=\\frac{1}{2\\pi\\sigma_1\\sigma_2\\sqrt(1-\\rho^2)}exp\\left[\\frac{-Z}{2(1-\\rho^2)}\\right]$ where $Z=\\frac{(x_1-\\mu_1)^2}{\\sigma_1^2}+\\frac{(x_2-\\mu_2)^2}{\\sigma_2^2}-\\frac{2\\rho(x_1-\\mu_1)(x_2-\\mu_2)}{\\sigma_1\\sigma_2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gaussian2d(x1, x2, mu1, mu2, s1, s2, rho):\n",
    "    # define gaussian mdn (eq 24, 25 from http://arxiv.org/abs/1308.0850)\n",
    "    x_mu1 = tf.sub(x1, mu1)\n",
    "    x_mu2 = tf.sub(x2, mu2)\n",
    "    Z = tf.square(tf.div(x_mu1, s1)) + \\\n",
    "        tf.square(tf.div(x_mu2, s2)) - \\\n",
    "        2*tf.div(tf.mul(rho, tf.mul(x_mu1, x_mu2)), tf.mul(s1, s2))\n",
    "    rho_square_term = 1-tf.square(rho)\n",
    "    power_e = tf.exp(tf.div(-Z,2*rho_square_term))\n",
    "    regularize_term = 2*np.pi*tf.mul(tf.mul(s1, s2), tf.sqrt(rho_square_term))\n",
    "    gaussian = tf.div(power_e, regularize_term)\n",
    "    return gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the mdn cap, our loss function becomes \n",
    "$$ \\mathcal{L}(x)=\\sum_{t=1}^{T} -log\\left(\\sum_{j} \\pi_t^j\\mathcal{N}(x_{t+1}|\\mu_t^j,\\sigma_t^j,\\rho_t^j)\n",
    "\\right)\n",
    "-\\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            \\log e_t & (x_{t+1})_3=1\\\\\n",
    "            \\log(1-e_t) & \\quad \\mathrm{otherwise}\n",
    "        \\end{array}\n",
    "    \\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_loss(pi, x1_data, x2_data, eos_data, mu1, mu2, sigma1, sigma2, rho, eos):\n",
    "    # define loss function (eq 26 of http://arxiv.org/abs/1308.0850)\n",
    "    gaussian = gaussian2d(x1_data, x2_data, mu1, mu2, sigma1, sigma2, rho)\n",
    "    term1 = tf.mul(gaussian, pi)\n",
    "    term1 = tf.reduce_sum(term1, 1, keep_dims=True) #do inner summation\n",
    "    term1 = -tf.log(tf.maximum(term1, 1e-20)) # some errors are zero -> numerical errors.\n",
    "\n",
    "    term2 = tf.mul(eos, eos_data) + tf.mul(1-eos, 1-eos_data) #modified Bernoulli -> eos probability\n",
    "    term2 = -tf.log(term2) #negative log error gives loss\n",
    "    \n",
    "    return tf.reduce_sum(term1 + term2) #do outer summation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gaussian mixture density network parameters are \n",
    "\n",
    "$$e_t=\\frac{1}{1+\\exp(\\hat e_t)} \\quad \\quad \\pi_t^j=\\frac{\\exp(\\hat \\pi_t^j)}{\\sum_{j'=1}^M\\exp(\\hat \\pi_t^{j'})} \\quad \\quad \\mu_t^j=\\hat \\mu_t^j \\quad \\quad \\sigma_t^j=\\exp(\\hat \\sigma_t^j)  \\quad \\quad  \\rho_t^j=\\tanh(\\hat \\rho_t^j)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# below is where we need to do MDN splitting of distribution params\n",
    "def get_mixture_coef(z):\n",
    "    # returns the tf slices containing mdn dist params (eq 18...23 of http://arxiv.org/abs/1308.0850)\n",
    "    z_eos = z[:, 0:1] #end of sentence tokens\n",
    "    z_pi, z_mu1, z_mu2, z_s1, z_s2, z_rho = tf.split(1, 6, z[:, 1:])\n",
    "    \n",
    "    # end of stroke signal\n",
    "    eos = tf.sigmoid(-1*z_eos) # technically we gained a negative sign\n",
    "\n",
    "    # softmax z_pi:\n",
    "    max_pi = tf.reduce_max(z_pi, 1, keep_dims=True)\n",
    "    z_pi = tf.exp( tf.sub(z_pi, max_pi) )\n",
    "    normalize_term = tf.inv(tf.reduce_sum(z_pi, 1, keep_dims=True))\n",
    "    pi = tf.mul(normalize_term, z_pi)\n",
    "    \n",
    "    #leave mu1, mu2 as they are\n",
    "    mu1 = z_mu1; mu2 = z_mu2\n",
    "    \n",
    "    # exp for sigmas\n",
    "    sigma1 = tf.exp(z_s1); sigma2 = tf.exp(z_s2)\n",
    "    \n",
    "    #tanh for rho (goes between -1 and 1)\n",
    "    rho = tf.tanh(z_rho)\n",
    "\n",
    "    return [eos, pi, mu1, mu2, sigma1, sigma2, rho]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reshape target data (as we did the input data)\n",
    "flat_target_data = tf.reshape(target_data,[-1, 3])\n",
    "[x1_data, x2_data, eos_data] = tf.split(1, 3, flat_target_data) #we might as well split these now\n",
    "\n",
    "[eos, pi, mu1, mu2, sigma1, sigma2, rho] = get_mixture_coef(output)\n",
    "\n",
    "loss = get_loss(pi, x1_data, x2_data, eos_data, mu1, mu2, sigma1, sigma2, rho, eos)\n",
    "cost = loss / (batch_size * tsteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model (build graph, then start session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#define how to train the model\n",
    "lr = tf.Variable(0.0, trainable=False)\n",
    "tvars = tf.trainable_variables()\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "optimizer = tf.train.AdamOptimizer(lr) #\n",
    "train_op = optimizer.apply_gradients(zip(grads, tvars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "try:\n",
    "    checkpoint_path = os.path.join('saved', 'model.ckpt')\n",
    "    sess = tf.InteractiveSession()\n",
    "    saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "    ckpt = tf.train.get_checkpoint_state('saved')\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "except:\n",
    "    print \"no saved model to load. starting new session\"\n",
    "    tf.initialize_all_variables().run()\n",
    "else:\n",
    "    print \"loaded model: \",ckpt.model_checkpoint_path\n",
    "\n",
    "merged = tf.merge_all_summaries()\n",
    "train_writer = tf.train.SummaryWriter('/tmp/scribe/train',sess.graph)\n",
    "saver = tf.train.Saver(tf.all_variables())\n",
    "data_loader = DataLoader(batch_size=batch_size, tsteps=tsteps, scale_factor=data_scale, U_items=U_items, alphabet=alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for e in xrange(num_epochs):\n",
    "    sess.run(tf.assign(lr, learning_rate * (decay_rate ** e)))\n",
    "#     data_loader.reset_batch_pointer()\n",
    "    c0, h0 = zstate_cell0.c.eval(), zstate_cell0.h.eval()\n",
    "    c1, h1 = zstate_cell1.c.eval(), zstate_cell1.h.eval()\n",
    "\n",
    "    kappa = np.zeros((batch_size, kmixtures, 1)) #this is a major problem for training\n",
    "    for b in xrange(data_loader.batch_size):\n",
    "        start = time.time()\n",
    "        i = e * data_loader.batch_size + b\n",
    "        x, y, c = data_loader.next_batch()\n",
    "\n",
    "        feed = {input_data: x, target_data: y, \\\n",
    "                zstate_cell0.c: c0, zstate_cell0.h: h0, zstate_cell1.c: c1, zstate_cell1.h: h1, \\\n",
    "                char_seq: c, init_kappa: kappa}\n",
    "        fetch = [merged, cost, \\\n",
    "                 pstate_cell0.c, pstate_cell0.h, pstate_cell1.c, pstate_cell1.h, \\\n",
    "                 train_op]\n",
    "        summary, train_loss, c0, h0, c1, c1, _ = sess.run(fetch, feed)\n",
    "        \n",
    "        train_writer.add_summary(summary, i)\n",
    "        end = time.time()\n",
    "#         print \"all_new_kappas: \", all_new_kappas_.shape, all_new_kappas_[0,:,0:5].T\n",
    "        print \"{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}\" \\\n",
    "            .format(i, num_epochs * data_loader.batch_size,\n",
    "                    e, train_loss, end - start)\n",
    "        if (e * data_loader.batch_size + b) % save_every == 0 and ((e * data_loader.batch_size + b) > 0):\n",
    "            checkpoint_path = os.path.join('saved', 'model.ckpt')\n",
    "            saver.save(sess, checkpoint_path, global_step = e * data_loader.batch_size + b)\n",
    "            print \"model saved to {}\".format(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
