{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [PROTOTYPE] Scribe: realistic handwriting with TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A deep learning project by Sam Greydanus. July 2016."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This version involves learning to draw a sequence of squares and circles. The next step is to train this same model on more complex data such as actual handwriting data like in the famous 2014 Alex Graves [paper](http://arxiv.org/abs/1308.0850).\n",
    "\n",
    "This iPython notebook (named **train_shapes**) demonstrates how to\n",
    "1. create training data\n",
    "2. build the model\n",
    "3. train and saves the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.matlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#general model params\n",
    "hidden = 128\n",
    "tsteps = 51\n",
    "nmixtures = 20\n",
    "\n",
    "# window params\n",
    "kmixtures = 10\n",
    "alphabet = 2\n",
    "U_items = 3\n",
    "\n",
    "#training params\n",
    "generate = False\n",
    "batch_size = 128\n",
    "dropout_keep = 0.85\n",
    "num_epochs = 15\n",
    "\n",
    "grad_clip = 10.\n",
    "learning_rate = .01\n",
    "decay_rate = .8 #decay learning rate quickly (we won't be training very long for this example)\n",
    "save_every =100\n",
    "\n",
    "#data params\n",
    "data_scale=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthesize data (just circles and squares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#make a circle\n",
    "def get_circle(batch_size=25, cycle = 25):\n",
    "    N = batch_size*cycle + 1 #need one extra value so that we can take differences between points\n",
    "    \n",
    "    # start of kernel for defining shape of training data (circle)\n",
    "    theta_max = (math.pi*2)*(N+1)/cycle\n",
    "    thetas = np.reshape(np.linspace(0,theta_max,N),(1,N))\n",
    "    data = np.concatenate((np.cos(thetas), np.sin(thetas)),axis=0) #two dimensional data\n",
    "    #end of kernel\n",
    "\n",
    "    data = np.concatenate((data,np.zeros_like(thetas)),axis=0) #eos dimension\n",
    "    data = data.T \n",
    "    \n",
    "    data = data[1:,:] - data[:-1,:] #take differences between points\n",
    "    data[-1,-1] = 1 #eos label\n",
    "    return data\n",
    "\n",
    "#make a square\n",
    "def get_square(batch_size, cycle):\n",
    "    N = batch_size*cycle + 1 #need one extra valus so that we can take differences\n",
    "    \n",
    "    # start of kernel for defining shape of training data (square)\n",
    "    minv = -1\n",
    "    maxv = 1\n",
    "    side123 = cycle/4 ; side4 = cycle - 3*side123\n",
    "    v = np.linspace(minv,maxv,side123) #bottom side\n",
    "    v = np.concatenate((v, [maxv]*side123)) #right side\n",
    "    v = np.concatenate((v, np.linspace(maxv,minv,side123))) #top side\n",
    "    v = np.concatenate((v, [minv]*side4)) #left side\n",
    "    w = np.concatenate((v[-side123:],v[:-side123])) #shift y vector to make square\n",
    "    v = np.reshape(v,(1,cycle))\n",
    "    w = np.reshape(w,(1,cycle))\n",
    "    vw = np.concatenate((v, w),axis=0)\n",
    "    data = vw.T\n",
    "    data = np.matlib.repmat(vw.T, 1+N/cycle,1) #repeat shape as many times as needed\n",
    "    data = np.reshape(data[:N,:],(N,-1)) #cut off so we have exactly N data points\n",
    "    \n",
    "    data = np.concatenate((data,np.zeros((data.shape[0],1))),axis=1) #eos dimension\n",
    "    data = data[1:,:] - data[:-1,:] # data = data[1:,:]\n",
    "    data[0,-1] = 1 #eos label\n",
    "    \n",
    "    return np.flipud(data) #just to make the square sequences more different from the circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#make a batched set of sequences of squares and circles\n",
    "def get_data_seq(U_items, alphabet, batch_size, cycle):\n",
    "    data = [] #split into batches\n",
    "    one_hots = []\n",
    "    for b in range(batch_size):\n",
    "        shapes = np.zeros((0,3))\n",
    "        seq = np.random.randint(0,alphabet, (U_items))\n",
    "        one_hot = np.zeros((U_items,alphabet))\n",
    "        one_hot[np.arange(U_items),seq] = 1 #encode sequence in one-hot form\n",
    "        one_hots.append(one_hot)\n",
    "        #choose either circle or square, slowly building up a sequence of the two\n",
    "        for i in range(U_items):\n",
    "            if seq[i] == 0:\n",
    "                shape = get_circle(batch_size=1, cycle = cycle)\n",
    "                if i is not 0:\n",
    "                    shape[0,0] += 2.5 #shift rightwards by 2.5 units\n",
    "            else:\n",
    "                shape = get_square(batch_size=1, cycle = cycle)\n",
    "                if i is not 0:\n",
    "                    shape[0,0] += 2.5 #shift rightwards by 2.5 units\n",
    "            shapes = np.concatenate((shapes, shape),axis=0)\n",
    "        data.append(shapes)\n",
    "    X_vol = [np.concatenate((np.zeros((1,3)),vec[:-1,:]),axis=0) for vec in data]\n",
    "    Y_vol = data\n",
    "    return X_vol, Y_vol, one_hots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#class wrapper for getting circle/square data\n",
    "class DataLoader():\n",
    "    def __init__(self, batch_size=50, tsteps=45, scale_factor=1, U_items=1, alphabet=1):\n",
    "        self.batch_size = batch_size\n",
    "        self.tsteps = tsteps\n",
    "        self.scale_factor = scale_factor\n",
    "        self.alphabet = alphabet\n",
    "        self.U_items = U_items\n",
    "    def next_batch(self):\n",
    "        cycle = self.tsteps/self.U_items\n",
    "        (x_batch,y_batch,one_hots) = get_data_seq(self.U_items, self.alphabet, batch_size=self.batch_size, cycle=cycle)\n",
    "        x_batch = [v/self.scale_factor for v in x_batch]\n",
    "        y_batch = [v/self.scale_factor for v in y_batch]\n",
    "        return x_batch, y_batch, one_hots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.]\n",
      " [ 1.  0.]\n",
      " [ 1.  0.]]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQ0AAACbCAYAAAB4Ws7iAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9lUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10fdf5c10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#function for plotting sequence data that has eos markers (1's and 0's in third vector dimension)\n",
    "def line_plot(strokes):\n",
    "    plt.figure(figsize=(4,2))\n",
    "    eos_preds = np.where(strokes[:,-1] == 1)\n",
    "    eos_preds = [0] + list(eos_preds[0]) + [-1] #add start and end indices\n",
    "    for i in range(len(eos_preds)-1):\n",
    "        start = eos_preds[i]+1\n",
    "        stop = eos_preds[i+1]\n",
    "#         print start, stop\n",
    "        plt.plot(strokes[start:stop,0], strokes[start:stop,1],'r-')\n",
    "    plt.title('Sample train data')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "    \n",
    "# test DataLoader class\n",
    "data_loader = DataLoader(batch_size=batch_size, tsteps=tsteps, scale_factor=data_scale, U_items=U_items, alphabet=alphabet)\n",
    "x, y, c = data_loader.next_batch()\n",
    "\n",
    "print c[0]\n",
    "r = y[0]\n",
    "r[:,:-1] = np.cumsum(r[:,:-1], axis=0)\n",
    "# print r\n",
    "line_plot(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build deep recurrent model with MDN densecap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize LSTM cells and build first cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cell0 = tf.nn.rnn_cell.BasicLSTMCell(hidden, state_is_tuple=True)\n",
    "cell1 = tf.nn.rnn_cell.BasicLSTMCell(hidden, state_is_tuple=True)\n",
    "\n",
    "if (generate == False and dropout_keep < 1): # training mode\n",
    "    cell0 = tf.nn.rnn_cell.DropoutWrapper(cell0, output_keep_prob = dropout_keep)\n",
    "    cell1 = tf.nn.rnn_cell.DropoutWrapper(cell1, output_keep_prob = dropout_keep)\n",
    "\n",
    "input_data = tf.placeholder(dtype=tf.float32, shape=[None, tsteps, 3])\n",
    "target_data = tf.placeholder(dtype=tf.float32, shape=[None, tsteps, 3])\n",
    "zstate_cell0 = cell0.zero_state(batch_size=batch_size, dtype=tf.float32)\n",
    "zstate_cell1 = cell1.zero_state(batch_size=batch_size, dtype=tf.float32)\n",
    "\n",
    "#slice the input volume into separate vols for each tstep\n",
    "inputs = [tf.squeeze(input_, [1]) for input_ in tf.split(1, tsteps, input_data)]\n",
    "\n",
    "#build cell0 computational graph\n",
    "outs_cell0, pstate_cell0 = tf.nn.seq2seq.rnn_decoder(inputs, zstate_cell0, cell0, loop_function=None, scope='cell0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add \"character window\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#add if you want visualizations with TensorBoard\n",
    "# def variable_summaries(var, name):\n",
    "#     #Attach a lot of summaries to a Tensor.\n",
    "#     with tf.name_scope('summaries'):\n",
    "#         mean = tf.reduce_mean(var)\n",
    "#         tf.scalar_summary('mean/' + name, mean)\n",
    "#         with tf.name_scope('stddev'):\n",
    "#             stddev = tf.sqrt(tf.reduce_sum(tf.square(var - mean)))\n",
    "#         tf.scalar_summary('sttdev/' + name, stddev)\n",
    "#         tf.scalar_summary('max/' + name, tf.reduce_max(var))\n",
    "#         tf.scalar_summary('min/' + name, tf.reduce_min(var))\n",
    "#         tf.histogram_summary(name, var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The character window $w_t$ is built from three parameters, $\\alpha$, $\\beta$, and $\\kappa$ as follows\n",
    "$$(\\hat \\alpha_t,\\hat \\beta_t, \\hat \\kappa_t)=W_{h^1 p}h_t^1+b_p$$\n",
    "\n",
    "$$\\alpha_t=\\exp (\\hat \\alpha_t) \\quad \\quad \\beta_t=\\exp (\\hat \\beta_t) \\quad \\quad \\kappa_t= \\kappa_{t-1} + \\exp (\\hat \\kappa_t)$$\n",
    "\n",
    "From these parameters we can construct the window as a convolution:\n",
    "$$w_t=\\sum_{u=1}^U \\phi(t,u)c_u \\quad \\quad \\phi(t,u)= \\sum_{k=1}^K \\alpha_t^k \\exp \\left( -\\beta_t^k(\\kappa_t^k-u)^2 \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_window(alpha, beta, kappa, c):\n",
    "    # phi -> [? x 1 x U_items] and is a tf matrix\n",
    "    # c -> [? x U_items x alphabet] and is a tf matrix\n",
    "    U_items = c.get_shape()[1].value #number of items in sequence\n",
    "    phi = get_phi(U_items, alpha, beta, kappa)\n",
    "    window = tf.batch_matmul(phi,c)\n",
    "    window = tf.squeeze(window, [1]) # window ~ [?,alphabet]\n",
    "    return window, phi\n",
    "    \n",
    "#get phi for all t,u (returns a [1 x tsteps] matrix) that defines the window\n",
    "def get_phi(U_items, alpha, beta, kappa):\n",
    "    # alpha, beta, kappa -> [?,kmixtures,1] and each is a tf variable\n",
    "    u = np.linspace(0,U_items-1,U_items) # weight all the U items in the sequence\n",
    "    kappa_term = tf.square( tf.sub(kappa,u))\n",
    "    exp_term = tf.mul(-beta,kappa_term)\n",
    "    phi_k = tf.mul(alpha, tf.exp(exp_term))\n",
    "    phi = tf.reduce_sum(phi_k,1, keep_dims=True)\n",
    "    return phi # phi ~ [?,1,U_items]\n",
    "    \n",
    "def get_window_params(i, out_cell0, kmixtures, prev_kappa, reuse=True):\n",
    "    hidden = out_cell0.get_shape()[1]\n",
    "    n_out = 3*kmixtures\n",
    "    with tf.variable_scope('window',reuse=reuse):\n",
    "        tnormal = tf.truncated_normal_initializer(mean=-0.5, stddev=.25, seed=None, dtype=tf.float32)\n",
    "        window_w = tf.get_variable(\"window_w\", [hidden, n_out], initializer=tnormal)\n",
    "#         #add next lineif you want visualizations with TensorBoard\n",
    "#         variable_summaries(window_w, 'window_w_' + str(i) + '/weights')\n",
    "        window_b = tf.get_variable(\"window_b\", [n_out])\n",
    "    abk_hats = tf.nn.xw_plus_b(out_cell0, window_w, window_b) # abk_hats ~ [?,n_out]\n",
    "    abk = tf.exp(tf.reshape(abk_hats, [-1, 3*kmixtures,1]))\n",
    "    \n",
    "    alpha, beta, kappa = tf.split(1, 3, abk) # alpha_hat, etc ~ [?,kmixtures]\n",
    "    kappa = kappa/10 + prev_kappa\n",
    "    return alpha, beta, kappa # each ~ [?,kmixtures,1]\n",
    "\n",
    "init_kappa = tf.placeholder(dtype=tf.float32, shape=[None, kmixtures, 1])\n",
    "char_seq = tf.placeholder(dtype=tf.float32, shape=[None, U_items, alphabet])\n",
    "prev_kappa = init_kappa\n",
    "prev_window = char_seq[:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#add gaussian window result\n",
    "reuse = False\n",
    "for i in range(len(outs_cell0)):\n",
    "    [alpha, beta, new_kappa] = get_window_params(i, outs_cell0[i], kmixtures, prev_kappa, reuse=reuse)\n",
    "    window, phi = get_window(alpha, beta, new_kappa, char_seq)\n",
    "    outs_cell0[i] = tf.concat(1, (outs_cell0[i],window)) #concat outputs\n",
    "    outs_cell0[i] = tf.concat(1, (outs_cell0[i],inputs[i])) #concat input data\n",
    "    outs_cell0[i] = tf.concat(1, (outs_cell0[i],prev_window)) #concat input data\n",
    "    prev_kappa = new_kappa\n",
    "    prev_window = window\n",
    "    reuse = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build graph for second LSTM cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_out = 1 + nmixtures * 6 # end_of_stroke + gaussian mixtures defining stroke locations\n",
    "\n",
    "#build cell1 computational graph\n",
    "outs_cell1, pstate_cell1 = tf.nn.seq2seq.rnn_decoder(outs_cell0, zstate_cell1, cell1, loop_function=None, scope='cell1')\n",
    "out_cell1 = tf.reshape(tf.concat(1, outs_cell1), [-1, hidden]) #concat outputs\n",
    "\n",
    "#put a dense cap on top of the rnn cells (to interface with the mixture density network)\n",
    "with tf.variable_scope('rnn_root'):\n",
    "    output_w = tf.get_variable(\"output_w\", [hidden, n_out])\n",
    "    output_b = tf.get_variable(\"output_b\", [n_out])\n",
    "\n",
    "#put dense cap on top of cell1\n",
    "output = tf.nn.xw_plus_b(out_cell1, output_w, output_b) #data flows through dense nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 2D gaussian looks like\n",
    "$\\mathcal{N}(x|\\mu,\\sigma,\\rho)=\\frac{1}{2\\pi\\sigma_1\\sigma_2\\sqrt(1-\\rho^2)}exp\\left[\\frac{-Z}{2(1-\\rho^2)}\\right]$ where $Z=\\frac{(x_1-\\mu_1)^2}{\\sigma_1^2}+\\frac{(x_2-\\mu_2)^2}{\\sigma_2^2}-\\frac{2\\rho(x_1-\\mu_1)(x_2-\\mu_2)}{\\sigma_1\\sigma_2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gaussian2d(x1, x2, mu1, mu2, s1, s2, rho):\n",
    "    # define gaussian mdn (eq 24, 25 from http://arxiv.org/abs/1308.0850)\n",
    "    x_mu1 = tf.sub(x1, mu1)\n",
    "    x_mu2 = tf.sub(x2, mu2)\n",
    "    Z = tf.square(tf.div(x_mu1, s1)) + \\\n",
    "        tf.square(tf.div(x_mu2, s2)) - \\\n",
    "        2*tf.div(tf.mul(rho, tf.mul(x_mu1, x_mu2)), tf.mul(s1, s2))\n",
    "    rho_square_term = 1-tf.square(rho)\n",
    "    power_e = tf.exp(tf.div(-Z,2*rho_square_term))\n",
    "    regularize_term = 2*np.pi*tf.mul(tf.mul(s1, s2), tf.sqrt(rho_square_term))\n",
    "    gaussian = tf.div(power_e, regularize_term)\n",
    "    return gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the mdn cap, our loss function becomes \n",
    "$$ \\mathcal{L}(x)=\\sum_{t=1}^{T} -log\\left(\\sum_{j} \\pi_t^j\\mathcal{N}(x_{t+1}|\\mu_t^j,\\sigma_t^j,\\rho_t^j)\n",
    "\\right)\n",
    "-\\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            \\log e_t & (x_{t+1})_3=1\\\\\n",
    "            \\log(1-e_t) & \\quad \\mathrm{otherwise}\n",
    "        \\end{array}\n",
    "    \\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_loss(pi, x1_data, x2_data, eos_data, mu1, mu2, sigma1, sigma2, rho, eos):\n",
    "    # define loss function (eq 26 of http://arxiv.org/abs/1308.0850)\n",
    "    gaussian = gaussian2d(x1_data, x2_data, mu1, mu2, sigma1, sigma2, rho)\n",
    "    term1 = tf.mul(gaussian, pi)\n",
    "    term1 = tf.reduce_sum(term1, 1, keep_dims=True) #do inner summation\n",
    "    term1 = -tf.log(tf.maximum(term1, 1e-20)) # some errors are zero -> numerical errors.\n",
    "\n",
    "    term2 = tf.mul(eos, eos_data) + tf.mul(1-eos, 1-eos_data) #modified Bernoulli -> eos probability\n",
    "    term2 = -tf.log(term2) #negative log error gives loss\n",
    "    \n",
    "    return tf.reduce_sum(term1 + term2) #do outer summation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gaussian mixture density network parameters are \n",
    "\n",
    "$$e_t=\\frac{1}{1+\\exp(\\hat e_t)} \\quad \\quad \\pi_t^j=\\frac{\\exp(\\hat \\pi_t^j)}{\\sum_{j'=1}^M\\exp(\\hat \\pi_t^{j'})} \\quad \\quad \\mu_t^j=\\hat \\mu_t^j \\quad \\quad \\sigma_t^j=\\exp(\\hat \\sigma_t^j)  \\quad \\quad  \\rho_t^j=\\tanh(\\hat \\rho_t^j)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# below is where we need to do MDN splitting of distribution params\n",
    "def get_mixture_coef(z):\n",
    "    # returns the tf slices containing mdn dist params (eq 18...23 of http://arxiv.org/abs/1308.0850)\n",
    "    z_eos = z[:, 0:1] #end of sentence tokens\n",
    "    z_pi, z_mu1, z_mu2, z_s1, z_s2, z_rho = tf.split(1, 6, z[:, 1:])\n",
    "    \n",
    "    # end of stroke signal\n",
    "    eos = tf.sigmoid(-1*z_eos) # technically we gained a negative sign\n",
    "\n",
    "    # softmax z_pi:\n",
    "    max_pi = tf.reduce_max(z_pi, 1, keep_dims=True)\n",
    "    z_pi = tf.exp( tf.sub(z_pi, max_pi) )\n",
    "    normalize_term = tf.inv(tf.reduce_sum(z_pi, 1, keep_dims=True))\n",
    "    pi = tf.mul(normalize_term, z_pi)\n",
    "    \n",
    "    #leave mu1, mu2 as they are\n",
    "    mu1 = z_mu1; mu2 = z_mu2\n",
    "    \n",
    "    # exp for sigmas\n",
    "    sigma1 = tf.exp(z_s1); sigma2 = tf.exp(z_s2)\n",
    "    \n",
    "    #tanh for rho (goes between -1 and 1)\n",
    "    rho = tf.tanh(z_rho)\n",
    "\n",
    "    return [eos, pi, mu1, mu2, sigma1, sigma2, rho]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# reshape target data (as we did the input data)\n",
    "flat_target_data = tf.reshape(target_data,[-1, 3])\n",
    "[x1_data, x2_data, eos_data] = tf.split(1, 3, flat_target_data) #we might as well split these now\n",
    "\n",
    "[eos, pi, mu1, mu2, sigma1, sigma2, rho] = get_mixture_coef(output)\n",
    "\n",
    "loss = get_loss(pi, x1_data, x2_data, eos_data, mu1, mu2, sigma1, sigma2, rho, eos)\n",
    "cost = loss / (batch_size * tsteps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model (build graph, then start session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#define how to train the model\n",
    "lr = tf.Variable(0.0, trainable=False)\n",
    "tvars = tf.trainable_variables()\n",
    "grads, _ = tf.clip_by_global_norm(tf.gradients(cost, tvars), grad_clip)\n",
    "optimizer = tf.train.AdamOptimizer(lr) #\n",
    "train_op = optimizer.apply_gradients(zip(grads, tvars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no saved model to load. starting new session\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception AssertionError: AssertionError(\"Nesting violated for default stack of <type 'weakref'> objects\",) in <bound method InteractiveSession.__del__ of <tensorflow.python.client.session.InteractiveSession object at 0x125707610>> ignored\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "try:\n",
    "    checkpoint_path = os.path.join('models', 'model.ckpt')\n",
    "    sess = tf.InteractiveSession()\n",
    "    saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "    ckpt = tf.train.get_checkpoint_state('models')\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "except:\n",
    "    print \"no saved model to load. starting new session\"\n",
    "    tf.initialize_all_variables().run()\n",
    "else:\n",
    "    print \"loaded model: \",ckpt.model_checkpoint_path\n",
    "\n",
    "merged = tf.merge_all_summaries()\n",
    "# #add if you want visualizations with TensorBoard\n",
    "# train_writer = tf.train.SummaryWriter('/tmp/scribe/train',sess.graph)\n",
    "saver = tf.train.Saver(tf.all_variables())\n",
    "data_loader = DataLoader(batch_size=batch_size, tsteps=tsteps, scale_factor=data_scale, U_items=U_items, alphabet=alphabet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0/1920 (epoch 0), train_loss = 4.227, time/batch = 3.359\n",
      "1/1920 (epoch 0), train_loss = 3.663, time/batch = 1.354\n",
      "2/1920 (epoch 0), train_loss = 2.623, time/batch = 0.938\n",
      "3/1920 (epoch 0), train_loss = 2.337, time/batch = 1.104\n",
      "4/1920 (epoch 0), train_loss = 1.760, time/batch = 1.159\n",
      "5/1920 (epoch 0), train_loss = 1.894, time/batch = 0.921\n",
      "6/1920 (epoch 0), train_loss = 1.457, time/batch = 0.927\n",
      "7/1920 (epoch 0), train_loss = 1.436, time/batch = 0.892\n",
      "8/1920 (epoch 0), train_loss = 1.337, time/batch = 1.055\n",
      "9/1920 (epoch 0), train_loss = 1.271, time/batch = 0.940\n",
      "10/1920 (epoch 0), train_loss = 1.434, time/batch = 1.018\n",
      "11/1920 (epoch 0), train_loss = 1.382, time/batch = 1.187\n",
      "12/1920 (epoch 0), train_loss = 1.319, time/batch = 0.908\n",
      "13/1920 (epoch 0), train_loss = 1.242, time/batch = 0.866\n",
      "14/1920 (epoch 0), train_loss = 1.266, time/batch = 0.817\n",
      "15/1920 (epoch 0), train_loss = 1.095, time/batch = 0.960\n",
      "16/1920 (epoch 0), train_loss = 1.137, time/batch = 0.916\n",
      "17/1920 (epoch 0), train_loss = 1.135, time/batch = 0.929\n",
      "18/1920 (epoch 0), train_loss = 1.098, time/batch = 0.949\n",
      "19/1920 (epoch 0), train_loss = 1.050, time/batch = 0.867\n",
      "20/1920 (epoch 0), train_loss = 0.971, time/batch = 0.850\n",
      "21/1920 (epoch 0), train_loss = 0.922, time/batch = 0.883\n",
      "22/1920 (epoch 0), train_loss = 0.910, time/batch = 0.939\n",
      "23/1920 (epoch 0), train_loss = 0.849, time/batch = 0.856\n",
      "24/1920 (epoch 0), train_loss = 0.778, time/batch = 0.842\n",
      "25/1920 (epoch 0), train_loss = 0.751, time/batch = 0.983\n",
      "26/1920 (epoch 0), train_loss = 0.725, time/batch = 0.975\n",
      "27/1920 (epoch 0), train_loss = 0.615, time/batch = 0.996\n",
      "28/1920 (epoch 0), train_loss = 0.539, time/batch = 1.129\n",
      "29/1920 (epoch 0), train_loss = 0.594, time/batch = 1.023\n",
      "30/1920 (epoch 0), train_loss = 0.731, time/batch = 1.140\n",
      "31/1920 (epoch 0), train_loss = 0.574, time/batch = 1.024\n",
      "32/1920 (epoch 0), train_loss = 0.514, time/batch = 1.221\n",
      "33/1920 (epoch 0), train_loss = 0.506, time/batch = 1.070\n",
      "34/1920 (epoch 0), train_loss = 0.388, time/batch = 1.074\n",
      "35/1920 (epoch 0), train_loss = 0.344, time/batch = 1.042\n",
      "36/1920 (epoch 0), train_loss = 0.273, time/batch = 1.037\n",
      "37/1920 (epoch 0), train_loss = 0.148, time/batch = 0.950\n",
      "38/1920 (epoch 0), train_loss = 0.228, time/batch = 1.070\n",
      "39/1920 (epoch 0), train_loss = 0.174, time/batch = 1.260\n",
      "40/1920 (epoch 0), train_loss = 0.026, time/batch = 1.179\n",
      "41/1920 (epoch 0), train_loss = -0.075, time/batch = 1.198\n",
      "42/1920 (epoch 0), train_loss = -0.083, time/batch = 1.265\n",
      "43/1920 (epoch 0), train_loss = -0.219, time/batch = 1.207\n",
      "44/1920 (epoch 0), train_loss = -0.339, time/batch = 1.258\n",
      "45/1920 (epoch 0), train_loss = -0.467, time/batch = 1.167\n",
      "46/1920 (epoch 0), train_loss = -0.571, time/batch = 1.006\n",
      "47/1920 (epoch 0), train_loss = -0.576, time/batch = 0.872\n",
      "48/1920 (epoch 0), train_loss = -0.577, time/batch = 0.860\n",
      "49/1920 (epoch 0), train_loss = -0.437, time/batch = 0.871\n",
      "50/1920 (epoch 0), train_loss = -0.287, time/batch = 1.203\n",
      "51/1920 (epoch 0), train_loss = -0.623, time/batch = 0.908\n",
      "52/1920 (epoch 0), train_loss = -0.611, time/batch = 0.867\n",
      "53/1920 (epoch 0), train_loss = -0.799, time/batch = 1.212\n",
      "54/1920 (epoch 0), train_loss = -0.775, time/batch = 1.054\n",
      "55/1920 (epoch 0), train_loss = -0.914, time/batch = 1.058\n",
      "56/1920 (epoch 0), train_loss = -1.022, time/batch = 0.920\n",
      "57/1920 (epoch 0), train_loss = -1.053, time/batch = 0.714\n",
      "58/1920 (epoch 0), train_loss = -1.063, time/batch = 0.650\n",
      "59/1920 (epoch 0), train_loss = -1.105, time/batch = 0.643\n",
      "60/1920 (epoch 0), train_loss = -1.157, time/batch = 0.757\n",
      "61/1920 (epoch 0), train_loss = -0.995, time/batch = 0.763\n",
      "62/1920 (epoch 0), train_loss = -1.121, time/batch = 0.699\n",
      "63/1920 (epoch 0), train_loss = -1.015, time/batch = 0.683\n",
      "64/1920 (epoch 0), train_loss = -1.128, time/batch = 0.708\n",
      "65/1920 (epoch 0), train_loss = -1.393, time/batch = 0.769\n",
      "66/1920 (epoch 0), train_loss = -1.448, time/batch = 0.661\n",
      "67/1920 (epoch 0), train_loss = -1.322, time/batch = 0.667\n",
      "68/1920 (epoch 0), train_loss = -1.516, time/batch = 0.641\n",
      "69/1920 (epoch 0), train_loss = -1.398, time/batch = 1.586\n",
      "70/1920 (epoch 0), train_loss = -1.676, time/batch = 1.567\n",
      "71/1920 (epoch 0), train_loss = -1.650, time/batch = 1.082\n",
      "72/1920 (epoch 0), train_loss = -1.751, time/batch = 1.175\n",
      "73/1920 (epoch 0), train_loss = -1.787, time/batch = 0.970\n",
      "74/1920 (epoch 0), train_loss = -1.768, time/batch = 0.889\n",
      "75/1920 (epoch 0), train_loss = -1.884, time/batch = 1.673\n",
      "76/1920 (epoch 0), train_loss = -1.866, time/batch = 1.332\n",
      "77/1920 (epoch 0), train_loss = -2.105, time/batch = 1.021\n",
      "78/1920 (epoch 0), train_loss = -2.004, time/batch = 0.990\n",
      "79/1920 (epoch 0), train_loss = -2.111, time/batch = 1.715\n",
      "80/1920 (epoch 0), train_loss = -2.163, time/batch = 1.699\n",
      "81/1920 (epoch 0), train_loss = -2.083, time/batch = 1.706\n",
      "82/1920 (epoch 0), train_loss = -2.233, time/batch = 1.516\n",
      "83/1920 (epoch 0), train_loss = -2.191, time/batch = 1.550\n",
      "84/1920 (epoch 0), train_loss = -2.326, time/batch = 1.277\n",
      "85/1920 (epoch 0), train_loss = -2.353, time/batch = 1.282\n",
      "86/1920 (epoch 0), train_loss = -2.313, time/batch = 0.988\n",
      "87/1920 (epoch 0), train_loss = -2.454, time/batch = 1.075\n",
      "88/1920 (epoch 0), train_loss = -2.560, time/batch = 1.021\n",
      "89/1920 (epoch 0), train_loss = -2.248, time/batch = 1.070\n",
      "90/1920 (epoch 0), train_loss = -2.555, time/batch = 1.122\n",
      "91/1920 (epoch 0), train_loss = -1.745, time/batch = 0.900\n",
      "92/1920 (epoch 0), train_loss = -1.821, time/batch = 0.795\n",
      "93/1920 (epoch 0), train_loss = -1.930, time/batch = 0.976\n",
      "94/1920 (epoch 0), train_loss = -2.122, time/batch = 0.789\n",
      "95/1920 (epoch 0), train_loss = -2.368, time/batch = 0.978\n",
      "96/1920 (epoch 0), train_loss = -2.449, time/batch = 0.764\n",
      "97/1920 (epoch 0), train_loss = -2.380, time/batch = 0.903\n",
      "98/1920 (epoch 0), train_loss = -2.523, time/batch = 0.839\n",
      "99/1920 (epoch 0), train_loss = -2.361, time/batch = 0.926\n",
      "100/1920 (epoch 0), train_loss = -2.422, time/batch = 0.978\n",
      "model saved to models/model.ckpt\n",
      "101/1920 (epoch 0), train_loss = -2.314, time/batch = 0.723\n",
      "102/1920 (epoch 0), train_loss = -2.438, time/batch = 0.681\n",
      "103/1920 (epoch 0), train_loss = -2.614, time/batch = 0.646\n",
      "104/1920 (epoch 0), train_loss = -2.681, time/batch = 0.716\n",
      "105/1920 (epoch 0), train_loss = -2.649, time/batch = 0.691\n",
      "106/1920 (epoch 0), train_loss = -2.783, time/batch = 0.677\n",
      "107/1920 (epoch 0), train_loss = -2.835, time/batch = 0.696\n",
      "108/1920 (epoch 0), train_loss = -2.851, time/batch = 0.731\n",
      "109/1920 (epoch 0), train_loss = -2.863, time/batch = 0.717\n",
      "110/1920 (epoch 0), train_loss = -2.852, time/batch = 0.671\n",
      "111/1920 (epoch 0), train_loss = -2.861, time/batch = 0.845\n",
      "112/1920 (epoch 0), train_loss = -2.767, time/batch = 0.752\n",
      "113/1920 (epoch 0), train_loss = -2.845, time/batch = 0.689\n",
      "114/1920 (epoch 0), train_loss = -2.877, time/batch = 0.714\n",
      "115/1920 (epoch 0), train_loss = -2.899, time/batch = 0.703\n",
      "116/1920 (epoch 0), train_loss = -3.054, time/batch = 0.637\n",
      "117/1920 (epoch 0), train_loss = -3.114, time/batch = 0.889\n",
      "118/1920 (epoch 0), train_loss = -2.881, time/batch = 0.745\n",
      "119/1920 (epoch 0), train_loss = -3.092, time/batch = 0.701\n",
      "120/1920 (epoch 0), train_loss = -2.848, time/batch = 0.670\n",
      "121/1920 (epoch 0), train_loss = -3.008, time/batch = 0.792\n",
      "122/1920 (epoch 0), train_loss = -3.148, time/batch = 0.799\n",
      "123/1920 (epoch 0), train_loss = -3.019, time/batch = 0.925\n",
      "124/1920 (epoch 0), train_loss = -3.233, time/batch = 0.881\n",
      "125/1920 (epoch 0), train_loss = -2.990, time/batch = 0.706\n",
      "126/1920 (epoch 0), train_loss = -3.165, time/batch = 0.735\n",
      "127/1920 (epoch 0), train_loss = -3.334, time/batch = 0.727\n",
      "128/1920 (epoch 1), train_loss = -1.982, time/batch = 0.701\n",
      "129/1920 (epoch 1), train_loss = -3.217, time/batch = 0.672\n",
      "130/1920 (epoch 1), train_loss = -3.327, time/batch = 0.666\n",
      "131/1920 (epoch 1), train_loss = -3.185, time/batch = 0.734\n",
      "132/1920 (epoch 1), train_loss = -3.407, time/batch = 0.801\n",
      "133/1920 (epoch 1), train_loss = -3.190, time/batch = 0.768\n",
      "134/1920 (epoch 1), train_loss = -3.010, time/batch = 0.686\n",
      "135/1920 (epoch 1), train_loss = -3.359, time/batch = 0.661\n",
      "136/1920 (epoch 1), train_loss = -3.355, time/batch = 0.638\n",
      "137/1920 (epoch 1), train_loss = -3.290, time/batch = 0.829\n",
      "138/1920 (epoch 1), train_loss = -3.505, time/batch = 1.016\n",
      "139/1920 (epoch 1), train_loss = -3.348, time/batch = 1.023\n",
      "140/1920 (epoch 1), train_loss = -3.333, time/batch = 0.899\n",
      "141/1920 (epoch 1), train_loss = -3.519, time/batch = 0.658\n",
      "142/1920 (epoch 1), train_loss = -3.297, time/batch = 1.026\n",
      "143/1920 (epoch 1), train_loss = -3.145, time/batch = 0.941\n",
      "144/1920 (epoch 1), train_loss = -3.339, time/batch = 0.951\n",
      "145/1920 (epoch 1), train_loss = -3.427, time/batch = 1.003\n",
      "146/1920 (epoch 1), train_loss = -3.436, time/batch = 0.929\n",
      "147/1920 (epoch 1), train_loss = -3.393, time/batch = 0.958\n",
      "148/1920 (epoch 1), train_loss = -3.308, time/batch = 0.840\n",
      "149/1920 (epoch 1), train_loss = -3.582, time/batch = 1.042\n",
      "150/1920 (epoch 1), train_loss = -3.449, time/batch = 0.859\n",
      "151/1920 (epoch 1), train_loss = -3.551, time/batch = 0.912\n",
      "152/1920 (epoch 1), train_loss = -3.593, time/batch = 0.938\n",
      "153/1920 (epoch 1), train_loss = -3.437, time/batch = 0.983\n",
      "154/1920 (epoch 1), train_loss = -3.717, time/batch = 0.884\n",
      "155/1920 (epoch 1), train_loss = -3.304, time/batch = 0.861\n",
      "156/1920 (epoch 1), train_loss = -3.295, time/batch = 1.117\n",
      "157/1920 (epoch 1), train_loss = -3.556, time/batch = 0.959\n",
      "158/1920 (epoch 1), train_loss = -3.692, time/batch = 0.865\n",
      "159/1920 (epoch 1), train_loss = -3.613, time/batch = 0.996\n",
      "160/1920 (epoch 1), train_loss = -3.672, time/batch = 0.877\n",
      "161/1920 (epoch 1), train_loss = -3.747, time/batch = 0.884\n",
      "162/1920 (epoch 1), train_loss = -3.693, time/batch = 0.836\n",
      "163/1920 (epoch 1), train_loss = -3.709, time/batch = 1.036\n",
      "164/1920 (epoch 1), train_loss = -3.785, time/batch = 0.905\n",
      "165/1920 (epoch 1), train_loss = -3.792, time/batch = 0.914\n",
      "166/1920 (epoch 1), train_loss = -3.805, time/batch = 0.970\n",
      "167/1920 (epoch 1), train_loss = -3.854, time/batch = 0.919\n",
      "168/1920 (epoch 1), train_loss = -3.651, time/batch = 0.889\n",
      "169/1920 (epoch 1), train_loss = -3.864, time/batch = 0.913\n",
      "170/1920 (epoch 1), train_loss = -3.496, time/batch = 1.139\n",
      "171/1920 (epoch 1), train_loss = -3.636, time/batch = 0.867\n",
      "172/1920 (epoch 1), train_loss = -3.631, time/batch = 0.902\n",
      "173/1920 (epoch 1), train_loss = -3.464, time/batch = 1.016\n",
      "174/1920 (epoch 1), train_loss = -3.720, time/batch = 0.901\n",
      "175/1920 (epoch 1), train_loss = -3.745, time/batch = 0.863\n",
      "176/1920 (epoch 1), train_loss = -3.718, time/batch = 0.792\n",
      "177/1920 (epoch 1), train_loss = -3.630, time/batch = 1.058\n",
      "178/1920 (epoch 1), train_loss = -3.649, time/batch = 1.001\n",
      "179/1920 (epoch 1), train_loss = -3.396, time/batch = 0.900\n",
      "180/1920 (epoch 1), train_loss = -3.543, time/batch = 0.819\n",
      "181/1920 (epoch 1), train_loss = -3.583, time/batch = 0.621\n",
      "182/1920 (epoch 1), train_loss = -3.760, time/batch = 0.649\n",
      "183/1920 (epoch 1), train_loss = -3.847, time/batch = 0.602\n",
      "184/1920 (epoch 1), train_loss = -3.799, time/batch = 0.944\n",
      "185/1920 (epoch 1), train_loss = -3.874, time/batch = 0.822\n",
      "186/1920 (epoch 1), train_loss = -3.842, time/batch = 0.844\n",
      "187/1920 (epoch 1), train_loss = -3.818, time/batch = 0.886\n",
      "188/1920 (epoch 1), train_loss = -3.817, time/batch = 0.853\n",
      "189/1920 (epoch 1), train_loss = -3.950, time/batch = 0.836\n",
      "190/1920 (epoch 1), train_loss = -3.575, time/batch = 0.814\n",
      "191/1920 (epoch 1), train_loss = -3.773, time/batch = 1.162\n",
      "192/1920 (epoch 1), train_loss = -3.519, time/batch = 0.933\n",
      "193/1920 (epoch 1), train_loss = -3.625, time/batch = 1.023\n",
      "194/1920 (epoch 1), train_loss = -4.010, time/batch = 0.928\n",
      "195/1920 (epoch 1), train_loss = -3.740, time/batch = 0.898\n",
      "196/1920 (epoch 1), train_loss = -3.954, time/batch = 0.868\n",
      "197/1920 (epoch 1), train_loss = -3.702, time/batch = 0.832\n",
      "198/1920 (epoch 1), train_loss = -3.542, time/batch = 0.996\n",
      "199/1920 (epoch 1), train_loss = -3.694, time/batch = 0.930\n",
      "200/1920 (epoch 1), train_loss = -3.973, time/batch = 0.878\n",
      "model saved to models/model.ckpt\n",
      "201/1920 (epoch 1), train_loss = -4.132, time/batch = 1.107\n",
      "202/1920 (epoch 1), train_loss = -3.944, time/batch = 0.903\n",
      "203/1920 (epoch 1), train_loss = -4.109, time/batch = 0.856\n",
      "204/1920 (epoch 1), train_loss = -3.685, time/batch = 1.001\n",
      "205/1920 (epoch 1), train_loss = -3.730, time/batch = 0.914\n",
      "206/1920 (epoch 1), train_loss = -4.171, time/batch = 0.970\n",
      "207/1920 (epoch 1), train_loss = -3.648, time/batch = 1.049\n",
      "208/1920 (epoch 1), train_loss = -4.059, time/batch = 0.927\n",
      "209/1920 (epoch 1), train_loss = -3.658, time/batch = 0.852\n",
      "210/1920 (epoch 1), train_loss = -3.590, time/batch = 0.826\n",
      "211/1920 (epoch 1), train_loss = -3.630, time/batch = 0.997\n",
      "212/1920 (epoch 1), train_loss = -3.772, time/batch = 0.831\n",
      "213/1920 (epoch 1), train_loss = -3.422, time/batch = 0.880\n",
      "214/1920 (epoch 1), train_loss = -3.620, time/batch = 0.943\n",
      "215/1920 (epoch 1), train_loss = -3.787, time/batch = 0.945\n",
      "216/1920 (epoch 1), train_loss = -3.723, time/batch = 0.899\n",
      "217/1920 (epoch 1), train_loss = -3.936, time/batch = 1.073\n",
      "218/1920 (epoch 1), train_loss = -3.811, time/batch = 1.114\n",
      "219/1920 (epoch 1), train_loss = -3.796, time/batch = 1.068\n",
      "220/1920 (epoch 1), train_loss = -3.929, time/batch = 0.916\n",
      "221/1920 (epoch 1), train_loss = -3.994, time/batch = 0.898\n",
      "222/1920 (epoch 1), train_loss = -4.040, time/batch = 0.973\n",
      "223/1920 (epoch 1), train_loss = -3.884, time/batch = 0.850\n",
      "224/1920 (epoch 1), train_loss = -4.108, time/batch = 0.804\n",
      "225/1920 (epoch 1), train_loss = -3.411, time/batch = 1.112\n",
      "226/1920 (epoch 1), train_loss = -3.314, time/batch = 0.882\n",
      "227/1920 (epoch 1), train_loss = -3.806, time/batch = 1.026\n",
      "228/1920 (epoch 1), train_loss = -3.930, time/batch = 0.968\n",
      "229/1920 (epoch 1), train_loss = -3.834, time/batch = 0.963\n",
      "230/1920 (epoch 1), train_loss = -3.982, time/batch = 0.895\n",
      "231/1920 (epoch 1), train_loss = -4.024, time/batch = 1.182\n",
      "232/1920 (epoch 1), train_loss = -4.008, time/batch = 1.070\n",
      "233/1920 (epoch 1), train_loss = -4.145, time/batch = 0.872\n",
      "234/1920 (epoch 1), train_loss = -3.747, time/batch = 0.895\n",
      "235/1920 (epoch 1), train_loss = -3.344, time/batch = 0.976\n",
      "236/1920 (epoch 1), train_loss = -3.752, time/batch = 0.904\n",
      "237/1920 (epoch 1), train_loss = -4.171, time/batch = 0.947\n",
      "238/1920 (epoch 1), train_loss = -3.752, time/batch = 0.840\n",
      "239/1920 (epoch 1), train_loss = -3.766, time/batch = 1.071\n",
      "240/1920 (epoch 1), train_loss = -3.403, time/batch = 0.875\n",
      "241/1920 (epoch 1), train_loss = -3.462, time/batch = 0.950\n",
      "242/1920 (epoch 1), train_loss = -3.780, time/batch = 1.076\n",
      "243/1920 (epoch 1), train_loss = -3.798, time/batch = 1.117\n",
      "244/1920 (epoch 1), train_loss = -3.993, time/batch = 1.022\n",
      "245/1920 (epoch 1), train_loss = -3.812, time/batch = 0.872\n",
      "246/1920 (epoch 1), train_loss = -3.882, time/batch = 1.108\n",
      "247/1920 (epoch 1), train_loss = -3.961, time/batch = 0.959\n",
      "248/1920 (epoch 1), train_loss = -3.679, time/batch = 1.100\n",
      "249/1920 (epoch 1), train_loss = -3.853, time/batch = 1.084\n",
      "250/1920 (epoch 1), train_loss = -3.742, time/batch = 0.920\n",
      "251/1920 (epoch 1), train_loss = -3.834, time/batch = 0.914\n",
      "252/1920 (epoch 1), train_loss = -3.986, time/batch = 0.885\n",
      "253/1920 (epoch 1), train_loss = -4.155, time/batch = 0.978\n",
      "254/1920 (epoch 1), train_loss = -4.187, time/batch = 0.911\n",
      "255/1920 (epoch 1), train_loss = -4.187, time/batch = 0.991\n",
      "256/1920 (epoch 2), train_loss = -3.577, time/batch = 1.255\n",
      "257/1920 (epoch 2), train_loss = -4.174, time/batch = 1.080\n",
      "258/1920 (epoch 2), train_loss = -4.251, time/batch = 0.947\n",
      "259/1920 (epoch 2), train_loss = -4.128, time/batch = 1.144\n",
      "260/1920 (epoch 2), train_loss = -4.225, time/batch = 0.887\n",
      "261/1920 (epoch 2), train_loss = -4.132, time/batch = 1.091\n",
      "262/1920 (epoch 2), train_loss = -3.939, time/batch = 0.994\n",
      "263/1920 (epoch 2), train_loss = -3.928, time/batch = 0.842\n",
      "264/1920 (epoch 2), train_loss = -4.302, time/batch = 0.988\n",
      "265/1920 (epoch 2), train_loss = -4.107, time/batch = 0.846\n",
      "266/1920 (epoch 2), train_loss = -3.937, time/batch = 1.126\n",
      "267/1920 (epoch 2), train_loss = -4.331, time/batch = 0.881\n",
      "268/1920 (epoch 2), train_loss = -4.123, time/batch = 1.119\n",
      "269/1920 (epoch 2), train_loss = -4.138, time/batch = 1.168\n",
      "270/1920 (epoch 2), train_loss = -4.273, time/batch = 1.186\n",
      "271/1920 (epoch 2), train_loss = -4.282, time/batch = 0.945\n",
      "272/1920 (epoch 2), train_loss = -4.139, time/batch = 1.058\n",
      "273/1920 (epoch 2), train_loss = -4.283, time/batch = 1.170\n",
      "274/1920 (epoch 2), train_loss = -4.426, time/batch = 1.185\n",
      "275/1920 (epoch 2), train_loss = -4.396, time/batch = 0.884\n",
      "276/1920 (epoch 2), train_loss = -4.333, time/batch = 0.987\n",
      "277/1920 (epoch 2), train_loss = -4.058, time/batch = 0.974\n",
      "278/1920 (epoch 2), train_loss = -4.330, time/batch = 0.878\n",
      "279/1920 (epoch 2), train_loss = -4.416, time/batch = 0.919\n",
      "280/1920 (epoch 2), train_loss = -4.459, time/batch = 1.125\n",
      "281/1920 (epoch 2), train_loss = -4.422, time/batch = 1.052\n",
      "282/1920 (epoch 2), train_loss = -3.482, time/batch = 1.090\n",
      "283/1920 (epoch 2), train_loss = -3.665, time/batch = 1.208\n",
      "284/1920 (epoch 2), train_loss = -3.376, time/batch = 0.859\n",
      "285/1920 (epoch 2), train_loss = -3.775, time/batch = 1.153\n",
      "286/1920 (epoch 2), train_loss = -4.130, time/batch = 0.978\n",
      "287/1920 (epoch 2), train_loss = -4.161, time/batch = 1.015\n",
      "288/1920 (epoch 2), train_loss = -4.273, time/batch = 0.880\n",
      "289/1920 (epoch 2), train_loss = -4.467, time/batch = 0.925\n",
      "290/1920 (epoch 2), train_loss = -4.335, time/batch = 0.892\n",
      "291/1920 (epoch 2), train_loss = -4.347, time/batch = 0.896\n",
      "292/1920 (epoch 2), train_loss = -4.464, time/batch = 0.842\n",
      "293/1920 (epoch 2), train_loss = -4.526, time/batch = 0.959\n",
      "294/1920 (epoch 2), train_loss = -4.493, time/batch = 1.038\n",
      "295/1920 (epoch 2), train_loss = -4.546, time/batch = 0.973\n",
      "296/1920 (epoch 2), train_loss = -4.626, time/batch = 1.209\n",
      "297/1920 (epoch 2), train_loss = -4.585, time/batch = 0.948\n",
      "298/1920 (epoch 2), train_loss = -4.464, time/batch = 1.170\n",
      "299/1920 (epoch 2), train_loss = -4.578, time/batch = 0.911\n",
      "300/1920 (epoch 2), train_loss = -4.713, time/batch = 0.884\n",
      "model saved to models/model.ckpt\n",
      "301/1920 (epoch 2), train_loss = -4.342, time/batch = 0.847\n",
      "302/1920 (epoch 2), train_loss = -4.518, time/batch = 0.930\n",
      "303/1920 (epoch 2), train_loss = -4.521, time/batch = 0.838\n",
      "304/1920 (epoch 2), train_loss = -4.300, time/batch = 1.091\n",
      "305/1920 (epoch 2), train_loss = -4.336, time/batch = 1.024\n",
      "306/1920 (epoch 2), train_loss = -4.429, time/batch = 1.045\n",
      "307/1920 (epoch 2), train_loss = -4.458, time/batch = 0.913\n",
      "308/1920 (epoch 2), train_loss = -4.710, time/batch = 1.027\n",
      "309/1920 (epoch 2), train_loss = -4.359, time/batch = 0.995\n",
      "310/1920 (epoch 2), train_loss = -4.411, time/batch = 0.924\n",
      "311/1920 (epoch 2), train_loss = -4.765, time/batch = 1.070\n",
      "312/1920 (epoch 2), train_loss = -4.204, time/batch = 0.850\n",
      "313/1920 (epoch 2), train_loss = -4.425, time/batch = 0.874\n",
      "314/1920 (epoch 2), train_loss = -4.058, time/batch = 1.221\n",
      "315/1920 (epoch 2), train_loss = -4.266, time/batch = 1.038\n",
      "316/1920 (epoch 2), train_loss = -4.263, time/batch = 0.974\n",
      "317/1920 (epoch 2), train_loss = -4.354, time/batch = 0.841\n",
      "318/1920 (epoch 2), train_loss = -4.642, time/batch = 0.657\n",
      "319/1920 (epoch 2), train_loss = -4.543, time/batch = 0.638\n",
      "320/1920 (epoch 2), train_loss = -4.605, time/batch = 0.584\n",
      "321/1920 (epoch 2), train_loss = -4.718, time/batch = 1.112\n",
      "322/1920 (epoch 2), train_loss = -4.653, time/batch = 0.967\n",
      "323/1920 (epoch 2), train_loss = -4.625, time/batch = 0.933\n",
      "324/1920 (epoch 2), train_loss = -4.810, time/batch = 1.018\n",
      "325/1920 (epoch 2), train_loss = -4.699, time/batch = 1.039\n",
      "326/1920 (epoch 2), train_loss = -4.722, time/batch = 0.969\n",
      "327/1920 (epoch 2), train_loss = -4.633, time/batch = 1.025\n",
      "328/1920 (epoch 2), train_loss = -4.678, time/batch = 1.162\n",
      "329/1920 (epoch 2), train_loss = -4.581, time/batch = 0.881\n",
      "330/1920 (epoch 2), train_loss = -4.621, time/batch = 0.921\n",
      "331/1920 (epoch 2), train_loss = -4.375, time/batch = 0.981\n",
      "332/1920 (epoch 2), train_loss = -4.629, time/batch = 0.782\n",
      "333/1920 (epoch 2), train_loss = -4.744, time/batch = 0.862\n",
      "334/1920 (epoch 2), train_loss = -4.548, time/batch = 0.945\n",
      "335/1920 (epoch 2), train_loss = -4.604, time/batch = 0.867\n",
      "336/1920 (epoch 2), train_loss = -4.657, time/batch = 0.833\n",
      "337/1920 (epoch 2), train_loss = -4.703, time/batch = 0.913\n",
      "338/1920 (epoch 2), train_loss = -4.596, time/batch = 1.122\n",
      "339/1920 (epoch 2), train_loss = -4.747, time/batch = 0.933\n",
      "340/1920 (epoch 2), train_loss = -4.816, time/batch = 0.946\n",
      "341/1920 (epoch 2), train_loss = -4.661, time/batch = 0.975\n",
      "342/1920 (epoch 2), train_loss = -4.784, time/batch = 1.046\n",
      "343/1920 (epoch 2), train_loss = -4.892, time/batch = 0.956\n",
      "344/1920 (epoch 2), train_loss = -4.774, time/batch = 0.868\n",
      "345/1920 (epoch 2), train_loss = -4.668, time/batch = 1.032\n",
      "346/1920 (epoch 2), train_loss = -4.897, time/batch = 0.865\n",
      "347/1920 (epoch 2), train_loss = -4.763, time/batch = 0.896\n",
      "348/1920 (epoch 2), train_loss = -4.436, time/batch = 0.945\n",
      "349/1920 (epoch 2), train_loss = -4.707, time/batch = 0.907\n",
      "350/1920 (epoch 2), train_loss = -4.747, time/batch = 0.865\n",
      "351/1920 (epoch 2), train_loss = -4.796, time/batch = 0.864\n",
      "352/1920 (epoch 2), train_loss = -4.540, time/batch = 1.085\n",
      "353/1920 (epoch 2), train_loss = -4.690, time/batch = 1.077\n",
      "354/1920 (epoch 2), train_loss = -4.407, time/batch = 0.923\n",
      "355/1920 (epoch 2), train_loss = -4.446, time/batch = 0.992\n",
      "356/1920 (epoch 2), train_loss = -4.832, time/batch = 0.913\n",
      "357/1920 (epoch 2), train_loss = -4.256, time/batch = 0.869\n",
      "358/1920 (epoch 2), train_loss = -4.654, time/batch = 0.864\n",
      "359/1920 (epoch 2), train_loss = -4.280, time/batch = 1.056\n",
      "360/1920 (epoch 2), train_loss = -4.086, time/batch = 0.850\n",
      "361/1920 (epoch 2), train_loss = -4.372, time/batch = 0.864\n",
      "362/1920 (epoch 2), train_loss = -4.659, time/batch = 1.058\n",
      "363/1920 (epoch 2), train_loss = -4.332, time/batch = 0.804\n",
      "364/1920 (epoch 2), train_loss = -4.593, time/batch = 1.013\n",
      "365/1920 (epoch 2), train_loss = -4.654, time/batch = 0.905\n",
      "366/1920 (epoch 2), train_loss = -4.620, time/batch = 1.212\n",
      "367/1920 (epoch 2), train_loss = -4.744, time/batch = 1.125\n",
      "368/1920 (epoch 2), train_loss = -4.637, time/batch = 1.064\n",
      "369/1920 (epoch 2), train_loss = -4.580, time/batch = 1.139\n",
      "370/1920 (epoch 2), train_loss = -4.713, time/batch = 0.932\n",
      "371/1920 (epoch 2), train_loss = -4.838, time/batch = 1.280\n",
      "372/1920 (epoch 2), train_loss = -4.337, time/batch = 1.515\n",
      "373/1920 (epoch 2), train_loss = -4.652, time/batch = 1.444\n",
      "374/1920 (epoch 2), train_loss = -4.148, time/batch = 1.510\n",
      "375/1920 (epoch 2), train_loss = -4.337, time/batch = 1.372\n",
      "376/1920 (epoch 2), train_loss = -4.889, time/batch = 1.674\n",
      "377/1920 (epoch 2), train_loss = -4.511, time/batch = 1.476\n",
      "378/1920 (epoch 2), train_loss = -4.597, time/batch = 1.644\n",
      "379/1920 (epoch 2), train_loss = -4.884, time/batch = 1.276\n",
      "380/1920 (epoch 2), train_loss = -4.497, time/batch = 1.331\n",
      "381/1920 (epoch 2), train_loss = -4.644, time/batch = 1.049\n",
      "382/1920 (epoch 2), train_loss = -4.500, time/batch = 1.246\n",
      "383/1920 (epoch 2), train_loss = -4.737, time/batch = 1.379\n",
      "384/1920 (epoch 3), train_loss = -4.136, time/batch = 1.241\n",
      "385/1920 (epoch 3), train_loss = -4.835, time/batch = 1.233\n",
      "386/1920 (epoch 3), train_loss = -4.677, time/batch = 1.263\n",
      "387/1920 (epoch 3), train_loss = -4.831, time/batch = 1.404\n",
      "388/1920 (epoch 3), train_loss = -4.846, time/batch = 1.171\n",
      "389/1920 (epoch 3), train_loss = -4.819, time/batch = 1.344\n",
      "390/1920 (epoch 3), train_loss = -4.958, time/batch = 1.210\n",
      "391/1920 (epoch 3), train_loss = -4.931, time/batch = 1.231\n",
      "392/1920 (epoch 3), train_loss = -4.835, time/batch = 1.375\n",
      "393/1920 (epoch 3), train_loss = -4.768, time/batch = 1.445\n",
      "394/1920 (epoch 3), train_loss = -4.962, time/batch = 1.253\n",
      "395/1920 (epoch 3), train_loss = -4.776, time/batch = 1.306\n",
      "396/1920 (epoch 3), train_loss = -4.896, time/batch = 1.256\n",
      "397/1920 (epoch 3), train_loss = -4.740, time/batch = 1.183\n",
      "398/1920 (epoch 3), train_loss = -4.485, time/batch = 1.357\n",
      "399/1920 (epoch 3), train_loss = -4.797, time/batch = 1.248\n",
      "400/1920 (epoch 3), train_loss = -4.901, time/batch = 1.201\n",
      "model saved to models/model.ckpt\n",
      "401/1920 (epoch 3), train_loss = -4.747, time/batch = 0.613\n",
      "402/1920 (epoch 3), train_loss = -4.847, time/batch = 0.614\n",
      "403/1920 (epoch 3), train_loss = -4.973, time/batch = 0.625\n",
      "404/1920 (epoch 3), train_loss = -4.907, time/batch = 0.586\n",
      "405/1920 (epoch 3), train_loss = -4.832, time/batch = 0.725\n",
      "406/1920 (epoch 3), train_loss = -4.896, time/batch = 0.658\n",
      "407/1920 (epoch 3), train_loss = -4.999, time/batch = 0.838\n",
      "408/1920 (epoch 3), train_loss = -5.074, time/batch = 0.761\n",
      "409/1920 (epoch 3), train_loss = -5.150, time/batch = 0.816\n",
      "410/1920 (epoch 3), train_loss = -4.526, time/batch = 0.628\n",
      "411/1920 (epoch 3), train_loss = -4.845, time/batch = 0.589\n",
      "412/1920 (epoch 3), train_loss = -3.987, time/batch = 0.659\n",
      "413/1920 (epoch 3), train_loss = -4.060, time/batch = 0.792\n",
      "414/1920 (epoch 3), train_loss = -5.121, time/batch = 0.620\n",
      "415/1920 (epoch 3), train_loss = -4.222, time/batch = 0.757\n",
      "416/1920 (epoch 3), train_loss = -3.937, time/batch = 0.779\n",
      "417/1920 (epoch 3), train_loss = -4.558, time/batch = 0.758\n",
      "418/1920 (epoch 3), train_loss = -5.079, time/batch = 0.640\n",
      "419/1920 (epoch 3), train_loss = -4.658, time/batch = 0.597\n",
      "420/1920 (epoch 3), train_loss = -4.757, time/batch = 0.581\n",
      "421/1920 (epoch 3), train_loss = -4.685, time/batch = 0.596\n",
      "422/1920 (epoch 3), train_loss = -4.788, time/batch = 0.594\n",
      "423/1920 (epoch 3), train_loss = -5.016, time/batch = 0.591\n",
      "424/1920 (epoch 3), train_loss = -4.789, time/batch = 0.871\n",
      "425/1920 (epoch 3), train_loss = -4.854, time/batch = 0.716\n",
      "426/1920 (epoch 3), train_loss = -4.972, time/batch = 0.713\n",
      "427/1920 (epoch 3), train_loss = -4.623, time/batch = 0.613\n",
      "428/1920 (epoch 3), train_loss = -4.803, time/batch = 0.595\n",
      "429/1920 (epoch 3), train_loss = -4.697, time/batch = 0.807\n",
      "430/1920 (epoch 3), train_loss = -4.215, time/batch = 0.609\n",
      "431/1920 (epoch 3), train_loss = -4.751, time/batch = 0.737\n",
      "432/1920 (epoch 3), train_loss = -4.328, time/batch = 0.629\n",
      "433/1920 (epoch 3), train_loss = -3.696, time/batch = 0.613\n",
      "434/1920 (epoch 3), train_loss = -4.270, time/batch = 0.664\n",
      "435/1920 (epoch 3), train_loss = -4.639, time/batch = 0.615\n",
      "436/1920 (epoch 3), train_loss = -4.278, time/batch = 0.630\n",
      "437/1920 (epoch 3), train_loss = -4.669, time/batch = 0.719\n",
      "438/1920 (epoch 3), train_loss = -4.703, time/batch = 0.738\n",
      "439/1920 (epoch 3), train_loss = -4.654, time/batch = 0.714\n",
      "440/1920 (epoch 3), train_loss = -4.598, time/batch = 0.815\n",
      "441/1920 (epoch 3), train_loss = -4.702, time/batch = 0.784\n",
      "442/1920 (epoch 3), train_loss = -4.515, time/batch = 0.669\n",
      "443/1920 (epoch 3), train_loss = -4.443, time/batch = 0.672\n",
      "444/1920 (epoch 3), train_loss = -4.521, time/batch = 0.681\n",
      "445/1920 (epoch 3), train_loss = -4.762, time/batch = 0.642\n",
      "446/1920 (epoch 3), train_loss = -4.678, time/batch = 0.591\n",
      "447/1920 (epoch 3), train_loss = -4.606, time/batch = 0.639\n",
      "448/1920 (epoch 3), train_loss = -4.763, time/batch = 0.635\n",
      "449/1920 (epoch 3), train_loss = -4.775, time/batch = 0.623\n",
      "450/1920 (epoch 3), train_loss = -4.817, time/batch = 0.703\n",
      "451/1920 (epoch 3), train_loss = -4.717, time/batch = 0.671\n",
      "452/1920 (epoch 3), train_loss = -4.859, time/batch = 0.728\n",
      "453/1920 (epoch 3), train_loss = -4.702, time/batch = 0.719\n",
      "454/1920 (epoch 3), train_loss = -4.681, time/batch = 0.695\n",
      "455/1920 (epoch 3), train_loss = -5.058, time/batch = 0.684\n",
      "456/1920 (epoch 3), train_loss = -4.904, time/batch = 0.628\n",
      "457/1920 (epoch 3), train_loss = -5.089, time/batch = 0.631\n",
      "458/1920 (epoch 3), train_loss = -4.672, time/batch = 0.654\n",
      "459/1920 (epoch 3), train_loss = -4.899, time/batch = 0.673\n",
      "460/1920 (epoch 3), train_loss = -4.698, time/batch = 0.650\n",
      "461/1920 (epoch 3), train_loss = -4.732, time/batch = 0.716\n",
      "462/1920 (epoch 3), train_loss = -4.888, time/batch = 0.666\n",
      "463/1920 (epoch 3), train_loss = -4.790, time/batch = 0.658\n",
      "464/1920 (epoch 3), train_loss = -4.531, time/batch = 0.665\n",
      "465/1920 (epoch 3), train_loss = -4.513, time/batch = 0.646\n",
      "466/1920 (epoch 3), train_loss = -4.904, time/batch = 0.653\n",
      "467/1920 (epoch 3), train_loss = -4.630, time/batch = 1.032\n",
      "468/1920 (epoch 3), train_loss = -4.955, time/batch = 0.964\n",
      "469/1920 (epoch 3), train_loss = -4.505, time/batch = 0.648\n",
      "470/1920 (epoch 3), train_loss = -4.312, time/batch = 0.682\n",
      "471/1920 (epoch 3), train_loss = -4.690, time/batch = 0.814\n",
      "472/1920 (epoch 3), train_loss = -4.322, time/batch = 0.734\n",
      "473/1920 (epoch 3), train_loss = -4.661, time/batch = 0.796\n",
      "474/1920 (epoch 3), train_loss = -4.705, time/batch = 0.651\n",
      "475/1920 (epoch 3), train_loss = -4.679, time/batch = 0.712\n",
      "476/1920 (epoch 3), train_loss = -4.654, time/batch = 0.641\n",
      "477/1920 (epoch 3), train_loss = -4.808, time/batch = 0.685\n",
      "478/1920 (epoch 3), train_loss = -4.605, time/batch = 0.700\n",
      "479/1920 (epoch 3), train_loss = -4.372, time/batch = 0.830\n",
      "480/1920 (epoch 3), train_loss = -4.514, time/batch = 0.905\n",
      "481/1920 (epoch 3), train_loss = -4.820, time/batch = 1.143\n",
      "482/1920 (epoch 3), train_loss = -4.702, time/batch = 1.067\n",
      "483/1920 (epoch 3), train_loss = -4.783, time/batch = 1.137\n",
      "484/1920 (epoch 3), train_loss = -4.941, time/batch = 1.361\n",
      "485/1920 (epoch 3), train_loss = -4.876, time/batch = 1.044\n",
      "486/1920 (epoch 3), train_loss = -4.964, time/batch = 1.419\n",
      "487/1920 (epoch 3), train_loss = -5.029, time/batch = 1.055\n",
      "488/1920 (epoch 3), train_loss = -4.994, time/batch = 1.047\n",
      "489/1920 (epoch 3), train_loss = -4.929, time/batch = 0.891\n",
      "490/1920 (epoch 3), train_loss = -4.892, time/batch = 1.056\n",
      "491/1920 (epoch 3), train_loss = -5.116, time/batch = 1.000\n",
      "492/1920 (epoch 3), train_loss = -5.173, time/batch = 1.435\n",
      "493/1920 (epoch 3), train_loss = -4.860, time/batch = 1.373\n",
      "494/1920 (epoch 3), train_loss = -5.235, time/batch = 1.346\n",
      "495/1920 (epoch 3), train_loss = -5.121, time/batch = 1.353\n",
      "496/1920 (epoch 3), train_loss = -5.125, time/batch = 0.933\n",
      "497/1920 (epoch 3), train_loss = -5.305, time/batch = 0.935\n",
      "498/1920 (epoch 3), train_loss = -5.049, time/batch = 0.897\n",
      "499/1920 (epoch 3), train_loss = -5.194, time/batch = 1.202\n",
      "500/1920 (epoch 3), train_loss = -4.872, time/batch = 0.969\n",
      "model saved to models/model.ckpt\n",
      "501/1920 (epoch 3), train_loss = -4.837, time/batch = 0.776\n",
      "502/1920 (epoch 3), train_loss = -5.108, time/batch = 0.780\n",
      "503/1920 (epoch 3), train_loss = -5.170, time/batch = 1.017\n",
      "504/1920 (epoch 3), train_loss = -4.977, time/batch = 0.865\n",
      "505/1920 (epoch 3), train_loss = -4.897, time/batch = 0.942\n",
      "506/1920 (epoch 3), train_loss = -5.282, time/batch = 1.019\n",
      "507/1920 (epoch 3), train_loss = -5.134, time/batch = 1.004\n",
      "508/1920 (epoch 3), train_loss = -4.967, time/batch = 0.933\n",
      "509/1920 (epoch 3), train_loss = -4.970, time/batch = 0.971\n",
      "510/1920 (epoch 3), train_loss = -5.072, time/batch = 0.913\n",
      "511/1920 (epoch 3), train_loss = -5.050, time/batch = 0.891\n",
      "512/1920 (epoch 4), train_loss = -4.693, time/batch = 0.983\n",
      "513/1920 (epoch 4), train_loss = -5.323, time/batch = 0.894\n",
      "514/1920 (epoch 4), train_loss = -5.114, time/batch = 0.850\n",
      "515/1920 (epoch 4), train_loss = -5.292, time/batch = 0.908\n",
      "516/1920 (epoch 4), train_loss = -5.205, time/batch = 1.081\n",
      "517/1920 (epoch 4), train_loss = -5.021, time/batch = 0.916\n",
      "518/1920 (epoch 4), train_loss = -5.303, time/batch = 1.001\n",
      "519/1920 (epoch 4), train_loss = -4.973, time/batch = 1.141\n",
      "520/1920 (epoch 4), train_loss = -4.970, time/batch = 0.821\n",
      "521/1920 (epoch 4), train_loss = -4.608, time/batch = 0.865\n",
      "522/1920 (epoch 4), train_loss = -4.620, time/batch = 0.762\n",
      "523/1920 (epoch 4), train_loss = -5.154, time/batch = 0.990\n",
      "524/1920 (epoch 4), train_loss = -5.082, time/batch = 0.941\n",
      "525/1920 (epoch 4), train_loss = -5.273, time/batch = 0.837\n",
      "526/1920 (epoch 4), train_loss = -5.184, time/batch = 0.995\n",
      "527/1920 (epoch 4), train_loss = -5.229, time/batch = 0.940\n",
      "528/1920 (epoch 4), train_loss = -5.175, time/batch = 0.894\n",
      "529/1920 (epoch 4), train_loss = -5.374, time/batch = 0.802\n",
      "530/1920 (epoch 4), train_loss = -5.271, time/batch = 1.257\n",
      "531/1920 (epoch 4), train_loss = -5.043, time/batch = 0.936\n",
      "532/1920 (epoch 4), train_loss = -5.320, time/batch = 0.866\n",
      "533/1920 (epoch 4), train_loss = -5.410, time/batch = 1.573\n",
      "534/1920 (epoch 4), train_loss = -4.747, time/batch = 1.130\n",
      "535/1920 (epoch 4), train_loss = -4.993, time/batch = 0.952\n",
      "536/1920 (epoch 4), train_loss = -4.522, time/batch = 0.789\n",
      "537/1920 (epoch 4), train_loss = -4.650, time/batch = 1.009\n",
      "538/1920 (epoch 4), train_loss = -5.223, time/batch = 0.877\n",
      "539/1920 (epoch 4), train_loss = -4.794, time/batch = 0.852\n",
      "540/1920 (epoch 4), train_loss = -5.152, time/batch = 0.963\n",
      "541/1920 (epoch 4), train_loss = -5.052, time/batch = 0.891\n",
      "542/1920 (epoch 4), train_loss = -5.056, time/batch = 0.921\n",
      "543/1920 (epoch 4), train_loss = -4.695, time/batch = 0.988\n",
      "544/1920 (epoch 4), train_loss = -5.020, time/batch = 0.940\n",
      "545/1920 (epoch 4), train_loss = -5.210, time/batch = 0.874\n",
      "546/1920 (epoch 4), train_loss = -5.139, time/batch = 0.873\n",
      "547/1920 (epoch 4), train_loss = -5.240, time/batch = 1.083\n",
      "548/1920 (epoch 4), train_loss = -5.327, time/batch = 0.889\n",
      "549/1920 (epoch 4), train_loss = -5.245, time/batch = 0.867\n",
      "550/1920 (epoch 4), train_loss = -5.341, time/batch = 0.978\n",
      "551/1920 (epoch 4), train_loss = -5.304, time/batch = 0.926\n",
      "552/1920 (epoch 4), train_loss = -5.059, time/batch = 1.091\n",
      "553/1920 (epoch 4), train_loss = -5.259, time/batch = 0.997\n",
      "554/1920 (epoch 4), train_loss = -5.051, time/batch = 1.147\n",
      "555/1920 (epoch 4), train_loss = -5.263, time/batch = 0.934\n",
      "556/1920 (epoch 4), train_loss = -5.178, time/batch = 0.950\n",
      "557/1920 (epoch 4), train_loss = -5.108, time/batch = 1.319\n",
      "558/1920 (epoch 4), train_loss = -5.280, time/batch = 1.258\n",
      "559/1920 (epoch 4), train_loss = -5.282, time/batch = 1.267\n",
      "560/1920 (epoch 4), train_loss = -5.125, time/batch = 1.234\n",
      "561/1920 (epoch 4), train_loss = -5.324, time/batch = 1.352\n",
      "562/1920 (epoch 4), train_loss = -5.330, time/batch = 1.011\n",
      "563/1920 (epoch 4), train_loss = -5.201, time/batch = 1.032\n",
      "564/1920 (epoch 4), train_loss = -5.354, time/batch = 0.923\n",
      "565/1920 (epoch 4), train_loss = -5.303, time/batch = 0.770\n",
      "566/1920 (epoch 4), train_loss = -5.354, time/batch = 0.767\n",
      "567/1920 (epoch 4), train_loss = -5.319, time/batch = 0.731\n",
      "568/1920 (epoch 4), train_loss = -5.317, time/batch = 0.695\n",
      "569/1920 (epoch 4), train_loss = -5.437, time/batch = 1.300\n",
      "570/1920 (epoch 4), train_loss = -5.207, time/batch = 1.015\n",
      "571/1920 (epoch 4), train_loss = -5.372, time/batch = 1.039\n",
      "572/1920 (epoch 4), train_loss = -5.320, time/batch = 1.264\n",
      "573/1920 (epoch 4), train_loss = -5.177, time/batch = 1.131\n",
      "574/1920 (epoch 4), train_loss = -5.386, time/batch = 0.988\n",
      "575/1920 (epoch 4), train_loss = -5.080, time/batch = 1.202\n",
      "576/1920 (epoch 4), train_loss = -4.991, time/batch = 1.239\n",
      "577/1920 (epoch 4), train_loss = -5.215, time/batch = 1.085\n",
      "578/1920 (epoch 4), train_loss = -5.351, time/batch = 1.162\n",
      "579/1920 (epoch 4), train_loss = -5.128, time/batch = 0.994\n",
      "580/1920 (epoch 4), train_loss = -5.185, time/batch = 1.177\n",
      "581/1920 (epoch 4), train_loss = -5.443, time/batch = 1.230\n",
      "582/1920 (epoch 4), train_loss = -5.443, time/batch = 1.086\n",
      "583/1920 (epoch 4), train_loss = -5.432, time/batch = 1.094\n",
      "584/1920 (epoch 4), train_loss = -5.447, time/batch = 1.002\n",
      "585/1920 (epoch 4), train_loss = -5.244, time/batch = 1.406\n",
      "586/1920 (epoch 4), train_loss = -5.303, time/batch = 1.423\n",
      "587/1920 (epoch 4), train_loss = -5.480, time/batch = 1.068\n",
      "588/1920 (epoch 4), train_loss = -5.381, time/batch = 1.107\n",
      "589/1920 (epoch 4), train_loss = -5.515, time/batch = 1.257\n",
      "590/1920 (epoch 4), train_loss = -5.445, time/batch = 1.166\n",
      "591/1920 (epoch 4), train_loss = -5.506, time/batch = 1.124\n",
      "592/1920 (epoch 4), train_loss = -5.527, time/batch = 1.005\n",
      "593/1920 (epoch 4), train_loss = -5.455, time/batch = 1.037\n",
      "594/1920 (epoch 4), train_loss = -5.606, time/batch = 1.025\n",
      "595/1920 (epoch 4), train_loss = -4.954, time/batch = 1.235\n",
      "596/1920 (epoch 4), train_loss = -4.856, time/batch = 1.299\n",
      "597/1920 (epoch 4), train_loss = -5.170, time/batch = 1.203\n",
      "598/1920 (epoch 4), train_loss = -5.308, time/batch = 1.036\n",
      "599/1920 (epoch 4), train_loss = -5.156, time/batch = 1.140\n",
      "600/1920 (epoch 4), train_loss = -5.221, time/batch = 1.255\n",
      "model saved to models/model.ckpt\n",
      "601/1920 (epoch 4), train_loss = -5.325, time/batch = 0.830\n",
      "602/1920 (epoch 4), train_loss = -5.415, time/batch = 0.835\n",
      "603/1920 (epoch 4), train_loss = -5.362, time/batch = 0.795\n",
      "604/1920 (epoch 4), train_loss = -5.445, time/batch = 1.052\n",
      "605/1920 (epoch 4), train_loss = -5.314, time/batch = 0.873\n",
      "606/1920 (epoch 4), train_loss = -5.450, time/batch = 1.199\n",
      "607/1920 (epoch 4), train_loss = -5.283, time/batch = 1.396\n",
      "608/1920 (epoch 4), train_loss = -5.286, time/batch = 0.895\n",
      "609/1920 (epoch 4), train_loss = -5.531, time/batch = 0.893\n",
      "610/1920 (epoch 4), train_loss = -5.434, time/batch = 0.818\n",
      "611/1920 (epoch 4), train_loss = -5.275, time/batch = 1.126\n",
      "612/1920 (epoch 4), train_loss = -5.332, time/batch = 0.885\n",
      "613/1920 (epoch 4), train_loss = -5.455, time/batch = 0.888\n",
      "614/1920 (epoch 4), train_loss = -5.342, time/batch = 0.909\n",
      "615/1920 (epoch 4), train_loss = -5.442, time/batch = 0.851\n",
      "616/1920 (epoch 4), train_loss = -5.442, time/batch = 0.867\n",
      "617/1920 (epoch 4), train_loss = -5.432, time/batch = 0.787\n",
      "618/1920 (epoch 4), train_loss = -5.561, time/batch = 1.126\n",
      "619/1920 (epoch 4), train_loss = -5.502, time/batch = 1.225\n",
      "620/1920 (epoch 4), train_loss = -5.447, time/batch = 1.064\n",
      "621/1920 (epoch 4), train_loss = -5.579, time/batch = 1.317\n",
      "622/1920 (epoch 4), train_loss = -5.590, time/batch = 1.039\n",
      "623/1920 (epoch 4), train_loss = -5.615, time/batch = 1.062\n",
      "624/1920 (epoch 4), train_loss = -5.609, time/batch = 1.068\n",
      "625/1920 (epoch 4), train_loss = -5.616, time/batch = 1.271\n",
      "626/1920 (epoch 4), train_loss = -5.687, time/batch = 1.221\n",
      "627/1920 (epoch 4), train_loss = -5.435, time/batch = 1.153\n",
      "628/1920 (epoch 4), train_loss = -5.681, time/batch = 1.220\n",
      "629/1920 (epoch 4), train_loss = -4.522, time/batch = 0.906\n",
      "630/1920 (epoch 4), train_loss = -4.235, time/batch = 0.893\n",
      "631/1920 (epoch 4), train_loss = -5.129, time/batch = 1.248\n",
      "632/1920 (epoch 4), train_loss = -5.179, time/batch = 1.186\n",
      "633/1920 (epoch 4), train_loss = -4.770, time/batch = 1.132\n",
      "634/1920 (epoch 4), train_loss = -5.551, time/batch = 1.052\n",
      "635/1920 (epoch 4), train_loss = -5.037, time/batch = 1.278\n",
      "636/1920 (epoch 4), train_loss = -4.595, time/batch = 1.258\n",
      "637/1920 (epoch 4), train_loss = -5.323, time/batch = 1.165\n",
      "638/1920 (epoch 4), train_loss = -5.341, time/batch = 0.973\n",
      "639/1920 (epoch 4), train_loss = -4.746, time/batch = 1.089\n",
      "640/1920 (epoch 5), train_loss = -4.622, time/batch = 0.959\n",
      "641/1920 (epoch 5), train_loss = -5.450, time/batch = 0.934\n",
      "642/1920 (epoch 5), train_loss = -5.522, time/batch = 0.940\n",
      "643/1920 (epoch 5), train_loss = -5.479, time/batch = 0.857\n",
      "644/1920 (epoch 5), train_loss = -5.574, time/batch = 0.813\n",
      "645/1920 (epoch 5), train_loss = -5.486, time/batch = 1.219\n",
      "646/1920 (epoch 5), train_loss = -5.503, time/batch = 1.033\n",
      "647/1920 (epoch 5), train_loss = -5.618, time/batch = 1.083\n",
      "648/1920 (epoch 5), train_loss = -5.501, time/batch = 1.388\n",
      "649/1920 (epoch 5), train_loss = -5.439, time/batch = 0.932\n",
      "650/1920 (epoch 5), train_loss = -5.570, time/batch = 0.866\n",
      "651/1920 (epoch 5), train_loss = -5.629, time/batch = 0.797\n",
      "652/1920 (epoch 5), train_loss = -5.500, time/batch = 0.998\n",
      "653/1920 (epoch 5), train_loss = -5.580, time/batch = 0.961\n",
      "654/1920 (epoch 5), train_loss = -5.438, time/batch = 0.841\n",
      "655/1920 (epoch 5), train_loss = -5.399, time/batch = 1.055\n",
      "656/1920 (epoch 5), train_loss = -5.555, time/batch = 0.868\n",
      "657/1920 (epoch 5), train_loss = -5.703, time/batch = 0.866\n",
      "658/1920 (epoch 5), train_loss = -5.212, time/batch = 0.789\n",
      "659/1920 (epoch 5), train_loss = -5.049, time/batch = 1.068\n",
      "660/1920 (epoch 5), train_loss = -5.142, time/batch = 0.926\n",
      "661/1920 (epoch 5), train_loss = -5.301, time/batch = 0.884\n",
      "662/1920 (epoch 5), train_loss = -5.157, time/batch = 0.929\n",
      "663/1920 (epoch 5), train_loss = -5.385, time/batch = 0.985\n",
      "664/1920 (epoch 5), train_loss = -5.473, time/batch = 0.942\n",
      "665/1920 (epoch 5), train_loss = -5.507, time/batch = 0.787\n",
      "666/1920 (epoch 5), train_loss = -5.554, time/batch = 1.418\n",
      "667/1920 (epoch 5), train_loss = -5.615, time/batch = 0.930\n",
      "668/1920 (epoch 5), train_loss = -5.375, time/batch = 1.329\n",
      "669/1920 (epoch 5), train_loss = -5.411, time/batch = 1.156\n",
      "670/1920 (epoch 5), train_loss = -5.617, time/batch = 0.905\n",
      "671/1920 (epoch 5), train_loss = -5.613, time/batch = 1.055\n",
      "672/1920 (epoch 5), train_loss = -5.630, time/batch = 1.171\n",
      "673/1920 (epoch 5), train_loss = -5.631, time/batch = 1.012\n",
      "674/1920 (epoch 5), train_loss = -5.561, time/batch = 0.861\n",
      "675/1920 (epoch 5), train_loss = -5.678, time/batch = 0.938\n",
      "676/1920 (epoch 5), train_loss = -5.611, time/batch = 1.098\n",
      "677/1920 (epoch 5), train_loss = -5.668, time/batch = 1.048\n",
      "678/1920 (epoch 5), train_loss = -5.630, time/batch = 1.013\n",
      "679/1920 (epoch 5), train_loss = -5.693, time/batch = 0.859\n",
      "680/1920 (epoch 5), train_loss = -5.645, time/batch = 1.094\n",
      "681/1920 (epoch 5), train_loss = -5.734, time/batch = 0.934\n",
      "682/1920 (epoch 5), train_loss = -5.675, time/batch = 1.082\n",
      "683/1920 (epoch 5), train_loss = -5.716, time/batch = 1.155\n",
      "684/1920 (epoch 5), train_loss = -5.594, time/batch = 0.834\n",
      "685/1920 (epoch 5), train_loss = -5.661, time/batch = 1.048\n",
      "686/1920 (epoch 5), train_loss = -5.817, time/batch = 0.993\n",
      "687/1920 (epoch 5), train_loss = -5.249, time/batch = 1.146\n",
      "688/1920 (epoch 5), train_loss = -5.384, time/batch = 0.898\n",
      "689/1920 (epoch 5), train_loss = -5.458, time/batch = 0.885\n",
      "690/1920 (epoch 5), train_loss = -5.625, time/batch = 1.010\n",
      "691/1920 (epoch 5), train_loss = -5.534, time/batch = 0.945\n",
      "692/1920 (epoch 5), train_loss = -5.865, time/batch = 0.902\n",
      "693/1920 (epoch 5), train_loss = -5.673, time/batch = 0.945\n",
      "694/1920 (epoch 5), train_loss = -5.709, time/batch = 0.922\n",
      "695/1920 (epoch 5), train_loss = -5.815, time/batch = 0.885\n",
      "696/1920 (epoch 5), train_loss = -5.619, time/batch = 0.949\n",
      "697/1920 (epoch 5), train_loss = -5.653, time/batch = 1.031\n",
      "698/1920 (epoch 5), train_loss = -5.837, time/batch = 0.930\n",
      "699/1920 (epoch 5), train_loss = -5.572, time/batch = 0.920\n",
      "700/1920 (epoch 5), train_loss = -5.679, time/batch = 0.987\n",
      "model saved to models/model.ckpt\n",
      "701/1920 (epoch 5), train_loss = -5.682, time/batch = 0.641\n",
      "702/1920 (epoch 5), train_loss = -5.828, time/batch = 0.787\n",
      "703/1920 (epoch 5), train_loss = -5.518, time/batch = 0.785\n",
      "704/1920 (epoch 5), train_loss = -5.581, time/batch = 0.791\n",
      "705/1920 (epoch 5), train_loss = -5.754, time/batch = 0.830\n",
      "706/1920 (epoch 5), train_loss = -5.469, time/batch = 0.797\n",
      "707/1920 (epoch 5), train_loss = -5.607, time/batch = 0.761\n",
      "708/1920 (epoch 5), train_loss = -5.702, time/batch = 0.769\n",
      "709/1920 (epoch 5), train_loss = -5.458, time/batch = 0.887\n",
      "710/1920 (epoch 5), train_loss = -5.786, time/batch = 0.848\n",
      "711/1920 (epoch 5), train_loss = -5.424, time/batch = 0.860\n",
      "712/1920 (epoch 5), train_loss = -5.506, time/batch = 0.865\n",
      "713/1920 (epoch 5), train_loss = -5.356, time/batch = 0.801\n",
      "714/1920 (epoch 5), train_loss = -5.721, time/batch = 0.748\n",
      "715/1920 (epoch 5), train_loss = -5.131, time/batch = 0.755\n",
      "716/1920 (epoch 5), train_loss = -5.114, time/batch = 0.641\n",
      "717/1920 (epoch 5), train_loss = -5.842, time/batch = 0.614\n",
      "718/1920 (epoch 5), train_loss = -4.963, time/batch = 0.654\n",
      "719/1920 (epoch 5), train_loss = -4.656, time/batch = 0.767\n",
      "720/1920 (epoch 5), train_loss = -4.737, time/batch = 0.629\n",
      "721/1920 (epoch 5), train_loss = -5.144, time/batch = 0.644\n",
      "722/1920 (epoch 5), train_loss = -5.703, time/batch = 0.671\n",
      "723/1920 (epoch 5), train_loss = -5.382, time/batch = 0.709\n",
      "724/1920 (epoch 5), train_loss = -5.326, time/batch = 0.647\n",
      "725/1920 (epoch 5), train_loss = -5.388, time/batch = 0.779\n",
      "726/1920 (epoch 5), train_loss = -5.551, time/batch = 0.656\n",
      "727/1920 (epoch 5), train_loss = -5.484, time/batch = 0.751\n",
      "728/1920 (epoch 5), train_loss = -5.278, time/batch = 0.701\n",
      "729/1920 (epoch 5), train_loss = -5.320, time/batch = 0.644\n",
      "730/1920 (epoch 5), train_loss = -5.539, time/batch = 0.603\n",
      "731/1920 (epoch 5), train_loss = -5.319, time/batch = 0.638\n",
      "732/1920 (epoch 5), train_loss = -5.379, time/batch = 0.661\n",
      "733/1920 (epoch 5), train_loss = -5.660, time/batch = 0.696\n",
      "734/1920 (epoch 5), train_loss = -5.628, time/batch = 0.634\n",
      "735/1920 (epoch 5), train_loss = -5.670, time/batch = 0.640\n",
      "736/1920 (epoch 5), train_loss = -5.673, time/batch = 0.628\n",
      "737/1920 (epoch 5), train_loss = -5.520, time/batch = 0.708\n",
      "738/1920 (epoch 5), train_loss = -5.547, time/batch = 0.867\n",
      "739/1920 (epoch 5), train_loss = -5.485, time/batch = 0.691\n",
      "740/1920 (epoch 5), train_loss = -5.398, time/batch = 0.735\n",
      "741/1920 (epoch 5), train_loss = -5.654, time/batch = 0.772\n",
      "742/1920 (epoch 5), train_loss = -5.524, time/batch = 0.702\n",
      "743/1920 (epoch 5), train_loss = -5.726, time/batch = 0.644\n",
      "744/1920 (epoch 5), train_loss = -5.564, time/batch = 0.977\n",
      "745/1920 (epoch 5), train_loss = -5.689, time/batch = 0.735\n",
      "746/1920 (epoch 5), train_loss = -5.348, time/batch = 0.728\n",
      "747/1920 (epoch 5), train_loss = -5.299, time/batch = 0.648\n",
      "748/1920 (epoch 5), train_loss = -5.543, time/batch = 0.831\n",
      "749/1920 (epoch 5), train_loss = -5.486, time/batch = 0.662\n",
      "750/1920 (epoch 5), train_loss = -5.389, time/batch = 0.659\n",
      "751/1920 (epoch 5), train_loss = -5.493, time/batch = 0.606\n",
      "752/1920 (epoch 5), train_loss = -5.545, time/batch = 0.615\n",
      "753/1920 (epoch 5), train_loss = -5.278, time/batch = 0.622\n",
      "754/1920 (epoch 5), train_loss = -5.589, time/batch = 0.631\n",
      "755/1920 (epoch 5), train_loss = -5.452, time/batch = 0.786\n",
      "756/1920 (epoch 5), train_loss = -4.925, time/batch = 0.930\n",
      "757/1920 (epoch 5), train_loss = -4.887, time/batch = 0.804\n",
      "758/1920 (epoch 5), train_loss = -5.456, time/batch = 0.853\n",
      "759/1920 (epoch 5), train_loss = -5.487, time/batch = 0.751\n",
      "760/1920 (epoch 5), train_loss = -5.014, time/batch = 0.683\n",
      "761/1920 (epoch 5), train_loss = -4.941, time/batch = 0.603\n",
      "762/1920 (epoch 5), train_loss = -5.225, time/batch = 0.841\n",
      "763/1920 (epoch 5), train_loss = -5.305, time/batch = 0.786\n",
      "764/1920 (epoch 5), train_loss = -5.080, time/batch = 0.956\n",
      "765/1920 (epoch 5), train_loss = -4.861, time/batch = 0.760\n",
      "766/1920 (epoch 5), train_loss = -5.234, time/batch = 0.834\n",
      "767/1920 (epoch 5), train_loss = -4.961, time/batch = 0.678\n",
      "768/1920 (epoch 6), train_loss = -4.695, time/batch = 0.654\n",
      "769/1920 (epoch 6), train_loss = -4.599, time/batch = 0.624\n",
      "770/1920 (epoch 6), train_loss = -4.436, time/batch = 0.679\n",
      "771/1920 (epoch 6), train_loss = -5.090, time/batch = 0.691\n",
      "772/1920 (epoch 6), train_loss = -5.168, time/batch = 0.672\n",
      "773/1920 (epoch 6), train_loss = -5.509, time/batch = 0.755\n",
      "774/1920 (epoch 6), train_loss = -4.785, time/batch = 0.802\n",
      "775/1920 (epoch 6), train_loss = -4.646, time/batch = 0.690\n",
      "776/1920 (epoch 6), train_loss = -4.981, time/batch = 0.682\n",
      "777/1920 (epoch 6), train_loss = -4.898, time/batch = 0.709\n",
      "778/1920 (epoch 6), train_loss = -5.246, time/batch = 0.636\n",
      "779/1920 (epoch 6), train_loss = -5.183, time/batch = 0.629\n",
      "780/1920 (epoch 6), train_loss = -5.141, time/batch = 0.648\n",
      "781/1920 (epoch 6), train_loss = -5.320, time/batch = 0.606\n",
      "782/1920 (epoch 6), train_loss = -5.280, time/batch = 0.652\n",
      "783/1920 (epoch 6), train_loss = -5.389, time/batch = 0.642\n",
      "784/1920 (epoch 6), train_loss = -5.336, time/batch = 0.664\n",
      "785/1920 (epoch 6), train_loss = -5.422, time/batch = 0.621\n",
      "786/1920 (epoch 6), train_loss = -5.433, time/batch = 0.722\n",
      "787/1920 (epoch 6), train_loss = -5.473, time/batch = 0.689\n",
      "788/1920 (epoch 6), train_loss = -5.464, time/batch = 0.625\n",
      "789/1920 (epoch 6), train_loss = -5.545, time/batch = 0.612\n",
      "790/1920 (epoch 6), train_loss = -5.403, time/batch = 0.662\n",
      "791/1920 (epoch 6), train_loss = -4.978, time/batch = 0.624\n",
      "792/1920 (epoch 6), train_loss = -4.962, time/batch = 0.618\n",
      "793/1920 (epoch 6), train_loss = -5.287, time/batch = 0.722\n",
      "794/1920 (epoch 6), train_loss = -5.573, time/batch = 0.680\n",
      "795/1920 (epoch 6), train_loss = -5.409, time/batch = 0.716\n",
      "796/1920 (epoch 6), train_loss = -5.187, time/batch = 0.778\n",
      "797/1920 (epoch 6), train_loss = -5.244, time/batch = 0.845\n",
      "798/1920 (epoch 6), train_loss = -5.512, time/batch = 0.811\n",
      "799/1920 (epoch 6), train_loss = -5.315, time/batch = 0.863\n",
      "800/1920 (epoch 6), train_loss = -5.376, time/batch = 0.646\n",
      "model saved to models/model.ckpt\n",
      "801/1920 (epoch 6), train_loss = -5.279, time/batch = 1.082\n",
      "802/1920 (epoch 6), train_loss = -5.436, time/batch = 1.049\n",
      "803/1920 (epoch 6), train_loss = -5.470, time/batch = 0.984\n",
      "804/1920 (epoch 6), train_loss = -5.452, time/batch = 1.097\n",
      "805/1920 (epoch 6), train_loss = -5.627, time/batch = 0.952\n",
      "806/1920 (epoch 6), train_loss = -5.656, time/batch = 1.026\n",
      "807/1920 (epoch 6), train_loss = -5.603, time/batch = 0.966\n",
      "808/1920 (epoch 6), train_loss = -5.682, time/batch = 1.263\n",
      "809/1920 (epoch 6), train_loss = -5.699, time/batch = 0.946\n",
      "810/1920 (epoch 6), train_loss = -5.560, time/batch = 0.982\n",
      "811/1920 (epoch 6), train_loss = -5.660, time/batch = 1.015\n",
      "812/1920 (epoch 6), train_loss = -5.764, time/batch = 0.903\n",
      "813/1920 (epoch 6), train_loss = -5.856, time/batch = 1.093\n",
      "814/1920 (epoch 6), train_loss = -5.734, time/batch = 0.925\n",
      "815/1920 (epoch 6), train_loss = -5.774, time/batch = 1.274\n",
      "816/1920 (epoch 6), train_loss = -5.809, time/batch = 0.948\n",
      "817/1920 (epoch 6), train_loss = -5.934, time/batch = 1.091\n",
      "818/1920 (epoch 6), train_loss = -5.592, time/batch = 1.075\n",
      "819/1920 (epoch 6), train_loss = -5.718, time/batch = 1.021\n",
      "820/1920 (epoch 6), train_loss = -5.521, time/batch = 0.992\n",
      "821/1920 (epoch 6), train_loss = -5.555, time/batch = 0.855\n",
      "822/1920 (epoch 6), train_loss = -5.537, time/batch = 1.223\n",
      "823/1920 (epoch 6), train_loss = -5.737, time/batch = 1.004\n",
      "824/1920 (epoch 6), train_loss = -5.705, time/batch = 1.046\n",
      "825/1920 (epoch 6), train_loss = -5.856, time/batch = 1.063\n",
      "826/1920 (epoch 6), train_loss = -5.873, time/batch = 1.026\n",
      "827/1920 (epoch 6), train_loss = -5.753, time/batch = 1.047\n",
      "828/1920 (epoch 6), train_loss = -5.916, time/batch = 1.070\n",
      "829/1920 (epoch 6), train_loss = -5.773, time/batch = 0.919\n",
      "830/1920 (epoch 6), train_loss = -5.772, time/batch = 1.027\n",
      "831/1920 (epoch 6), train_loss = -6.001, time/batch = 1.051\n",
      "832/1920 (epoch 6), train_loss = -5.712, time/batch = 1.252\n",
      "833/1920 (epoch 6), train_loss = -5.938, time/batch = 0.906\n",
      "834/1920 (epoch 6), train_loss = -5.545, time/batch = 0.915\n",
      "835/1920 (epoch 6), train_loss = -5.475, time/batch = 0.945\n",
      "836/1920 (epoch 6), train_loss = -5.141, time/batch = 0.931\n",
      "837/1920 (epoch 6), train_loss = -5.424, time/batch = 1.101\n",
      "838/1920 (epoch 6), train_loss = -5.934, time/batch = 1.092\n",
      "839/1920 (epoch 6), train_loss = -5.866, time/batch = 1.252\n",
      "840/1920 (epoch 6), train_loss = -5.698, time/batch = 0.953\n",
      "841/1920 (epoch 6), train_loss = -5.864, time/batch = 0.919\n",
      "842/1920 (epoch 6), train_loss = -5.643, time/batch = 1.181\n",
      "843/1920 (epoch 6), train_loss = -5.709, time/batch = 1.006\n",
      "844/1920 (epoch 6), train_loss = -5.976, time/batch = 1.023\n",
      "845/1920 (epoch 6), train_loss = -6.051, time/batch = 0.821\n",
      "846/1920 (epoch 6), train_loss = -5.931, time/batch = 1.138\n",
      "847/1920 (epoch 6), train_loss = -5.978, time/batch = 0.928\n",
      "848/1920 (epoch 6), train_loss = -5.784, time/batch = 1.082\n",
      "849/1920 (epoch 6), train_loss = -5.780, time/batch = 1.089\n",
      "850/1920 (epoch 6), train_loss = -6.074, time/batch = 1.111\n",
      "851/1920 (epoch 6), train_loss = -5.936, time/batch = 0.961\n",
      "852/1920 (epoch 6), train_loss = -6.014, time/batch = 1.240\n",
      "853/1920 (epoch 6), train_loss = -5.825, time/batch = 1.037\n",
      "854/1920 (epoch 6), train_loss = -5.854, time/batch = 1.050\n",
      "855/1920 (epoch 6), train_loss = -5.950, time/batch = 1.144\n",
      "856/1920 (epoch 6), train_loss = -5.875, time/batch = 1.206\n",
      "857/1920 (epoch 6), train_loss = -6.050, time/batch = 1.032\n",
      "858/1920 (epoch 6), train_loss = -5.999, time/batch = 0.996\n",
      "859/1920 (epoch 6), train_loss = -6.049, time/batch = 0.888\n",
      "860/1920 (epoch 6), train_loss = -5.927, time/batch = 1.018\n",
      "861/1920 (epoch 6), train_loss = -5.985, time/batch = 0.856\n",
      "862/1920 (epoch 6), train_loss = -5.991, time/batch = 0.819\n",
      "863/1920 (epoch 6), train_loss = -6.034, time/batch = 1.038\n",
      "864/1920 (epoch 6), train_loss = -6.047, time/batch = 1.259\n",
      "865/1920 (epoch 6), train_loss = -6.024, time/batch = 0.983\n",
      "866/1920 (epoch 6), train_loss = -6.054, time/batch = 0.771\n",
      "867/1920 (epoch 6), train_loss = -5.949, time/batch = 1.091\n",
      "868/1920 (epoch 6), train_loss = -6.151, time/batch = 1.055\n",
      "869/1920 (epoch 6), train_loss = -6.070, time/batch = 1.193\n",
      "870/1920 (epoch 6), train_loss = -6.017, time/batch = 0.947\n",
      "871/1920 (epoch 6), train_loss = -6.086, time/batch = 0.889\n",
      "872/1920 (epoch 6), train_loss = -6.046, time/batch = 0.909\n",
      "873/1920 (epoch 6), train_loss = -6.023, time/batch = 0.966\n",
      "874/1920 (epoch 6), train_loss = -5.807, time/batch = 1.030\n",
      "875/1920 (epoch 6), train_loss = -5.715, time/batch = 0.866\n",
      "876/1920 (epoch 6), train_loss = -6.104, time/batch = 0.868\n",
      "877/1920 (epoch 6), train_loss = -5.833, time/batch = 1.188\n",
      "878/1920 (epoch 6), train_loss = -5.890, time/batch = 0.993\n",
      "879/1920 (epoch 6), train_loss = -5.970, time/batch = 1.149\n",
      "880/1920 (epoch 6), train_loss = -5.792, time/batch = 1.056\n",
      "881/1920 (epoch 6), train_loss = -5.802, time/batch = 1.194\n",
      "882/1920 (epoch 6), train_loss = -6.018, time/batch = 1.080\n",
      "883/1920 (epoch 6), train_loss = -5.597, time/batch = 0.863\n",
      "884/1920 (epoch 6), train_loss = -5.546, time/batch = 0.879\n",
      "885/1920 (epoch 6), train_loss = -5.899, time/batch = 0.869\n",
      "886/1920 (epoch 6), train_loss = -5.804, time/batch = 0.903\n",
      "887/1920 (epoch 6), train_loss = -5.686, time/batch = 0.812\n",
      "888/1920 (epoch 6), train_loss = -5.771, time/batch = 1.099\n",
      "889/1920 (epoch 6), train_loss = -5.942, time/batch = 0.961\n",
      "890/1920 (epoch 6), train_loss = -5.885, time/batch = 0.885\n",
      "891/1920 (epoch 6), train_loss = -6.074, time/batch = 0.964\n",
      "892/1920 (epoch 6), train_loss = -5.831, time/batch = 0.890\n",
      "893/1920 (epoch 6), train_loss = -5.791, time/batch = 0.943\n",
      "894/1920 (epoch 6), train_loss = -6.182, time/batch = 1.067\n",
      "895/1920 (epoch 6), train_loss = -5.311, time/batch = 1.143\n",
      "896/1920 (epoch 7), train_loss = -4.639, time/batch = 0.798\n",
      "897/1920 (epoch 7), train_loss = -5.390, time/batch = 0.931\n",
      "898/1920 (epoch 7), train_loss = -5.626, time/batch = 0.797\n",
      "899/1920 (epoch 7), train_loss = -5.941, time/batch = 0.836\n",
      "900/1920 (epoch 7), train_loss = -5.945, time/batch = 0.778\n",
      "model saved to models/model.ckpt\n",
      "901/1920 (epoch 7), train_loss = -5.786, time/batch = 0.837\n",
      "902/1920 (epoch 7), train_loss = -5.886, time/batch = 1.012\n",
      "903/1920 (epoch 7), train_loss = -5.918, time/batch = 1.070\n",
      "904/1920 (epoch 7), train_loss = -5.770, time/batch = 1.042\n",
      "905/1920 (epoch 7), train_loss = -5.755, time/batch = 0.894\n",
      "906/1920 (epoch 7), train_loss = -5.828, time/batch = 0.800\n",
      "907/1920 (epoch 7), train_loss = -5.731, time/batch = 1.014\n",
      "908/1920 (epoch 7), train_loss = -5.644, time/batch = 0.877\n",
      "909/1920 (epoch 7), train_loss = -5.710, time/batch = 0.884\n",
      "910/1920 (epoch 7), train_loss = -5.890, time/batch = 0.936\n",
      "911/1920 (epoch 7), train_loss = -6.058, time/batch = 0.851\n",
      "912/1920 (epoch 7), train_loss = -5.999, time/batch = 0.935\n",
      "913/1920 (epoch 7), train_loss = -5.980, time/batch = 1.054\n",
      "914/1920 (epoch 7), train_loss = -6.033, time/batch = 0.880\n",
      "915/1920 (epoch 7), train_loss = -5.871, time/batch = 0.864\n",
      "916/1920 (epoch 7), train_loss = -5.864, time/batch = 0.787\n",
      "917/1920 (epoch 7), train_loss = -5.996, time/batch = 0.963\n",
      "918/1920 (epoch 7), train_loss = -5.947, time/batch = 0.902\n",
      "919/1920 (epoch 7), train_loss = -5.975, time/batch = 0.836\n",
      "920/1920 (epoch 7), train_loss = -6.088, time/batch = 0.916\n",
      "921/1920 (epoch 7), train_loss = -6.061, time/batch = 0.855\n",
      "922/1920 (epoch 7), train_loss = -6.040, time/batch = 0.854\n",
      "923/1920 (epoch 7), train_loss = -6.050, time/batch = 0.928\n",
      "924/1920 (epoch 7), train_loss = -6.112, time/batch = 0.865\n",
      "925/1920 (epoch 7), train_loss = -6.117, time/batch = 0.827\n",
      "926/1920 (epoch 7), train_loss = -6.118, time/batch = 0.794\n",
      "927/1920 (epoch 7), train_loss = -6.055, time/batch = 0.952\n",
      "928/1920 (epoch 7), train_loss = -6.129, time/batch = 0.842\n",
      "929/1920 (epoch 7), train_loss = -6.104, time/batch = 0.813\n",
      "930/1920 (epoch 7), train_loss = -6.099, time/batch = 0.888\n",
      "931/1920 (epoch 7), train_loss = -6.191, time/batch = 0.848\n",
      "932/1920 (epoch 7), train_loss = -6.158, time/batch = 0.885\n",
      "933/1920 (epoch 7), train_loss = -6.254, time/batch = 0.809\n",
      "934/1920 (epoch 7), train_loss = -6.012, time/batch = 0.940\n",
      "935/1920 (epoch 7), train_loss = -5.994, time/batch = 0.908\n",
      "936/1920 (epoch 7), train_loss = -6.111, time/batch = 1.089\n",
      "937/1920 (epoch 7), train_loss = -6.034, time/batch = 1.177\n",
      "938/1920 (epoch 7), train_loss = -6.178, time/batch = 0.913\n",
      "939/1920 (epoch 7), train_loss = -6.094, time/batch = 0.882\n",
      "940/1920 (epoch 7), train_loss = -6.141, time/batch = 0.815\n",
      "941/1920 (epoch 7), train_loss = -5.749, time/batch = 1.045\n",
      "942/1920 (epoch 7), train_loss = -5.946, time/batch = 0.934\n",
      "943/1920 (epoch 7), train_loss = -5.821, time/batch = 0.946\n",
      "944/1920 (epoch 7), train_loss = -6.015, time/batch = 1.248\n",
      "945/1920 (epoch 7), train_loss = -5.744, time/batch = 0.911\n",
      "946/1920 (epoch 7), train_loss = -5.914, time/batch = 1.175\n",
      "947/1920 (epoch 7), train_loss = -6.075, time/batch = 1.048\n",
      "948/1920 (epoch 7), train_loss = -6.070, time/batch = 1.181\n",
      "949/1920 (epoch 7), train_loss = -6.020, time/batch = 0.832\n",
      "950/1920 (epoch 7), train_loss = -6.157, time/batch = 1.059\n",
      "951/1920 (epoch 7), train_loss = -6.068, time/batch = 1.270\n",
      "952/1920 (epoch 7), train_loss = -6.171, time/batch = 1.130\n",
      "953/1920 (epoch 7), train_loss = -6.076, time/batch = 0.966\n",
      "954/1920 (epoch 7), train_loss = -6.024, time/batch = 1.021\n",
      "955/1920 (epoch 7), train_loss = -6.129, time/batch = 1.105\n",
      "956/1920 (epoch 7), train_loss = -6.071, time/batch = 1.119\n",
      "957/1920 (epoch 7), train_loss = -6.145, time/batch = 0.940\n",
      "958/1920 (epoch 7), train_loss = -6.052, time/batch = 1.203\n",
      "959/1920 (epoch 7), train_loss = -6.154, time/batch = 0.919\n",
      "960/1920 (epoch 7), train_loss = -6.189, time/batch = 0.938\n",
      "961/1920 (epoch 7), train_loss = -6.009, time/batch = 0.834\n",
      "962/1920 (epoch 7), train_loss = -6.196, time/batch = 0.703\n",
      "963/1920 (epoch 7), train_loss = -6.057, time/batch = 0.637\n",
      "964/1920 (epoch 7), train_loss = -5.674, time/batch = 0.655\n",
      "965/1920 (epoch 7), train_loss = -6.094, time/batch = 1.028\n",
      "966/1920 (epoch 7), train_loss = -6.081, time/batch = 0.870\n",
      "967/1920 (epoch 7), train_loss = -5.493, time/batch = 1.004\n",
      "968/1920 (epoch 7), train_loss = -5.592, time/batch = 0.911\n",
      "969/1920 (epoch 7), train_loss = -5.950, time/batch = 1.174\n",
      "970/1920 (epoch 7), train_loss = -6.115, time/batch = 1.048\n",
      "971/1920 (epoch 7), train_loss = -6.046, time/batch = 1.061\n",
      "972/1920 (epoch 7), train_loss = -5.962, time/batch = 1.049\n",
      "973/1920 (epoch 7), train_loss = -6.205, time/batch = 0.932\n",
      "974/1920 (epoch 7), train_loss = -5.506, time/batch = 0.995\n",
      "975/1920 (epoch 7), train_loss = -5.214, time/batch = 0.856\n",
      "976/1920 (epoch 7), train_loss = -5.532, time/batch = 1.347\n",
      "977/1920 (epoch 7), train_loss = -5.524, time/batch = 1.029\n",
      "978/1920 (epoch 7), train_loss = -5.585, time/batch = 1.090\n",
      "979/1920 (epoch 7), train_loss = -5.602, time/batch = 1.199\n",
      "980/1920 (epoch 7), train_loss = -5.845, time/batch = 1.190\n",
      "981/1920 (epoch 7), train_loss = -5.928, time/batch = 1.070\n",
      "982/1920 (epoch 7), train_loss = -5.763, time/batch = 1.126\n",
      "983/1920 (epoch 7), train_loss = -6.053, time/batch = 1.110\n",
      "984/1920 (epoch 7), train_loss = -6.027, time/batch = 1.083\n",
      "985/1920 (epoch 7), train_loss = -5.908, time/batch = 1.036\n",
      "986/1920 (epoch 7), train_loss = -6.127, time/batch = 1.280\n",
      "987/1920 (epoch 7), train_loss = -5.960, time/batch = 1.200\n",
      "988/1920 (epoch 7), train_loss = -6.119, time/batch = 1.250\n",
      "989/1920 (epoch 7), train_loss = -5.780, time/batch = 1.190\n",
      "990/1920 (epoch 7), train_loss = -5.895, time/batch = 1.118\n",
      "991/1920 (epoch 7), train_loss = -5.317, time/batch = 1.192\n",
      "992/1920 (epoch 7), train_loss = -5.277, time/batch = 1.155\n",
      "993/1920 (epoch 7), train_loss = -5.999, time/batch = 1.322\n",
      "994/1920 (epoch 7), train_loss = -5.829, time/batch = 0.908\n",
      "995/1920 (epoch 7), train_loss = -5.966, time/batch = 1.032\n",
      "996/1920 (epoch 7), train_loss = -6.227, time/batch = 0.847\n",
      "997/1920 (epoch 7), train_loss = -5.523, time/batch = 1.124\n",
      "998/1920 (epoch 7), train_loss = -5.436, time/batch = 0.890\n",
      "999/1920 (epoch 7), train_loss = -5.913, time/batch = 1.034\n",
      "1000/1920 (epoch 7), train_loss = -5.823, time/batch = 1.026\n",
      "model saved to models/model.ckpt\n",
      "1001/1920 (epoch 7), train_loss = -5.773, time/batch = 1.243\n",
      "1002/1920 (epoch 7), train_loss = -5.944, time/batch = 0.982\n",
      "1003/1920 (epoch 7), train_loss = -6.104, time/batch = 0.958\n",
      "1004/1920 (epoch 7), train_loss = -5.925, time/batch = 1.044\n",
      "1005/1920 (epoch 7), train_loss = -6.153, time/batch = 0.970\n",
      "1006/1920 (epoch 7), train_loss = -5.882, time/batch = 0.815\n",
      "1007/1920 (epoch 7), train_loss = -5.769, time/batch = 0.993\n",
      "1008/1920 (epoch 7), train_loss = -5.916, time/batch = 0.929\n",
      "1009/1920 (epoch 7), train_loss = -6.044, time/batch = 0.941\n",
      "1010/1920 (epoch 7), train_loss = -5.878, time/batch = 0.928\n",
      "1011/1920 (epoch 7), train_loss = -6.033, time/batch = 1.295\n",
      "1012/1920 (epoch 7), train_loss = -6.047, time/batch = 0.955\n",
      "1013/1920 (epoch 7), train_loss = -6.088, time/batch = 1.051\n",
      "1014/1920 (epoch 7), train_loss = -6.072, time/batch = 1.049\n",
      "1015/1920 (epoch 7), train_loss = -6.134, time/batch = 0.916\n",
      "1016/1920 (epoch 7), train_loss = -6.183, time/batch = 0.995\n",
      "1017/1920 (epoch 7), train_loss = -6.229, time/batch = 1.094\n",
      "1018/1920 (epoch 7), train_loss = -6.055, time/batch = 1.140\n",
      "1019/1920 (epoch 7), train_loss = -6.102, time/batch = 1.085\n",
      "1020/1920 (epoch 7), train_loss = -6.122, time/batch = 1.052\n",
      "1021/1920 (epoch 7), train_loss = -6.154, time/batch = 1.044\n",
      "1022/1920 (epoch 7), train_loss = -6.134, time/batch = 0.979\n",
      "1023/1920 (epoch 7), train_loss = -6.109, time/batch = 0.929\n",
      "1024/1920 (epoch 8), train_loss = -5.506, time/batch = 0.791\n",
      "1025/1920 (epoch 8), train_loss = -6.165, time/batch = 1.130\n",
      "1026/1920 (epoch 8), train_loss = -6.200, time/batch = 1.051\n",
      "1027/1920 (epoch 8), train_loss = -6.158, time/batch = 1.166\n",
      "1028/1920 (epoch 8), train_loss = -6.312, time/batch = 0.900\n",
      "1029/1920 (epoch 8), train_loss = -6.287, time/batch = 1.054\n",
      "1030/1920 (epoch 8), train_loss = -6.182, time/batch = 0.824\n",
      "1031/1920 (epoch 8), train_loss = -6.284, time/batch = 1.084\n",
      "1032/1920 (epoch 8), train_loss = -6.185, time/batch = 0.883\n",
      "1033/1920 (epoch 8), train_loss = -6.266, time/batch = 1.026\n",
      "1034/1920 (epoch 8), train_loss = -6.109, time/batch = 0.937\n",
      "1035/1920 (epoch 8), train_loss = -6.029, time/batch = 0.835\n",
      "1036/1920 (epoch 8), train_loss = -6.285, time/batch = 0.680\n",
      "1037/1920 (epoch 8), train_loss = -5.805, time/batch = 0.719\n",
      "1038/1920 (epoch 8), train_loss = -5.502, time/batch = 0.611\n",
      "1039/1920 (epoch 8), train_loss = -5.952, time/batch = 0.578\n",
      "1040/1920 (epoch 8), train_loss = -5.983, time/batch = 0.603\n",
      "1041/1920 (epoch 8), train_loss = -5.548, time/batch = 0.611\n",
      "1042/1920 (epoch 8), train_loss = -5.805, time/batch = 0.599\n",
      "1043/1920 (epoch 8), train_loss = -5.998, time/batch = 0.590\n",
      "1044/1920 (epoch 8), train_loss = -5.877, time/batch = 0.622\n",
      "1045/1920 (epoch 8), train_loss = -5.996, time/batch = 0.582\n",
      "1046/1920 (epoch 8), train_loss = -6.090, time/batch = 0.609\n",
      "1047/1920 (epoch 8), train_loss = -6.186, time/batch = 0.643\n",
      "1048/1920 (epoch 8), train_loss = -6.089, time/batch = 0.598\n",
      "1049/1920 (epoch 8), train_loss = -6.214, time/batch = 0.630\n",
      "1050/1920 (epoch 8), train_loss = -6.057, time/batch = 0.586\n",
      "1051/1920 (epoch 8), train_loss = -6.056, time/batch = 0.657\n",
      "1052/1920 (epoch 8), train_loss = -6.216, time/batch = 0.660\n",
      "1053/1920 (epoch 8), train_loss = -6.075, time/batch = 0.664\n",
      "1054/1920 (epoch 8), train_loss = -6.026, time/batch = 0.696\n",
      "1055/1920 (epoch 8), train_loss = -6.265, time/batch = 0.688\n",
      "1056/1920 (epoch 8), train_loss = -6.063, time/batch = 0.826\n",
      "1057/1920 (epoch 8), train_loss = -6.169, time/batch = 0.642\n",
      "1058/1920 (epoch 8), train_loss = -6.249, time/batch = 0.661\n",
      "1059/1920 (epoch 8), train_loss = -6.221, time/batch = 0.808\n",
      "1060/1920 (epoch 8), train_loss = -6.209, time/batch = 0.802\n",
      "1061/1920 (epoch 8), train_loss = -6.252, time/batch = 0.734\n",
      "1062/1920 (epoch 8), train_loss = -6.359, time/batch = 0.782\n",
      "1063/1920 (epoch 8), train_loss = -6.258, time/batch = 0.714\n",
      "1064/1920 (epoch 8), train_loss = -6.233, time/batch = 0.613\n",
      "1065/1920 (epoch 8), train_loss = -6.345, time/batch = 0.694\n",
      "1066/1920 (epoch 8), train_loss = -6.225, time/batch = 0.662\n",
      "1067/1920 (epoch 8), train_loss = -6.300, time/batch = 0.756\n",
      "1068/1920 (epoch 8), train_loss = -6.221, time/batch = 0.846\n",
      "1069/1920 (epoch 8), train_loss = -6.236, time/batch = 0.703\n",
      "1070/1920 (epoch 8), train_loss = -6.265, time/batch = 0.661\n",
      "1071/1920 (epoch 8), train_loss = -6.204, time/batch = 0.623\n",
      "1072/1920 (epoch 8), train_loss = -6.131, time/batch = 0.588\n",
      "1073/1920 (epoch 8), train_loss = -6.315, time/batch = 0.598\n",
      "1074/1920 (epoch 8), train_loss = -6.263, time/batch = 0.576\n",
      "1075/1920 (epoch 8), train_loss = -6.219, time/batch = 0.570\n",
      "1076/1920 (epoch 8), train_loss = -6.145, time/batch = 0.598\n",
      "1077/1920 (epoch 8), train_loss = -6.348, time/batch = 0.600\n",
      "1078/1920 (epoch 8), train_loss = -5.877, time/batch = 0.581\n",
      "1079/1920 (epoch 8), train_loss = -5.908, time/batch = 0.577\n",
      "1080/1920 (epoch 8), train_loss = -6.391, time/batch = 0.578\n",
      "1081/1920 (epoch 8), train_loss = -5.918, time/batch = 0.593\n",
      "1082/1920 (epoch 8), train_loss = -6.144, time/batch = 0.625\n",
      "1083/1920 (epoch 8), train_loss = -5.905, time/batch = 0.642\n",
      "1084/1920 (epoch 8), train_loss = -5.831, time/batch = 0.596\n",
      "1085/1920 (epoch 8), train_loss = -6.204, time/batch = 0.606\n",
      "1086/1920 (epoch 8), train_loss = -6.347, time/batch = 0.605\n",
      "1087/1920 (epoch 8), train_loss = -5.973, time/batch = 0.611\n",
      "1088/1920 (epoch 8), train_loss = -6.060, time/batch = 0.638\n",
      "1089/1920 (epoch 8), train_loss = -6.239, time/batch = 0.601\n",
      "1090/1920 (epoch 8), train_loss = -6.304, time/batch = 0.596\n",
      "1091/1920 (epoch 8), train_loss = -6.286, time/batch = 0.582\n",
      "1092/1920 (epoch 8), train_loss = -6.268, time/batch = 0.579\n",
      "1093/1920 (epoch 8), train_loss = -6.326, time/batch = 0.595\n",
      "1094/1920 (epoch 8), train_loss = -6.254, time/batch = 0.609\n",
      "1095/1920 (epoch 8), train_loss = -6.370, time/batch = 0.571\n",
      "1096/1920 (epoch 8), train_loss = -6.308, time/batch = 0.591\n",
      "1097/1920 (epoch 8), train_loss = -6.370, time/batch = 0.644\n",
      "1098/1920 (epoch 8), train_loss = -6.304, time/batch = 0.634\n",
      "1099/1920 (epoch 8), train_loss = -6.344, time/batch = 0.638\n",
      "1100/1920 (epoch 8), train_loss = -6.332, time/batch = 0.651\n",
      "model saved to models/model.ckpt\n",
      "1101/1920 (epoch 8), train_loss = -6.261, time/batch = 0.643\n",
      "1102/1920 (epoch 8), train_loss = -6.406, time/batch = 0.586\n",
      "1103/1920 (epoch 8), train_loss = -6.319, time/batch = 0.616\n",
      "1104/1920 (epoch 8), train_loss = -6.339, time/batch = 0.625\n",
      "1105/1920 (epoch 8), train_loss = -6.263, time/batch = 0.601\n",
      "1106/1920 (epoch 8), train_loss = -6.369, time/batch = 0.660\n",
      "1107/1920 (epoch 8), train_loss = -6.241, time/batch = 0.577\n",
      "1108/1920 (epoch 8), train_loss = -6.284, time/batch = 0.599\n",
      "1109/1920 (epoch 8), train_loss = -6.412, time/batch = 0.968\n",
      "1110/1920 (epoch 8), train_loss = -6.338, time/batch = 0.866\n",
      "1111/1920 (epoch 8), train_loss = -6.287, time/batch = 0.846\n",
      "1112/1920 (epoch 8), train_loss = -6.342, time/batch = 0.869\n",
      "1113/1920 (epoch 8), train_loss = -6.457, time/batch = 0.889\n",
      "1114/1920 (epoch 8), train_loss = -6.244, time/batch = 0.827\n",
      "1115/1920 (epoch 8), train_loss = -6.387, time/batch = 0.803\n",
      "1116/1920 (epoch 8), train_loss = -6.361, time/batch = 1.050\n",
      "1117/1920 (epoch 8), train_loss = -6.344, time/batch = 0.872\n",
      "1118/1920 (epoch 8), train_loss = -6.417, time/batch = 0.960\n",
      "1119/1920 (epoch 8), train_loss = -6.273, time/batch = 0.994\n",
      "1120/1920 (epoch 8), train_loss = -6.473, time/batch = 0.970\n",
      "1121/1920 (epoch 8), train_loss = -6.396, time/batch = 0.932\n",
      "1122/1920 (epoch 8), train_loss = -6.268, time/batch = 0.807\n",
      "1123/1920 (epoch 8), train_loss = -6.497, time/batch = 1.101\n",
      "1124/1920 (epoch 8), train_loss = -6.100, time/batch = 0.868\n",
      "1125/1920 (epoch 8), train_loss = -6.017, time/batch = 0.829\n",
      "1126/1920 (epoch 8), train_loss = -6.348, time/batch = 1.035\n",
      "1127/1920 (epoch 8), train_loss = -5.953, time/batch = 1.022\n",
      "1128/1920 (epoch 8), train_loss = -5.805, time/batch = 0.883\n",
      "1129/1920 (epoch 8), train_loss = -5.791, time/batch = 0.917\n",
      "1130/1920 (epoch 8), train_loss = -6.079, time/batch = 1.077\n",
      "1131/1920 (epoch 8), train_loss = -5.771, time/batch = 1.041\n",
      "1132/1920 (epoch 8), train_loss = -5.716, time/batch = 0.890\n",
      "1133/1920 (epoch 8), train_loss = -6.193, time/batch = 0.881\n",
      "1134/1920 (epoch 8), train_loss = -6.255, time/batch = 0.856\n",
      "1135/1920 (epoch 8), train_loss = -6.178, time/batch = 0.832\n",
      "1136/1920 (epoch 8), train_loss = -6.187, time/batch = 0.756\n",
      "1137/1920 (epoch 8), train_loss = -6.156, time/batch = 0.975\n",
      "1138/1920 (epoch 8), train_loss = -6.443, time/batch = 0.872\n",
      "1139/1920 (epoch 8), train_loss = -5.901, time/batch = 0.872\n",
      "1140/1920 (epoch 8), train_loss = -5.865, time/batch = 0.923\n",
      "1141/1920 (epoch 8), train_loss = -6.050, time/batch = 0.883\n",
      "1142/1920 (epoch 8), train_loss = -6.262, time/batch = 0.879\n",
      "1143/1920 (epoch 8), train_loss = -6.261, time/batch = 0.902\n",
      "1144/1920 (epoch 8), train_loss = -6.252, time/batch = 1.081\n",
      "1145/1920 (epoch 8), train_loss = -6.260, time/batch = 0.903\n",
      "1146/1920 (epoch 8), train_loss = -6.244, time/batch = 0.999\n",
      "1147/1920 (epoch 8), train_loss = -6.172, time/batch = 0.915\n",
      "1148/1920 (epoch 8), train_loss = -6.153, time/batch = 1.121\n",
      "1149/1920 (epoch 8), train_loss = -6.246, time/batch = 1.144\n",
      "1150/1920 (epoch 8), train_loss = -6.257, time/batch = 1.194\n",
      "1151/1920 (epoch 8), train_loss = -6.385, time/batch = 1.213\n",
      "1152/1920 (epoch 9), train_loss = -5.788, time/batch = 0.954\n",
      "1153/1920 (epoch 9), train_loss = -6.280, time/batch = 1.291\n",
      "1154/1920 (epoch 9), train_loss = -6.324, time/batch = 1.137\n",
      "1155/1920 (epoch 9), train_loss = -6.228, time/batch = 0.896\n",
      "1156/1920 (epoch 9), train_loss = -6.435, time/batch = 0.905\n",
      "1157/1920 (epoch 9), train_loss = -6.326, time/batch = 1.006\n",
      "1158/1920 (epoch 9), train_loss = -6.435, time/batch = 0.899\n",
      "1159/1920 (epoch 9), train_loss = -6.456, time/batch = 0.948\n",
      "1160/1920 (epoch 9), train_loss = -6.305, time/batch = 0.974\n",
      "1161/1920 (epoch 9), train_loss = -6.421, time/batch = 0.950\n",
      "1162/1920 (epoch 9), train_loss = -6.234, time/batch = 0.942\n",
      "1163/1920 (epoch 9), train_loss = -6.264, time/batch = 1.098\n",
      "1164/1920 (epoch 9), train_loss = -6.475, time/batch = 1.187\n",
      "1165/1920 (epoch 9), train_loss = -6.335, time/batch = 1.070\n",
      "1166/1920 (epoch 9), train_loss = -6.347, time/batch = 1.016\n",
      "1167/1920 (epoch 9), train_loss = -6.147, time/batch = 1.044\n",
      "1168/1920 (epoch 9), train_loss = -6.446, time/batch = 0.908\n",
      "1169/1920 (epoch 9), train_loss = -6.177, time/batch = 0.882\n",
      "1170/1920 (epoch 9), train_loss = -6.241, time/batch = 0.879\n",
      "1171/1920 (epoch 9), train_loss = -6.065, time/batch = 0.874\n",
      "1172/1920 (epoch 9), train_loss = -6.196, time/batch = 0.862\n",
      "1173/1920 (epoch 9), train_loss = -6.289, time/batch = 0.819\n",
      "1174/1920 (epoch 9), train_loss = -6.327, time/batch = 0.994\n",
      "1175/1920 (epoch 9), train_loss = -6.235, time/batch = 0.902\n",
      "1176/1920 (epoch 9), train_loss = -6.360, time/batch = 0.919\n",
      "1177/1920 (epoch 9), train_loss = -6.246, time/batch = 0.920\n",
      "1178/1920 (epoch 9), train_loss = -6.248, time/batch = 0.832\n",
      "1179/1920 (epoch 9), train_loss = -6.305, time/batch = 0.825\n",
      "1180/1920 (epoch 9), train_loss = -6.401, time/batch = 0.867\n",
      "1181/1920 (epoch 9), train_loss = -6.275, time/batch = 1.020\n",
      "1182/1920 (epoch 9), train_loss = -6.416, time/batch = 0.893\n",
      "1183/1920 (epoch 9), train_loss = -6.346, time/batch = 0.844\n",
      "1184/1920 (epoch 9), train_loss = -6.340, time/batch = 0.874\n",
      "1185/1920 (epoch 9), train_loss = -6.347, time/batch = 0.941\n",
      "1186/1920 (epoch 9), train_loss = -6.348, time/batch = 0.906\n",
      "1187/1920 (epoch 9), train_loss = -6.461, time/batch = 0.932\n",
      "1188/1920 (epoch 9), train_loss = -6.537, time/batch = 1.037\n",
      "1189/1920 (epoch 9), train_loss = -6.389, time/batch = 0.845\n",
      "1190/1920 (epoch 9), train_loss = -6.367, time/batch = 0.828\n",
      "1191/1920 (epoch 9), train_loss = -6.411, time/batch = 0.878\n",
      "1192/1920 (epoch 9), train_loss = -6.277, time/batch = 0.859\n",
      "1193/1920 (epoch 9), train_loss = -6.582, time/batch = 0.866\n",
      "1194/1920 (epoch 9), train_loss = -6.376, time/batch = 0.891\n",
      "1195/1920 (epoch 9), train_loss = -6.375, time/batch = 1.069\n",
      "1196/1920 (epoch 9), train_loss = -6.060, time/batch = 0.971\n",
      "1197/1920 (epoch 9), train_loss = -5.876, time/batch = 0.955\n",
      "1198/1920 (epoch 9), train_loss = -6.394, time/batch = 0.993\n",
      "1199/1920 (epoch 9), train_loss = -6.002, time/batch = 0.985\n",
      "1200/1920 (epoch 9), train_loss = -6.024, time/batch = 0.858\n",
      "model saved to models/model.ckpt\n",
      "1201/1920 (epoch 9), train_loss = -6.410, time/batch = 0.966\n",
      "1202/1920 (epoch 9), train_loss = -6.180, time/batch = 0.920\n",
      "1203/1920 (epoch 9), train_loss = -6.309, time/batch = 0.919\n",
      "1204/1920 (epoch 9), train_loss = -6.220, time/batch = 1.283\n",
      "1205/1920 (epoch 9), train_loss = -6.158, time/batch = 0.929\n",
      "1206/1920 (epoch 9), train_loss = -6.372, time/batch = 0.873\n",
      "1207/1920 (epoch 9), train_loss = -6.405, time/batch = 0.761\n",
      "1208/1920 (epoch 9), train_loss = -6.428, time/batch = 1.161\n",
      "1209/1920 (epoch 9), train_loss = -6.464, time/batch = 1.226\n",
      "1210/1920 (epoch 9), train_loss = -6.272, time/batch = 0.843\n",
      "1211/1920 (epoch 9), train_loss = -6.267, time/batch = 1.142\n",
      "1212/1920 (epoch 9), train_loss = -6.369, time/batch = 1.093\n",
      "1213/1920 (epoch 9), train_loss = -6.439, time/batch = 1.083\n",
      "1214/1920 (epoch 9), train_loss = -6.314, time/batch = 0.967\n",
      "1215/1920 (epoch 9), train_loss = -6.437, time/batch = 1.230\n",
      "1216/1920 (epoch 9), train_loss = -6.410, time/batch = 1.040\n",
      "1217/1920 (epoch 9), train_loss = -6.394, time/batch = 0.898\n",
      "1218/1920 (epoch 9), train_loss = -6.463, time/batch = 0.938\n",
      "1219/1920 (epoch 9), train_loss = -6.403, time/batch = 0.945\n",
      "1220/1920 (epoch 9), train_loss = -6.456, time/batch = 1.291\n",
      "1221/1920 (epoch 9), train_loss = -6.473, time/batch = 1.096\n",
      "1222/1920 (epoch 9), train_loss = -6.464, time/batch = 1.435\n",
      "1223/1920 (epoch 9), train_loss = -6.342, time/batch = 1.001\n",
      "1224/1920 (epoch 9), train_loss = -6.460, time/batch = 0.989\n",
      "1225/1920 (epoch 9), train_loss = -6.469, time/batch = 1.259\n",
      "1226/1920 (epoch 9), train_loss = -6.353, time/batch = 0.936\n",
      "1227/1920 (epoch 9), train_loss = -6.579, time/batch = 0.843\n",
      "1228/1920 (epoch 9), train_loss = -6.390, time/batch = 0.837\n",
      "1229/1920 (epoch 9), train_loss = -6.349, time/batch = 1.040\n",
      "1230/1920 (epoch 9), train_loss = -6.583, time/batch = 0.929\n",
      "1231/1920 (epoch 9), train_loss = -6.401, time/batch = 0.994\n",
      "1232/1920 (epoch 9), train_loss = -6.591, time/batch = 1.156\n",
      "1233/1920 (epoch 9), train_loss = -6.000, time/batch = 0.942\n",
      "1234/1920 (epoch 9), train_loss = -5.800, time/batch = 1.050\n",
      "1235/1920 (epoch 9), train_loss = -6.138, time/batch = 1.263\n",
      "1236/1920 (epoch 9), train_loss = -6.252, time/batch = 1.296\n",
      "1237/1920 (epoch 9), train_loss = -6.190, time/batch = 1.243\n",
      "1238/1920 (epoch 9), train_loss = -6.219, time/batch = 1.129\n",
      "1239/1920 (epoch 9), train_loss = -6.334, time/batch = 1.185\n",
      "1240/1920 (epoch 9), train_loss = -6.335, time/batch = 1.090\n",
      "1241/1920 (epoch 9), train_loss = -6.289, time/batch = 1.177\n",
      "1242/1920 (epoch 9), train_loss = -6.455, time/batch = 1.201\n",
      "1243/1920 (epoch 9), train_loss = -6.490, time/batch = 1.314\n",
      "1244/1920 (epoch 9), train_loss = -6.338, time/batch = 1.165\n",
      "1245/1920 (epoch 9), train_loss = -6.447, time/batch = 0.946\n",
      "1246/1920 (epoch 9), train_loss = -6.221, time/batch = 1.110\n",
      "1247/1920 (epoch 9), train_loss = -6.198, time/batch = 1.087\n",
      "1248/1920 (epoch 9), train_loss = -6.406, time/batch = 1.282\n",
      "1249/1920 (epoch 9), train_loss = -6.407, time/batch = 1.185\n",
      "1250/1920 (epoch 9), train_loss = -6.425, time/batch = 1.343\n",
      "1251/1920 (epoch 9), train_loss = -6.519, time/batch = 1.023\n",
      "1252/1920 (epoch 9), train_loss = -6.368, time/batch = 0.984\n",
      "1253/1920 (epoch 9), train_loss = -6.379, time/batch = 0.963\n",
      "1254/1920 (epoch 9), train_loss = -6.491, time/batch = 0.882\n",
      "1255/1920 (epoch 9), train_loss = -6.492, time/batch = 0.847\n",
      "1256/1920 (epoch 9), train_loss = -6.534, time/batch = 1.092\n",
      "1257/1920 (epoch 9), train_loss = -6.448, time/batch = 1.103\n",
      "1258/1920 (epoch 9), train_loss = -6.338, time/batch = 0.915\n",
      "1259/1920 (epoch 9), train_loss = -6.632, time/batch = 0.822\n",
      "1260/1920 (epoch 9), train_loss = -6.206, time/batch = 0.964\n",
      "1261/1920 (epoch 9), train_loss = -6.210, time/batch = 0.916\n",
      "1262/1920 (epoch 9), train_loss = -6.485, time/batch = 0.929\n",
      "1263/1920 (epoch 9), train_loss = -6.333, time/batch = 0.876\n",
      "1264/1920 (epoch 9), train_loss = -6.369, time/batch = 1.072\n",
      "1265/1920 (epoch 9), train_loss = -6.451, time/batch = 0.857\n",
      "1266/1920 (epoch 9), train_loss = -6.476, time/batch = 0.962\n",
      "1267/1920 (epoch 9), train_loss = -6.327, time/batch = 0.979\n",
      "1268/1920 (epoch 9), train_loss = -6.444, time/batch = 0.901\n",
      "1269/1920 (epoch 9), train_loss = -6.378, time/batch = 0.854\n",
      "1270/1920 (epoch 9), train_loss = -6.428, time/batch = 0.826\n",
      "1271/1920 (epoch 9), train_loss = -6.523, time/batch = 1.036\n",
      "1272/1920 (epoch 9), train_loss = -6.395, time/batch = 0.934\n",
      "1273/1920 (epoch 9), train_loss = -6.399, time/batch = 0.937\n",
      "1274/1920 (epoch 9), train_loss = -6.481, time/batch = 0.919\n",
      "1275/1920 (epoch 9), train_loss = -6.480, time/batch = 0.930\n",
      "1276/1920 (epoch 9), train_loss = -6.274, time/batch = 0.930\n",
      "1277/1920 (epoch 9), train_loss = -6.499, time/batch = 0.983\n",
      "1278/1920 (epoch 9), train_loss = -6.474, time/batch = 1.323\n",
      "1279/1920 (epoch 9), train_loss = -6.332, time/batch = 0.868\n",
      "1280/1920 (epoch 10), train_loss = -5.969, time/batch = 1.093\n",
      "1281/1920 (epoch 10), train_loss = -6.143, time/batch = 1.248\n",
      "1282/1920 (epoch 10), train_loss = -5.868, time/batch = 1.208\n",
      "1283/1920 (epoch 10), train_loss = -6.119, time/batch = 0.967\n",
      "1284/1920 (epoch 10), train_loss = -6.231, time/batch = 1.016\n",
      "1285/1920 (epoch 10), train_loss = -6.255, time/batch = 0.929\n",
      "1286/1920 (epoch 10), train_loss = -6.214, time/batch = 1.018\n",
      "1287/1920 (epoch 10), train_loss = -6.317, time/batch = 1.095\n",
      "1288/1920 (epoch 10), train_loss = -6.448, time/batch = 1.049\n",
      "1289/1920 (epoch 10), train_loss = -6.497, time/batch = 0.908\n",
      "1290/1920 (epoch 10), train_loss = -6.301, time/batch = 0.918\n",
      "1291/1920 (epoch 10), train_loss = -6.264, time/batch = 1.155\n",
      "1292/1920 (epoch 10), train_loss = -6.616, time/batch = 0.959\n",
      "1293/1920 (epoch 10), train_loss = -6.210, time/batch = 0.989\n",
      "1294/1920 (epoch 10), train_loss = -6.235, time/batch = 1.017\n",
      "1295/1920 (epoch 10), train_loss = -5.911, time/batch = 1.140\n",
      "1296/1920 (epoch 10), train_loss = -5.953, time/batch = 1.144\n",
      "1297/1920 (epoch 10), train_loss = -6.360, time/batch = 0.925\n",
      "1298/1920 (epoch 10), train_loss = -6.440, time/batch = 1.076\n",
      "1299/1920 (epoch 10), train_loss = -6.292, time/batch = 0.929\n",
      "1300/1920 (epoch 10), train_loss = -6.308, time/batch = 1.214\n",
      "model saved to models/model.ckpt\n",
      "1301/1920 (epoch 10), train_loss = -6.310, time/batch = 0.729\n",
      "1302/1920 (epoch 10), train_loss = -6.264, time/batch = 0.857\n",
      "1303/1920 (epoch 10), train_loss = -6.444, time/batch = 0.848\n",
      "1304/1920 (epoch 10), train_loss = -6.411, time/batch = 0.939\n",
      "1305/1920 (epoch 10), train_loss = -6.413, time/batch = 0.834\n",
      "1306/1920 (epoch 10), train_loss = -6.254, time/batch = 0.822\n",
      "1307/1920 (epoch 10), train_loss = -6.356, time/batch = 0.927\n",
      "1308/1920 (epoch 10), train_loss = -6.472, time/batch = 0.892\n",
      "1309/1920 (epoch 10), train_loss = -6.430, time/batch = 0.854\n",
      "1310/1920 (epoch 10), train_loss = -6.448, time/batch = 0.761\n",
      "1311/1920 (epoch 10), train_loss = -6.518, time/batch = 0.978\n",
      "1312/1920 (epoch 10), train_loss = -6.417, time/batch = 0.987\n",
      "1313/1920 (epoch 10), train_loss = -6.493, time/batch = 0.846\n",
      "1314/1920 (epoch 10), train_loss = -6.476, time/batch = 0.974\n",
      "1315/1920 (epoch 10), train_loss = -6.463, time/batch = 0.872\n",
      "1316/1920 (epoch 10), train_loss = -6.542, time/batch = 0.844\n",
      "1317/1920 (epoch 10), train_loss = -6.416, time/batch = 1.100\n",
      "1318/1920 (epoch 10), train_loss = -6.501, time/batch = 1.338\n",
      "1319/1920 (epoch 10), train_loss = -6.483, time/batch = 1.173\n",
      "1320/1920 (epoch 10), train_loss = -6.479, time/batch = 0.981\n",
      "1321/1920 (epoch 10), train_loss = -6.625, time/batch = 1.046\n",
      "1322/1920 (epoch 10), train_loss = -6.609, time/batch = 1.006\n",
      "1323/1920 (epoch 10), train_loss = -6.569, time/batch = 0.891\n",
      "1324/1920 (epoch 10), train_loss = -6.392, time/batch = 0.804\n",
      "1325/1920 (epoch 10), train_loss = -6.582, time/batch = 1.049\n",
      "1326/1920 (epoch 10), train_loss = -6.496, time/batch = 0.891\n",
      "1327/1920 (epoch 10), train_loss = -6.594, time/batch = 0.857\n",
      "1328/1920 (epoch 10), train_loss = -6.483, time/batch = 1.039\n",
      "1329/1920 (epoch 10), train_loss = -6.401, time/batch = 0.931\n",
      "1330/1920 (epoch 10), train_loss = -6.450, time/batch = 0.845\n",
      "1331/1920 (epoch 10), train_loss = -6.486, time/batch = 0.761\n",
      "1332/1920 (epoch 10), train_loss = -6.491, time/batch = 1.109\n",
      "1333/1920 (epoch 10), train_loss = -6.435, time/batch = 0.907\n",
      "1334/1920 (epoch 10), train_loss = -6.542, time/batch = 0.809\n",
      "1335/1920 (epoch 10), train_loss = -6.174, time/batch = 0.964\n",
      "1336/1920 (epoch 10), train_loss = -6.207, time/batch = 0.811\n",
      "1337/1920 (epoch 10), train_loss = -6.348, time/batch = 0.850\n",
      "1338/1920 (epoch 10), train_loss = -6.479, time/batch = 0.828\n",
      "1339/1920 (epoch 10), train_loss = -6.308, time/batch = 1.046\n",
      "1340/1920 (epoch 10), train_loss = -6.511, time/batch = 0.847\n",
      "1341/1920 (epoch 10), train_loss = -6.259, time/batch = 0.878\n",
      "1342/1920 (epoch 10), train_loss = -6.188, time/batch = 0.946\n",
      "1343/1920 (epoch 10), train_loss = -6.685, time/batch = 0.855\n",
      "1344/1920 (epoch 10), train_loss = -6.412, time/batch = 0.897\n",
      "1345/1920 (epoch 10), train_loss = -6.379, time/batch = 0.964\n",
      "1346/1920 (epoch 10), train_loss = -6.579, time/batch = 0.876\n",
      "1347/1920 (epoch 10), train_loss = -6.336, time/batch = 0.962\n",
      "1348/1920 (epoch 10), train_loss = -6.414, time/batch = 0.845\n",
      "1349/1920 (epoch 10), train_loss = -6.590, time/batch = 1.015\n",
      "1350/1920 (epoch 10), train_loss = -6.546, time/batch = 0.858\n",
      "1351/1920 (epoch 10), train_loss = -6.530, time/batch = 0.874\n",
      "1352/1920 (epoch 10), train_loss = -6.574, time/batch = 1.021\n",
      "1353/1920 (epoch 10), train_loss = -6.496, time/batch = 0.952\n",
      "1354/1920 (epoch 10), train_loss = -6.571, time/batch = 0.845\n",
      "1355/1920 (epoch 10), train_loss = -6.703, time/batch = 0.849\n",
      "1356/1920 (epoch 10), train_loss = -6.526, time/batch = 0.686\n",
      "1357/1920 (epoch 10), train_loss = -6.600, time/batch = 0.645\n",
      "1358/1920 (epoch 10), train_loss = -6.484, time/batch = 0.656\n",
      "1359/1920 (epoch 10), train_loss = -6.378, time/batch = 0.861\n",
      "1360/1920 (epoch 10), train_loss = -6.816, time/batch = 0.944\n",
      "1361/1920 (epoch 10), train_loss = -6.448, time/batch = 0.887\n",
      "1362/1920 (epoch 10), train_loss = -6.535, time/batch = 0.768\n",
      "1363/1920 (epoch 10), train_loss = -6.630, time/batch = 0.981\n",
      "1364/1920 (epoch 10), train_loss = -6.506, time/batch = 0.852\n",
      "1365/1920 (epoch 10), train_loss = -6.674, time/batch = 0.849\n",
      "1366/1920 (epoch 10), train_loss = -6.480, time/batch = 0.889\n",
      "1367/1920 (epoch 10), train_loss = -6.495, time/batch = 0.862\n",
      "1368/1920 (epoch 10), train_loss = -6.743, time/batch = 0.838\n",
      "1369/1920 (epoch 10), train_loss = -6.241, time/batch = 0.882\n",
      "1370/1920 (epoch 10), train_loss = -6.285, time/batch = 0.919\n",
      "1371/1920 (epoch 10), train_loss = -6.539, time/batch = 0.861\n",
      "1372/1920 (epoch 10), train_loss = -6.607, time/batch = 0.803\n",
      "1373/1920 (epoch 10), train_loss = -6.320, time/batch = 0.918\n",
      "1374/1920 (epoch 10), train_loss = -6.354, time/batch = 0.815\n",
      "1375/1920 (epoch 10), train_loss = -6.542, time/batch = 0.903\n",
      "1376/1920 (epoch 10), train_loss = -6.618, time/batch = 0.899\n",
      "1377/1920 (epoch 10), train_loss = -6.422, time/batch = 0.590\n",
      "1378/1920 (epoch 10), train_loss = -6.514, time/batch = 0.695\n",
      "1379/1920 (epoch 10), train_loss = -6.492, time/batch = 0.661\n",
      "1380/1920 (epoch 10), train_loss = -6.528, time/batch = 0.641\n",
      "1381/1920 (epoch 10), train_loss = -6.559, time/batch = 0.609\n",
      "1382/1920 (epoch 10), train_loss = -6.569, time/batch = 0.629\n",
      "1383/1920 (epoch 10), train_loss = -6.505, time/batch = 0.641\n",
      "1384/1920 (epoch 10), train_loss = -6.549, time/batch = 0.632\n",
      "1385/1920 (epoch 10), train_loss = -6.727, time/batch = 0.637\n",
      "1386/1920 (epoch 10), train_loss = -6.585, time/batch = 0.575\n",
      "1387/1920 (epoch 10), train_loss = -6.531, time/batch = 0.626\n",
      "1388/1920 (epoch 10), train_loss = -6.611, time/batch = 0.650\n",
      "1389/1920 (epoch 10), train_loss = -6.525, time/batch = 0.623\n",
      "1390/1920 (epoch 10), train_loss = -6.614, time/batch = 0.583\n",
      "1391/1920 (epoch 10), train_loss = -6.610, time/batch = 0.661\n",
      "1392/1920 (epoch 10), train_loss = -6.508, time/batch = 0.661\n",
      "1393/1920 (epoch 10), train_loss = -6.711, time/batch = 0.620\n",
      "1394/1920 (epoch 10), train_loss = -6.669, time/batch = 0.640\n",
      "1395/1920 (epoch 10), train_loss = -6.709, time/batch = 0.648\n",
      "1396/1920 (epoch 10), train_loss = -6.563, time/batch = 0.692\n",
      "1397/1920 (epoch 10), train_loss = -6.635, time/batch = 0.674\n",
      "1398/1920 (epoch 10), train_loss = -6.702, time/batch = 0.662\n",
      "1399/1920 (epoch 10), train_loss = -6.627, time/batch = 0.634\n",
      "1400/1920 (epoch 10), train_loss = -6.705, time/batch = 0.612\n",
      "model saved to models/model.ckpt\n",
      "1401/1920 (epoch 10), train_loss = -6.514, time/batch = 0.607\n",
      "1402/1920 (epoch 10), train_loss = -6.607, time/batch = 0.647\n",
      "1403/1920 (epoch 10), train_loss = -6.807, time/batch = 0.606\n",
      "1404/1920 (epoch 10), train_loss = -6.635, time/batch = 0.614\n",
      "1405/1920 (epoch 10), train_loss = -6.665, time/batch = 0.661\n",
      "1406/1920 (epoch 10), train_loss = -6.576, time/batch = 0.624\n",
      "1407/1920 (epoch 10), train_loss = -6.683, time/batch = 0.639\n",
      "1408/1920 (epoch 11), train_loss = -5.861, time/batch = 0.604\n",
      "1409/1920 (epoch 11), train_loss = -6.566, time/batch = 0.605\n",
      "1410/1920 (epoch 11), train_loss = -6.481, time/batch = 0.616\n",
      "1411/1920 (epoch 11), train_loss = -6.553, time/batch = 0.630\n",
      "1412/1920 (epoch 11), train_loss = -6.612, time/batch = 0.639\n",
      "1413/1920 (epoch 11), train_loss = -6.517, time/batch = 0.630\n",
      "1414/1920 (epoch 11), train_loss = -6.699, time/batch = 0.641\n",
      "1415/1920 (epoch 11), train_loss = -6.591, time/batch = 0.624\n",
      "1416/1920 (epoch 11), train_loss = -6.697, time/batch = 0.622\n",
      "1417/1920 (epoch 11), train_loss = -6.773, time/batch = 0.612\n",
      "1418/1920 (epoch 11), train_loss = -6.263, time/batch = 0.597\n",
      "1419/1920 (epoch 11), train_loss = -6.194, time/batch = 0.610\n",
      "1420/1920 (epoch 11), train_loss = -6.649, time/batch = 0.624\n",
      "1421/1920 (epoch 11), train_loss = -6.394, time/batch = 0.624\n",
      "1422/1920 (epoch 11), train_loss = -6.422, time/batch = 0.595\n",
      "1423/1920 (epoch 11), train_loss = -6.563, time/batch = 0.595\n",
      "1424/1920 (epoch 11), train_loss = -6.656, time/batch = 0.597\n",
      "1425/1920 (epoch 11), train_loss = -6.636, time/batch = 0.640\n",
      "1426/1920 (epoch 11), train_loss = -6.600, time/batch = 0.672\n",
      "1427/1920 (epoch 11), train_loss = -6.577, time/batch = 0.622\n",
      "1428/1920 (epoch 11), train_loss = -6.535, time/batch = 0.633\n",
      "1429/1920 (epoch 11), train_loss = -6.626, time/batch = 0.603\n",
      "1430/1920 (epoch 11), train_loss = -6.734, time/batch = 0.612\n",
      "1431/1920 (epoch 11), train_loss = -6.736, time/batch = 0.613\n",
      "1432/1920 (epoch 11), train_loss = -6.797, time/batch = 0.607\n",
      "1433/1920 (epoch 11), train_loss = -6.683, time/batch = 0.609\n",
      "1434/1920 (epoch 11), train_loss = -6.655, time/batch = 0.623\n",
      "1435/1920 (epoch 11), train_loss = -6.675, time/batch = 0.638\n",
      "1436/1920 (epoch 11), train_loss = -6.661, time/batch = 0.619\n",
      "1437/1920 (epoch 11), train_loss = -6.677, time/batch = 0.591\n",
      "1438/1920 (epoch 11), train_loss = -6.643, time/batch = 0.609\n",
      "1439/1920 (epoch 11), train_loss = -6.752, time/batch = 0.596\n",
      "1440/1920 (epoch 11), train_loss = -6.690, time/batch = 0.631\n",
      "1441/1920 (epoch 11), train_loss = -6.547, time/batch = 0.905\n",
      "1442/1920 (epoch 11), train_loss = -6.626, time/batch = 0.702\n",
      "1443/1920 (epoch 11), train_loss = -6.812, time/batch = 0.646\n",
      "1444/1920 (epoch 11), train_loss = -6.682, time/batch = 1.163\n",
      "1445/1920 (epoch 11), train_loss = -6.671, time/batch = 0.988\n",
      "1446/1920 (epoch 11), train_loss = -6.659, time/batch = 0.848\n",
      "1447/1920 (epoch 11), train_loss = -6.790, time/batch = 0.998\n",
      "1448/1920 (epoch 11), train_loss = -6.640, time/batch = 0.896\n",
      "1449/1920 (epoch 11), train_loss = -6.741, time/batch = 0.831\n",
      "1450/1920 (epoch 11), train_loss = -6.514, time/batch = 0.815\n",
      "1451/1920 (epoch 11), train_loss = -6.589, time/batch = 0.987\n",
      "1452/1920 (epoch 11), train_loss = -6.703, time/batch = 0.873\n",
      "1453/1920 (epoch 11), train_loss = -6.304, time/batch = 0.863\n",
      "1454/1920 (epoch 11), train_loss = -6.617, time/batch = 0.935\n",
      "1455/1920 (epoch 11), train_loss = -6.577, time/batch = 0.869\n",
      "1456/1920 (epoch 11), train_loss = -6.438, time/batch = 0.820\n",
      "1457/1920 (epoch 11), train_loss = -6.599, time/batch = 0.758\n",
      "1458/1920 (epoch 11), train_loss = -6.719, time/batch = 0.993\n",
      "1459/1920 (epoch 11), train_loss = -6.455, time/batch = 0.892\n",
      "1460/1920 (epoch 11), train_loss = -6.462, time/batch = 0.825\n",
      "1461/1920 (epoch 11), train_loss = -6.629, time/batch = 0.880\n",
      "1462/1920 (epoch 11), train_loss = -6.650, time/batch = 0.853\n",
      "1463/1920 (epoch 11), train_loss = -6.502, time/batch = 0.834\n",
      "1464/1920 (epoch 11), train_loss = -6.786, time/batch = 0.875\n",
      "1465/1920 (epoch 11), train_loss = -6.472, time/batch = 0.933\n",
      "1466/1920 (epoch 11), train_loss = -6.422, time/batch = 0.842\n",
      "1467/1920 (epoch 11), train_loss = -6.799, time/batch = 0.840\n",
      "1468/1920 (epoch 11), train_loss = -6.554, time/batch = 0.918\n",
      "1469/1920 (epoch 11), train_loss = -6.702, time/batch = 0.887\n",
      "1470/1920 (epoch 11), train_loss = -6.723, time/batch = 0.858\n",
      "1471/1920 (epoch 11), train_loss = -6.618, time/batch = 0.832\n",
      "1472/1920 (epoch 11), train_loss = -6.760, time/batch = 0.980\n",
      "1473/1920 (epoch 11), train_loss = -6.544, time/batch = 0.869\n",
      "1474/1920 (epoch 11), train_loss = -6.555, time/batch = 0.865\n",
      "1475/1920 (epoch 11), train_loss = -6.545, time/batch = 0.916\n",
      "1476/1920 (epoch 11), train_loss = -6.708, time/batch = 0.922\n",
      "1477/1920 (epoch 11), train_loss = -6.728, time/batch = 0.843\n",
      "1478/1920 (epoch 11), train_loss = -6.787, time/batch = 0.825\n",
      "1479/1920 (epoch 11), train_loss = -6.594, time/batch = 0.980\n",
      "1480/1920 (epoch 11), train_loss = -6.709, time/batch = 0.872\n",
      "1481/1920 (epoch 11), train_loss = -6.766, time/batch = 1.064\n",
      "1482/1920 (epoch 11), train_loss = -6.686, time/batch = 0.836\n",
      "1483/1920 (epoch 11), train_loss = -6.633, time/batch = 0.882\n",
      "1484/1920 (epoch 11), train_loss = -6.688, time/batch = 0.845\n",
      "1485/1920 (epoch 11), train_loss = -6.756, time/batch = 0.738\n",
      "1486/1920 (epoch 11), train_loss = -6.810, time/batch = 1.077\n",
      "1487/1920 (epoch 11), train_loss = -6.686, time/batch = 0.949\n",
      "1488/1920 (epoch 11), train_loss = -6.638, time/batch = 0.864\n",
      "1489/1920 (epoch 11), train_loss = -6.774, time/batch = 0.948\n",
      "1490/1920 (epoch 11), train_loss = -6.785, time/batch = 0.937\n",
      "1491/1920 (epoch 11), train_loss = -6.744, time/batch = 1.045\n",
      "1492/1920 (epoch 11), train_loss = -6.711, time/batch = 0.922\n",
      "1493/1920 (epoch 11), train_loss = -6.822, time/batch = 0.719\n",
      "1494/1920 (epoch 11), train_loss = -6.504, time/batch = 0.805\n",
      "1495/1920 (epoch 11), train_loss = -6.614, time/batch = 1.140\n",
      "1496/1920 (epoch 11), train_loss = -6.548, time/batch = 1.000\n",
      "1497/1920 (epoch 11), train_loss = -6.724, time/batch = 0.995\n",
      "1498/1920 (epoch 11), train_loss = -6.583, time/batch = 1.006\n",
      "1499/1920 (epoch 11), train_loss = -6.745, time/batch = 0.966\n",
      "1500/1920 (epoch 11), train_loss = -6.718, time/batch = 1.078\n",
      "model saved to models/model.ckpt\n",
      "1501/1920 (epoch 11), train_loss = -6.638, time/batch = 0.735\n",
      "1502/1920 (epoch 11), train_loss = -6.737, time/batch = 0.971\n",
      "1503/1920 (epoch 11), train_loss = -6.346, time/batch = 0.857\n",
      "1504/1920 (epoch 11), train_loss = -6.229, time/batch = 0.995\n",
      "1505/1920 (epoch 11), train_loss = -6.723, time/batch = 0.865\n",
      "1506/1920 (epoch 11), train_loss = -6.325, time/batch = 0.976\n",
      "1507/1920 (epoch 11), train_loss = -6.225, time/batch = 0.873\n",
      "1508/1920 (epoch 11), train_loss = -6.521, time/batch = 0.841\n",
      "1509/1920 (epoch 11), train_loss = -6.482, time/batch = 0.959\n",
      "1510/1920 (epoch 11), train_loss = -6.598, time/batch = 0.975\n",
      "1511/1920 (epoch 11), train_loss = -6.695, time/batch = 0.838\n",
      "1512/1920 (epoch 11), train_loss = -6.650, time/batch = 0.876\n",
      "1513/1920 (epoch 11), train_loss = -6.581, time/batch = 1.003\n",
      "1514/1920 (epoch 11), train_loss = -6.724, time/batch = 0.815\n",
      "1515/1920 (epoch 11), train_loss = -6.623, time/batch = 0.829\n",
      "1516/1920 (epoch 11), train_loss = -6.928, time/batch = 1.046\n",
      "1517/1920 (epoch 11), train_loss = -6.596, time/batch = 0.817\n",
      "1518/1920 (epoch 11), train_loss = -6.814, time/batch = 0.823\n",
      "1519/1920 (epoch 11), train_loss = -6.109, time/batch = 0.966\n",
      "1520/1920 (epoch 11), train_loss = -5.957, time/batch = 0.874\n",
      "1521/1920 (epoch 11), train_loss = -6.662, time/batch = 0.829\n",
      "1522/1920 (epoch 11), train_loss = -6.161, time/batch = 0.848\n",
      "1523/1920 (epoch 11), train_loss = -5.791, time/batch = 1.038\n",
      "1524/1920 (epoch 11), train_loss = -6.338, time/batch = 0.893\n",
      "1525/1920 (epoch 11), train_loss = -6.490, time/batch = 0.940\n",
      "1526/1920 (epoch 11), train_loss = -6.511, time/batch = 0.849\n",
      "1527/1920 (epoch 11), train_loss = -6.378, time/batch = 0.870\n",
      "1528/1920 (epoch 11), train_loss = -6.765, time/batch = 0.905\n",
      "1529/1920 (epoch 11), train_loss = -6.412, time/batch = 0.791\n",
      "1530/1920 (epoch 11), train_loss = -6.369, time/batch = 1.486\n",
      "1531/1920 (epoch 11), train_loss = -6.588, time/batch = 0.921\n",
      "1532/1920 (epoch 11), train_loss = -6.630, time/batch = 0.936\n",
      "1533/1920 (epoch 11), train_loss = -6.531, time/batch = 0.953\n",
      "1534/1920 (epoch 11), train_loss = -6.484, time/batch = 0.937\n",
      "1535/1920 (epoch 11), train_loss = -6.768, time/batch = 0.867\n",
      "1536/1920 (epoch 12), train_loss = -6.090, time/batch = 0.969\n",
      "1537/1920 (epoch 12), train_loss = -6.903, time/batch = 0.996\n",
      "1538/1920 (epoch 12), train_loss = -6.616, time/batch = 0.948\n",
      "1539/1920 (epoch 12), train_loss = -6.573, time/batch = 1.300\n",
      "1540/1920 (epoch 12), train_loss = -6.741, time/batch = 1.023\n",
      "1541/1920 (epoch 12), train_loss = -6.736, time/batch = 0.901\n",
      "1542/1920 (epoch 12), train_loss = -6.773, time/batch = 0.878\n",
      "1543/1920 (epoch 12), train_loss = -6.659, time/batch = 1.042\n",
      "1544/1920 (epoch 12), train_loss = -6.598, time/batch = 0.895\n",
      "1545/1920 (epoch 12), train_loss = -6.748, time/batch = 0.879\n",
      "1546/1920 (epoch 12), train_loss = -6.815, time/batch = 0.908\n",
      "1547/1920 (epoch 12), train_loss = -6.794, time/batch = 0.872\n",
      "1548/1920 (epoch 12), train_loss = -6.765, time/batch = 1.059\n",
      "1549/1920 (epoch 12), train_loss = -6.672, time/batch = 1.144\n",
      "1550/1920 (epoch 12), train_loss = -6.852, time/batch = 1.035\n",
      "1551/1920 (epoch 12), train_loss = -6.633, time/batch = 0.865\n",
      "1552/1920 (epoch 12), train_loss = -6.659, time/batch = 0.950\n",
      "1553/1920 (epoch 12), train_loss = -6.694, time/batch = 1.152\n",
      "1554/1920 (epoch 12), train_loss = -6.821, time/batch = 1.176\n",
      "1555/1920 (epoch 12), train_loss = -6.799, time/batch = 1.042\n",
      "1556/1920 (epoch 12), train_loss = -6.676, time/batch = 0.830\n",
      "1557/1920 (epoch 12), train_loss = -6.726, time/batch = 1.031\n",
      "1558/1920 (epoch 12), train_loss = -6.762, time/batch = 0.909\n",
      "1559/1920 (epoch 12), train_loss = -6.786, time/batch = 0.994\n",
      "1560/1920 (epoch 12), train_loss = -6.718, time/batch = 1.131\n",
      "1561/1920 (epoch 12), train_loss = -6.835, time/batch = 0.916\n",
      "1562/1920 (epoch 12), train_loss = -6.634, time/batch = 0.909\n",
      "1563/1920 (epoch 12), train_loss = -6.465, time/batch = 0.791\n",
      "1564/1920 (epoch 12), train_loss = -6.799, time/batch = 0.990\n",
      "1565/1920 (epoch 12), train_loss = -6.403, time/batch = 0.878\n",
      "1566/1920 (epoch 12), train_loss = -5.945, time/batch = 0.858\n",
      "1567/1920 (epoch 12), train_loss = -6.420, time/batch = 0.985\n",
      "1568/1920 (epoch 12), train_loss = -6.610, time/batch = 0.896\n",
      "1569/1920 (epoch 12), train_loss = -6.319, time/batch = 0.846\n",
      "1570/1920 (epoch 12), train_loss = -6.487, time/batch = 0.806\n",
      "1571/1920 (epoch 12), train_loss = -6.605, time/batch = 1.046\n",
      "1572/1920 (epoch 12), train_loss = -6.602, time/batch = 0.891\n",
      "1573/1920 (epoch 12), train_loss = -6.676, time/batch = 0.871\n",
      "1574/1920 (epoch 12), train_loss = -6.699, time/batch = 0.886\n",
      "1575/1920 (epoch 12), train_loss = -6.593, time/batch = 0.917\n",
      "1576/1920 (epoch 12), train_loss = -6.787, time/batch = 0.876\n",
      "1577/1920 (epoch 12), train_loss = -6.726, time/batch = 0.819\n",
      "1578/1920 (epoch 12), train_loss = -6.743, time/batch = 0.996\n",
      "1579/1920 (epoch 12), train_loss = -6.827, time/batch = 0.918\n",
      "1580/1920 (epoch 12), train_loss = -6.769, time/batch = 0.851\n",
      "1581/1920 (epoch 12), train_loss = -6.718, time/batch = 0.972\n",
      "1582/1920 (epoch 12), train_loss = -6.831, time/batch = 0.900\n",
      "1583/1920 (epoch 12), train_loss = -6.722, time/batch = 1.036\n",
      "1584/1920 (epoch 12), train_loss = -6.834, time/batch = 0.972\n",
      "1585/1920 (epoch 12), train_loss = -6.747, time/batch = 1.081\n",
      "1586/1920 (epoch 12), train_loss = -6.820, time/batch = 0.848\n",
      "1587/1920 (epoch 12), train_loss = -6.959, time/batch = 0.889\n",
      "1588/1920 (epoch 12), train_loss = -6.792, time/batch = 0.941\n",
      "1589/1920 (epoch 12), train_loss = -6.780, time/batch = 0.872\n",
      "1590/1920 (epoch 12), train_loss = -6.714, time/batch = 0.896\n",
      "1591/1920 (epoch 12), train_loss = -6.767, time/batch = 0.946\n",
      "1592/1920 (epoch 12), train_loss = -6.750, time/batch = 0.874\n",
      "1593/1920 (epoch 12), train_loss = -6.810, time/batch = 0.846\n",
      "1594/1920 (epoch 12), train_loss = -6.818, time/batch = 0.824\n",
      "1595/1920 (epoch 12), train_loss = -6.843, time/batch = 1.026\n",
      "1596/1920 (epoch 12), train_loss = -6.683, time/batch = 0.829\n",
      "1597/1920 (epoch 12), train_loss = -6.727, time/batch = 0.855\n",
      "1598/1920 (epoch 12), train_loss = -6.831, time/batch = 0.898\n",
      "1599/1920 (epoch 12), train_loss = -6.802, time/batch = 0.881\n",
      "1600/1920 (epoch 12), train_loss = -6.796, time/batch = 0.864\n",
      "model saved to models/model.ckpt\n",
      "1601/1920 (epoch 12), train_loss = -6.791, time/batch = 0.935\n",
      "1602/1920 (epoch 12), train_loss = -6.782, time/batch = 0.887\n",
      "1603/1920 (epoch 12), train_loss = -6.734, time/batch = 0.861\n",
      "1604/1920 (epoch 12), train_loss = -6.916, time/batch = 0.787\n",
      "1605/1920 (epoch 12), train_loss = -6.869, time/batch = 0.978\n",
      "1606/1920 (epoch 12), train_loss = -6.758, time/batch = 0.858\n",
      "1607/1920 (epoch 12), train_loss = -6.804, time/batch = 0.811\n",
      "1608/1920 (epoch 12), train_loss = -6.856, time/batch = 0.956\n",
      "1609/1920 (epoch 12), train_loss = -6.806, time/batch = 0.909\n",
      "1610/1920 (epoch 12), train_loss = -6.810, time/batch = 0.856\n",
      "1611/1920 (epoch 12), train_loss = -6.798, time/batch = 0.757\n",
      "1612/1920 (epoch 12), train_loss = -6.913, time/batch = 1.023\n",
      "1613/1920 (epoch 12), train_loss = -6.936, time/batch = 0.853\n",
      "1614/1920 (epoch 12), train_loss = -6.840, time/batch = 0.839\n",
      "1615/1920 (epoch 12), train_loss = -6.960, time/batch = 0.950\n",
      "1616/1920 (epoch 12), train_loss = -6.810, time/batch = 0.903\n",
      "1617/1920 (epoch 12), train_loss = -6.782, time/batch = 0.850\n",
      "1618/1920 (epoch 12), train_loss = -6.868, time/batch = 0.773\n",
      "1619/1920 (epoch 12), train_loss = -6.770, time/batch = 1.012\n",
      "1620/1920 (epoch 12), train_loss = -6.793, time/batch = 0.915\n",
      "1621/1920 (epoch 12), train_loss = -6.675, time/batch = 0.980\n",
      "1622/1920 (epoch 12), train_loss = -6.721, time/batch = 0.944\n",
      "1623/1920 (epoch 12), train_loss = -6.679, time/batch = 0.809\n",
      "1624/1920 (epoch 12), train_loss = -6.809, time/batch = 0.967\n",
      "1625/1920 (epoch 12), train_loss = -6.898, time/batch = 1.180\n",
      "1626/1920 (epoch 12), train_loss = -6.539, time/batch = 0.950\n",
      "1627/1920 (epoch 12), train_loss = -6.521, time/batch = 0.907\n",
      "1628/1920 (epoch 12), train_loss = -6.632, time/batch = 0.880\n",
      "1629/1920 (epoch 12), train_loss = -6.765, time/batch = 1.238\n",
      "1630/1920 (epoch 12), train_loss = -6.550, time/batch = 0.900\n",
      "1631/1920 (epoch 12), train_loss = -6.608, time/batch = 0.873\n",
      "1632/1920 (epoch 12), train_loss = -6.709, time/batch = 0.976\n",
      "1633/1920 (epoch 12), train_loss = -6.798, time/batch = 0.867\n",
      "1634/1920 (epoch 12), train_loss = -6.518, time/batch = 0.892\n",
      "1635/1920 (epoch 12), train_loss = -6.542, time/batch = 0.822\n",
      "1636/1920 (epoch 12), train_loss = -6.744, time/batch = 1.052\n",
      "1637/1920 (epoch 12), train_loss = -6.693, time/batch = 1.126\n",
      "1638/1920 (epoch 12), train_loss = -6.657, time/batch = 0.944\n",
      "1639/1920 (epoch 12), train_loss = -6.837, time/batch = 1.071\n",
      "1640/1920 (epoch 12), train_loss = -6.636, time/batch = 0.919\n",
      "1641/1920 (epoch 12), train_loss = -6.724, time/batch = 0.868\n",
      "1642/1920 (epoch 12), train_loss = -6.862, time/batch = 0.847\n",
      "1643/1920 (epoch 12), train_loss = -6.834, time/batch = 1.153\n",
      "1644/1920 (epoch 12), train_loss = -6.738, time/batch = 0.904\n",
      "1645/1920 (epoch 12), train_loss = -6.994, time/batch = 0.844\n",
      "1646/1920 (epoch 12), train_loss = -6.662, time/batch = 0.941\n",
      "1647/1920 (epoch 12), train_loss = -6.803, time/batch = 1.049\n",
      "1648/1920 (epoch 12), train_loss = -6.869, time/batch = 0.892\n",
      "1649/1920 (epoch 12), train_loss = -7.040, time/batch = 0.829\n",
      "1650/1920 (epoch 12), train_loss = -6.621, time/batch = 0.955\n",
      "1651/1920 (epoch 12), train_loss = -6.725, time/batch = 0.905\n",
      "1652/1920 (epoch 12), train_loss = -6.874, time/batch = 0.827\n",
      "1653/1920 (epoch 12), train_loss = -6.787, time/batch = 1.050\n",
      "1654/1920 (epoch 12), train_loss = -6.906, time/batch = 0.898\n",
      "1655/1920 (epoch 12), train_loss = -6.857, time/batch = 0.902\n",
      "1656/1920 (epoch 12), train_loss = -6.904, time/batch = 0.875\n",
      "1657/1920 (epoch 12), train_loss = -6.898, time/batch = 1.089\n",
      "1658/1920 (epoch 12), train_loss = -6.797, time/batch = 1.012\n",
      "1659/1920 (epoch 12), train_loss = -6.738, time/batch = 0.861\n",
      "1660/1920 (epoch 12), train_loss = -6.726, time/batch = 0.916\n",
      "1661/1920 (epoch 12), train_loss = -6.711, time/batch = 0.839\n",
      "1662/1920 (epoch 12), train_loss = -6.877, time/batch = 0.899\n",
      "1663/1920 (epoch 12), train_loss = -6.397, time/batch = 0.767\n",
      "1664/1920 (epoch 13), train_loss = -5.800, time/batch = 0.867\n",
      "1665/1920 (epoch 13), train_loss = -6.724, time/batch = 0.932\n",
      "1666/1920 (epoch 13), train_loss = -6.608, time/batch = 0.996\n",
      "1667/1920 (epoch 13), train_loss = -6.345, time/batch = 0.846\n",
      "1668/1920 (epoch 13), train_loss = -6.586, time/batch = 0.864\n",
      "1669/1920 (epoch 13), train_loss = -6.759, time/batch = 0.833\n",
      "1670/1920 (epoch 13), train_loss = -6.798, time/batch = 0.996\n",
      "1671/1920 (epoch 13), train_loss = -6.665, time/batch = 0.926\n",
      "1672/1920 (epoch 13), train_loss = -6.636, time/batch = 0.870\n",
      "1673/1920 (epoch 13), train_loss = -6.902, time/batch = 1.040\n",
      "1674/1920 (epoch 13), train_loss = -6.366, time/batch = 0.918\n",
      "1675/1920 (epoch 13), train_loss = -5.912, time/batch = 0.836\n",
      "1676/1920 (epoch 13), train_loss = -6.399, time/batch = 1.206\n",
      "1677/1920 (epoch 13), train_loss = -6.986, time/batch = 0.979\n",
      "1678/1920 (epoch 13), train_loss = -6.721, time/batch = 0.968\n",
      "1679/1920 (epoch 13), train_loss = -6.720, time/batch = 0.834\n",
      "1680/1920 (epoch 13), train_loss = -6.818, time/batch = 0.976\n",
      "1681/1920 (epoch 13), train_loss = -6.920, time/batch = 0.974\n",
      "1682/1920 (epoch 13), train_loss = -6.859, time/batch = 0.904\n",
      "1683/1920 (epoch 13), train_loss = -6.816, time/batch = 1.044\n",
      "1684/1920 (epoch 13), train_loss = -6.738, time/batch = 0.926\n",
      "1685/1920 (epoch 13), train_loss = -6.685, time/batch = 0.884\n",
      "1686/1920 (epoch 13), train_loss = -7.042, time/batch = 0.906\n",
      "1687/1920 (epoch 13), train_loss = -6.713, time/batch = 1.077\n",
      "1688/1920 (epoch 13), train_loss = -6.713, time/batch = 1.108\n",
      "1689/1920 (epoch 13), train_loss = -6.827, time/batch = 0.982\n",
      "1690/1920 (epoch 13), train_loss = -6.704, time/batch = 1.210\n",
      "1691/1920 (epoch 13), train_loss = -6.891, time/batch = 1.021\n",
      "1692/1920 (epoch 13), train_loss = -6.889, time/batch = 1.061\n",
      "1693/1920 (epoch 13), train_loss = -6.923, time/batch = 0.953\n",
      "1694/1920 (epoch 13), train_loss = -6.876, time/batch = 1.067\n",
      "1695/1920 (epoch 13), train_loss = -6.865, time/batch = 0.870\n",
      "1696/1920 (epoch 13), train_loss = -7.034, time/batch = 0.887\n",
      "1697/1920 (epoch 13), train_loss = -6.808, time/batch = 1.098\n",
      "1698/1920 (epoch 13), train_loss = -6.813, time/batch = 0.969\n",
      "1699/1920 (epoch 13), train_loss = -6.944, time/batch = 0.881\n",
      "1700/1920 (epoch 13), train_loss = -6.842, time/batch = 0.818\n",
      "model saved to models/model.ckpt\n",
      "1701/1920 (epoch 13), train_loss = -6.786, time/batch = 0.660\n",
      "1702/1920 (epoch 13), train_loss = -6.903, time/batch = 0.602\n",
      "1703/1920 (epoch 13), train_loss = -6.769, time/batch = 0.598\n",
      "1704/1920 (epoch 13), train_loss = -6.729, time/batch = 0.561\n",
      "1705/1920 (epoch 13), train_loss = -6.860, time/batch = 0.673\n",
      "1706/1920 (epoch 13), train_loss = -6.851, time/batch = 0.589\n",
      "1707/1920 (epoch 13), train_loss = -6.728, time/batch = 0.626\n",
      "1708/1920 (epoch 13), train_loss = -6.632, time/batch = 0.576\n",
      "1709/1920 (epoch 13), train_loss = -6.722, time/batch = 0.619\n",
      "1710/1920 (epoch 13), train_loss = -6.735, time/batch = 0.613\n",
      "1711/1920 (epoch 13), train_loss = -6.959, time/batch = 0.600\n",
      "1712/1920 (epoch 13), train_loss = -6.731, time/batch = 0.632\n",
      "1713/1920 (epoch 13), train_loss = -6.780, time/batch = 0.640\n",
      "1714/1920 (epoch 13), train_loss = -6.719, time/batch = 0.651\n",
      "1715/1920 (epoch 13), train_loss = -6.925, time/batch = 0.740\n",
      "1716/1920 (epoch 13), train_loss = -6.855, time/batch = 0.628\n",
      "1717/1920 (epoch 13), train_loss = -6.770, time/batch = 0.807\n",
      "1718/1920 (epoch 13), train_loss = -6.869, time/batch = 0.609\n",
      "1719/1920 (epoch 13), train_loss = -6.713, time/batch = 0.643\n",
      "1720/1920 (epoch 13), train_loss = -6.934, time/batch = 0.602\n",
      "1721/1920 (epoch 13), train_loss = -6.770, time/batch = 0.740\n",
      "1722/1920 (epoch 13), train_loss = -6.953, time/batch = 0.763\n",
      "1723/1920 (epoch 13), train_loss = -6.748, time/batch = 0.698\n",
      "1724/1920 (epoch 13), train_loss = -6.920, time/batch = 0.704\n",
      "1725/1920 (epoch 13), train_loss = -6.830, time/batch = 0.644\n",
      "1726/1920 (epoch 13), train_loss = -6.973, time/batch = 0.634\n",
      "1727/1920 (epoch 13), train_loss = -6.928, time/batch = 0.665\n",
      "1728/1920 (epoch 13), train_loss = -6.798, time/batch = 0.862\n",
      "1729/1920 (epoch 13), train_loss = -6.889, time/batch = 0.663\n",
      "1730/1920 (epoch 13), train_loss = -7.029, time/batch = 0.666\n",
      "1731/1920 (epoch 13), train_loss = -6.974, time/batch = 0.667\n",
      "1732/1920 (epoch 13), train_loss = -6.844, time/batch = 0.631\n",
      "1733/1920 (epoch 13), train_loss = -6.930, time/batch = 0.641\n",
      "1734/1920 (epoch 13), train_loss = -6.970, time/batch = 0.614\n",
      "1735/1920 (epoch 13), train_loss = -6.805, time/batch = 0.668\n",
      "1736/1920 (epoch 13), train_loss = -6.745, time/batch = 0.653\n",
      "1737/1920 (epoch 13), train_loss = -6.900, time/batch = 0.622\n",
      "1738/1920 (epoch 13), train_loss = -6.916, time/batch = 0.646\n",
      "1739/1920 (epoch 13), train_loss = -6.872, time/batch = 0.637\n",
      "1740/1920 (epoch 13), train_loss = -6.875, time/batch = 0.621\n",
      "1741/1920 (epoch 13), train_loss = -6.966, time/batch = 0.700\n",
      "1742/1920 (epoch 13), train_loss = -6.950, time/batch = 0.686\n",
      "1743/1920 (epoch 13), train_loss = -7.011, time/batch = 0.642\n",
      "1744/1920 (epoch 13), train_loss = -6.839, time/batch = 0.643\n",
      "1745/1920 (epoch 13), train_loss = -6.937, time/batch = 0.623\n",
      "1746/1920 (epoch 13), train_loss = -6.870, time/batch = 0.617\n",
      "1747/1920 (epoch 13), train_loss = -7.001, time/batch = 0.667\n",
      "1748/1920 (epoch 13), train_loss = -6.952, time/batch = 0.763\n",
      "1749/1920 (epoch 13), train_loss = -6.881, time/batch = 0.640\n",
      "1750/1920 (epoch 13), train_loss = -6.797, time/batch = 0.694\n",
      "1751/1920 (epoch 13), train_loss = -6.981, time/batch = 0.676\n",
      "1752/1920 (epoch 13), train_loss = -6.922, time/batch = 0.635\n",
      "1753/1920 (epoch 13), train_loss = -6.903, time/batch = 0.627\n",
      "1754/1920 (epoch 13), train_loss = -6.982, time/batch = 0.881\n",
      "1755/1920 (epoch 13), train_loss = -7.009, time/batch = 0.869\n",
      "1756/1920 (epoch 13), train_loss = -7.103, time/batch = 0.745\n",
      "1757/1920 (epoch 13), train_loss = -7.034, time/batch = 0.683\n",
      "1758/1920 (epoch 13), train_loss = -6.918, time/batch = 0.604\n",
      "1759/1920 (epoch 13), train_loss = -6.950, time/batch = 0.621\n",
      "1760/1920 (epoch 13), train_loss = -6.946, time/batch = 0.952\n",
      "1761/1920 (epoch 13), train_loss = -6.882, time/batch = 0.744\n",
      "1762/1920 (epoch 13), train_loss = -7.075, time/batch = 0.693\n",
      "1763/1920 (epoch 13), train_loss = -6.945, time/batch = 0.627\n",
      "1764/1920 (epoch 13), train_loss = -7.005, time/batch = 0.600\n",
      "1765/1920 (epoch 13), train_loss = -7.061, time/batch = 0.653\n",
      "1766/1920 (epoch 13), train_loss = -6.996, time/batch = 0.589\n",
      "1767/1920 (epoch 13), train_loss = -6.966, time/batch = 0.653\n",
      "1768/1920 (epoch 13), train_loss = -7.014, time/batch = 0.628\n",
      "1769/1920 (epoch 13), train_loss = -6.789, time/batch = 0.635\n",
      "1770/1920 (epoch 13), train_loss = -6.912, time/batch = 0.623\n",
      "1771/1920 (epoch 13), train_loss = -6.842, time/batch = 0.640\n",
      "1772/1920 (epoch 13), train_loss = -6.993, time/batch = 0.663\n",
      "1773/1920 (epoch 13), train_loss = -6.718, time/batch = 0.606\n",
      "1774/1920 (epoch 13), train_loss = -6.976, time/batch = 0.618\n",
      "1775/1920 (epoch 13), train_loss = -6.977, time/batch = 0.630\n",
      "1776/1920 (epoch 13), train_loss = -6.890, time/batch = 0.592\n",
      "1777/1920 (epoch 13), train_loss = -7.080, time/batch = 0.650\n",
      "1778/1920 (epoch 13), train_loss = -7.046, time/batch = 0.619\n",
      "1779/1920 (epoch 13), train_loss = -7.015, time/batch = 0.615\n",
      "1780/1920 (epoch 13), train_loss = -6.873, time/batch = 0.639\n",
      "1781/1920 (epoch 13), train_loss = -6.923, time/batch = 0.608\n",
      "1782/1920 (epoch 13), train_loss = -7.000, time/batch = 0.602\n",
      "1783/1920 (epoch 13), train_loss = -6.906, time/batch = 0.656\n",
      "1784/1920 (epoch 13), train_loss = -7.117, time/batch = 0.709\n",
      "1785/1920 (epoch 13), train_loss = -6.823, time/batch = 0.690\n",
      "1786/1920 (epoch 13), train_loss = -6.726, time/batch = 0.685\n",
      "1787/1920 (epoch 13), train_loss = -6.704, time/batch = 0.641\n",
      "1788/1920 (epoch 13), train_loss = -6.912, time/batch = 0.612\n",
      "1789/1920 (epoch 13), train_loss = -6.970, time/batch = 0.621\n",
      "1790/1920 (epoch 13), train_loss = -6.932, time/batch = 0.959\n",
      "1791/1920 (epoch 13), train_loss = -7.101, time/batch = 0.947\n",
      "1792/1920 (epoch 14), train_loss = -6.481, time/batch = 0.703\n",
      "1793/1920 (epoch 14), train_loss = -6.939, time/batch = 1.015\n",
      "1794/1920 (epoch 14), train_loss = -6.798, time/batch = 0.972\n",
      "1795/1920 (epoch 14), train_loss = -7.024, time/batch = 0.890\n",
      "1796/1920 (epoch 14), train_loss = -6.836, time/batch = 1.030\n",
      "1797/1920 (epoch 14), train_loss = -6.788, time/batch = 1.025\n",
      "1798/1920 (epoch 14), train_loss = -7.038, time/batch = 1.046\n",
      "1799/1920 (epoch 14), train_loss = -6.765, time/batch = 0.976\n",
      "1800/1920 (epoch 14), train_loss = -6.990, time/batch = 1.119\n",
      "model saved to models/model.ckpt\n",
      "1801/1920 (epoch 14), train_loss = -7.038, time/batch = 0.989\n",
      "1802/1920 (epoch 14), train_loss = -7.012, time/batch = 0.898\n",
      "1803/1920 (epoch 14), train_loss = -6.806, time/batch = 1.003\n",
      "1804/1920 (epoch 14), train_loss = -6.895, time/batch = 1.073\n",
      "1805/1920 (epoch 14), train_loss = -6.950, time/batch = 1.083\n",
      "1806/1920 (epoch 14), train_loss = -6.773, time/batch = 1.036\n",
      "1807/1920 (epoch 14), train_loss = -6.931, time/batch = 0.906\n",
      "1808/1920 (epoch 14), train_loss = -7.028, time/batch = 1.137\n",
      "1809/1920 (epoch 14), train_loss = -6.875, time/batch = 0.984\n",
      "1810/1920 (epoch 14), train_loss = -7.042, time/batch = 0.928\n",
      "1811/1920 (epoch 14), train_loss = -6.907, time/batch = 1.267\n",
      "1812/1920 (epoch 14), train_loss = -6.909, time/batch = 1.014\n",
      "1813/1920 (epoch 14), train_loss = -7.000, time/batch = 1.021\n",
      "1814/1920 (epoch 14), train_loss = -6.848, time/batch = 1.048\n",
      "1815/1920 (epoch 14), train_loss = -7.053, time/batch = 1.069\n",
      "1816/1920 (epoch 14), train_loss = -6.997, time/batch = 1.006\n",
      "1817/1920 (epoch 14), train_loss = -6.911, time/batch = 0.966\n",
      "1818/1920 (epoch 14), train_loss = -7.000, time/batch = 1.094\n",
      "1819/1920 (epoch 14), train_loss = -6.964, time/batch = 0.996\n",
      "1820/1920 (epoch 14), train_loss = -6.927, time/batch = 0.998\n",
      "1821/1920 (epoch 14), train_loss = -7.063, time/batch = 0.865\n",
      "1822/1920 (epoch 14), train_loss = -6.871, time/batch = 0.761\n",
      "1823/1920 (epoch 14), train_loss = -7.027, time/batch = 0.878\n",
      "1824/1920 (epoch 14), train_loss = -7.038, time/batch = 0.714\n",
      "1825/1920 (epoch 14), train_loss = -6.869, time/batch = 1.156\n",
      "1826/1920 (epoch 14), train_loss = -6.900, time/batch = 1.001\n",
      "1827/1920 (epoch 14), train_loss = -6.965, time/batch = 1.180\n",
      "1828/1920 (epoch 14), train_loss = -7.016, time/batch = 1.199\n",
      "1829/1920 (epoch 14), train_loss = -7.001, time/batch = 1.159\n",
      "1830/1920 (epoch 14), train_loss = -7.011, time/batch = 1.130\n",
      "1831/1920 (epoch 14), train_loss = -7.012, time/batch = 1.133\n",
      "1832/1920 (epoch 14), train_loss = -6.912, time/batch = 0.948\n",
      "1833/1920 (epoch 14), train_loss = -7.105, time/batch = 1.215\n",
      "1834/1920 (epoch 14), train_loss = -7.033, time/batch = 1.194\n",
      "1835/1920 (epoch 14), train_loss = -6.979, time/batch = 1.196\n",
      "1836/1920 (epoch 14), train_loss = -6.943, time/batch = 1.003\n",
      "1837/1920 (epoch 14), train_loss = -7.111, time/batch = 1.053\n",
      "1838/1920 (epoch 14), train_loss = -7.161, time/batch = 1.037\n",
      "1839/1920 (epoch 14), train_loss = -7.051, time/batch = 0.954\n",
      "1840/1920 (epoch 14), train_loss = -6.888, time/batch = 0.923\n",
      "1841/1920 (epoch 14), train_loss = -6.995, time/batch = 0.920\n",
      "1842/1920 (epoch 14), train_loss = -7.043, time/batch = 1.048\n",
      "1843/1920 (epoch 14), train_loss = -6.947, time/batch = 0.850\n",
      "1844/1920 (epoch 14), train_loss = -7.065, time/batch = 1.054\n",
      "1845/1920 (epoch 14), train_loss = -6.916, time/batch = 1.135\n",
      "1846/1920 (epoch 14), train_loss = -6.957, time/batch = 0.969\n",
      "1847/1920 (epoch 14), train_loss = -7.007, time/batch = 0.933\n",
      "1848/1920 (epoch 14), train_loss = -7.085, time/batch = 0.909\n",
      "1849/1920 (epoch 14), train_loss = -6.907, time/batch = 1.096\n",
      "1850/1920 (epoch 14), train_loss = -7.029, time/batch = 0.910\n",
      "1851/1920 (epoch 14), train_loss = -7.014, time/batch = 0.988\n",
      "1852/1920 (epoch 14), train_loss = -6.972, time/batch = 1.064\n",
      "1853/1920 (epoch 14), train_loss = -6.987, time/batch = 0.928\n",
      "1854/1920 (epoch 14), train_loss = -6.621, time/batch = 1.165\n",
      "1855/1920 (epoch 14), train_loss = -6.650, time/batch = 1.064\n",
      "1856/1920 (epoch 14), train_loss = -7.176, time/batch = 1.069\n",
      "1857/1920 (epoch 14), train_loss = -6.806, time/batch = 0.933\n",
      "1858/1920 (epoch 14), train_loss = -6.782, time/batch = 0.890\n",
      "1859/1920 (epoch 14), train_loss = -6.987, time/batch = 1.030\n",
      "1860/1920 (epoch 14), train_loss = -6.758, time/batch = 0.982\n",
      "1861/1920 (epoch 14), train_loss = -6.702, time/batch = 0.942\n",
      "1862/1920 (epoch 14), train_loss = -6.980, time/batch = 0.946\n",
      "1863/1920 (epoch 14), train_loss = -6.860, time/batch = 1.000\n",
      "1864/1920 (epoch 14), train_loss = -6.474, time/batch = 0.914\n",
      "1865/1920 (epoch 14), train_loss = -6.886, time/batch = 0.930\n",
      "1866/1920 (epoch 14), train_loss = -6.874, time/batch = 1.038\n",
      "1867/1920 (epoch 14), train_loss = -6.821, time/batch = 0.941\n",
      "1868/1920 (epoch 14), train_loss = -6.844, time/batch = 0.924\n",
      "1869/1920 (epoch 14), train_loss = -7.014, time/batch = 0.948\n",
      "1870/1920 (epoch 14), train_loss = -6.849, time/batch = 0.884\n",
      "1871/1920 (epoch 14), train_loss = -6.827, time/batch = 0.880\n",
      "1872/1920 (epoch 14), train_loss = -7.046, time/batch = 0.963\n",
      "1873/1920 (epoch 14), train_loss = -7.027, time/batch = 0.997\n",
      "1874/1920 (epoch 14), train_loss = -6.992, time/batch = 0.827\n",
      "1875/1920 (epoch 14), train_loss = -7.106, time/batch = 1.140\n",
      "1876/1920 (epoch 14), train_loss = -6.985, time/batch = 0.975\n",
      "1877/1920 (epoch 14), train_loss = -7.054, time/batch = 0.876\n",
      "1878/1920 (epoch 14), train_loss = -6.520, time/batch = 0.876\n",
      "1879/1920 (epoch 14), train_loss = -6.418, time/batch = 0.952\n",
      "1880/1920 (epoch 14), train_loss = -6.898, time/batch = 1.210\n",
      "1881/1920 (epoch 14), train_loss = -6.659, time/batch = 0.910\n",
      "1882/1920 (epoch 14), train_loss = -6.630, time/batch = 1.033\n",
      "1883/1920 (epoch 14), train_loss = -6.757, time/batch = 1.209\n",
      "1884/1920 (epoch 14), train_loss = -6.980, time/batch = 1.007\n",
      "1885/1920 (epoch 14), train_loss = -6.774, time/batch = 1.100\n",
      "1886/1920 (epoch 14), train_loss = -6.722, time/batch = 0.898\n",
      "1887/1920 (epoch 14), train_loss = -6.890, time/batch = 1.070\n",
      "1888/1920 (epoch 14), train_loss = -6.986, time/batch = 0.968\n",
      "1889/1920 (epoch 14), train_loss = -6.741, time/batch = 1.090\n",
      "1890/1920 (epoch 14), train_loss = -6.989, time/batch = 1.092\n",
      "1891/1920 (epoch 14), train_loss = -6.982, time/batch = 1.080\n",
      "1892/1920 (epoch 14), train_loss = -6.881, time/batch = 0.890\n",
      "1893/1920 (epoch 14), train_loss = -7.016, time/batch = 0.879\n",
      "1894/1920 (epoch 14), train_loss = -7.026, time/batch = 1.046\n",
      "1895/1920 (epoch 14), train_loss = -6.832, time/batch = 0.996\n",
      "1896/1920 (epoch 14), train_loss = -7.141, time/batch = 0.953\n",
      "1897/1920 (epoch 14), train_loss = -6.822, time/batch = 1.039\n",
      "1898/1920 (epoch 14), train_loss = -6.729, time/batch = 0.989\n",
      "1899/1920 (epoch 14), train_loss = -7.091, time/batch = 0.970\n",
      "1900/1920 (epoch 14), train_loss = -6.963, time/batch = 0.968\n",
      "model saved to models/model.ckpt\n",
      "1901/1920 (epoch 14), train_loss = -6.898, time/batch = 0.549\n",
      "1902/1920 (epoch 14), train_loss = -7.085, time/batch = 0.539\n",
      "1903/1920 (epoch 14), train_loss = -6.950, time/batch = 0.506\n",
      "1904/1920 (epoch 14), train_loss = -7.036, time/batch = 0.495\n",
      "1905/1920 (epoch 14), train_loss = -7.020, time/batch = 0.466\n",
      "1906/1920 (epoch 14), train_loss = -6.859, time/batch = 0.517\n",
      "1907/1920 (epoch 14), train_loss = -6.972, time/batch = 0.462\n",
      "1908/1920 (epoch 14), train_loss = -6.760, time/batch = 0.558\n",
      "1909/1920 (epoch 14), train_loss = -6.802, time/batch = 0.500\n",
      "1910/1920 (epoch 14), train_loss = -6.936, time/batch = 0.524\n",
      "1911/1920 (epoch 14), train_loss = -6.953, time/batch = 0.555\n",
      "1912/1920 (epoch 14), train_loss = -7.073, time/batch = 0.513\n",
      "1913/1920 (epoch 14), train_loss = -6.911, time/batch = 0.599\n",
      "1914/1920 (epoch 14), train_loss = -7.019, time/batch = 0.546\n",
      "1915/1920 (epoch 14), train_loss = -6.998, time/batch = 0.495\n",
      "1916/1920 (epoch 14), train_loss = -6.992, time/batch = 0.486\n",
      "1917/1920 (epoch 14), train_loss = -7.051, time/batch = 0.499\n",
      "1918/1920 (epoch 14), train_loss = -6.877, time/batch = 0.476\n",
      "1919/1920 (epoch 14), train_loss = -7.002, time/batch = 0.469\n"
     ]
    }
   ],
   "source": [
    "for e in xrange(num_epochs):\n",
    "    sess.run(tf.assign(lr, learning_rate * (decay_rate ** e)))\n",
    "#     data_loader.reset_batch_pointer()\n",
    "    c0, h0 = zstate_cell0.c.eval(), zstate_cell0.h.eval()\n",
    "    c1, h1 = zstate_cell1.c.eval(), zstate_cell1.h.eval()\n",
    "\n",
    "    kappa = np.zeros((batch_size, kmixtures, 1)) #this is a major problem for training\n",
    "    for b in xrange(data_loader.batch_size):\n",
    "        start = time.time()\n",
    "        i = e * data_loader.batch_size + b\n",
    "        x, y, c = data_loader.next_batch()\n",
    "\n",
    "        feed = {input_data: x, target_data: y, \\\n",
    "                zstate_cell0.c: c0, zstate_cell0.h: h0, zstate_cell1.c: c1, zstate_cell1.h: h1, \\\n",
    "                char_seq: c, init_kappa: kappa}\n",
    "        fetch = [cost, \\\n",
    "                 pstate_cell0.c, pstate_cell0.h, pstate_cell1.c, pstate_cell1.h, \\\n",
    "                 train_op]\n",
    "        train_loss, c0, h0, c1, c1, _ = sess.run(fetch, feed)\n",
    "#         #add if you want visualizations with TensorBoard\n",
    "#         fetch = [merged,cost, \\\n",
    "#                  pstate_cell0.c, pstate_cell0.h, pstate_cell1.c, pstate_cell1.h, \\\n",
    "#                  train_op]\n",
    "#         summary, train_loss, c0, h0, c1, c1, _ = sess.run(fetch, feed)\n",
    "#         train_writer.add_summary(summary, i)\n",
    "\n",
    "        end = time.time()\n",
    "#         print \"all_new_kappas: \", all_new_kappas_.shape, all_new_kappas_[0,:,0:5].T\n",
    "        print \"{}/{} (epoch {}), train_loss = {:.3f}, time/batch = {:.3f}\" \\\n",
    "            .format(i, num_epochs * data_loader.batch_size,\n",
    "                    e, train_loss, end - start)\n",
    "        if (e * data_loader.batch_size + b) % save_every == 0 and ((e * data_loader.batch_size + b) > 0):\n",
    "            checkpoint_path = os.path.join('models', 'model.ckpt')\n",
    "            saver.save(sess, checkpoint_path, global_step = e * data_loader.batch_size + b)\n",
    "            print \"model saved to {}\".format(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
